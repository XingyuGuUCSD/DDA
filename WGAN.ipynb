{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTya_dtvpdaX"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e9W0cTFEPmAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold\n",
    "from math import exp\n",
    "from torch.autograd import Variable\n",
    "from my_dataset import MNIST_M\n",
    "from my_dataset import ST_Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the numpy input image with dim 3 x height x width\n",
    "def convert_to_plt(input_image):\n",
    "    input_image = input_image*np.asarray([0.5,0.5,0.5]) + np.asarray([0.5,0.5,0.5])\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1523506225267,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "4BrzNF_oQL77",
    "outputId": "fe8e2212-37c8-4027-d176-ef46991bcd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3W2qqi9azoVs"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            #inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs, _, _ = model(inputs)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels.data).sum().item()\n",
    "            total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oBeLwnRLrcY"
   },
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "root_dir = \"./data/mnist/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, )),\n",
    "    #transforms.Normalize((0.13066047712053577, ), (1, )),\n",
    "    transforms.Lambda(lambda x: torch.cat((x, x, x), dim=0))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root_dir, train=True, transform=transform,\n",
    "                                      target_transform=None, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset = torchvision.datasets.MNIST(root_dir, train=False, transform=transform,\n",
    "                                      target_transform=None, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 123th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAAz5JREFUeJztms8rdGEYhq8jKaWUnWJhYSWzNSkWLCwoK/F3jI1iZSVJYmErK4qVWFlYKNmgbCalEFt/gGK+hd6Zb8wwP4zbmXFfuznndM47V/c8Pc97JspkMhgNTb+9gL+EZQuxbCGWLcSyhVi2EMsWYtlCLFtIs/JhURQ17LiayWSiUtc42UIsW4hlC7FsIZYtxLKFWLYQyxZi2UIsW4hlC7FsIZYtxLKFWLYQ6X52rent7QVgYmKi4FwUvW8vLywsANDe3l70Hk1N73m7vLwEYHl5GYCdnZ3aLhYnW0qk/K9ftW9qkskkAN3d3QAMDw8DMD09DUBHR0exZwFQ6vt9vO7l5QWAqakpAA4PD8tao9/UxIxY1uzR0VEAFhcXgVxtDgkuJ7VnZ2clrwEYHBzM+9zS0gJAa2trpcsuiZMtJJbJDgkeGBgoev7x8RGAt7c3ADY2NgB4eHjIXrO3t/flM0J38vz8nHf85uYGyP0yaomTLSSWyb66ugLg/v4egJOTEwCur68BWFtbq/reIdHHx8dFz29tbQHw9PRU9TM+w8kWUhd9di3o7OwE4OjoCIBEIgHkJsjd3V0AZmZmqrp/OX12LMvITzA5OQlAf38/kGsJ0+k0AHNzcz++BpcRIQ1fRsKAtL+/D0BbWxsAd3d3AIyMjAD5bWM1eFyPGQ1bs7u6ugBIpVJALtG3t7cAjI+PA99PdCU42UIatma/vr4ChRtRobUrNc5Ximt2zGiomh1ej83OzmaHldBHb25uArVPdCU42UIaItlhSzZMgclkMrv9ur29DcD6+vrvLO4/nGwhdZ3sMB2urq4C0NfXlz0XXnddXFzoF/YJTraQukz2x+kwJDpMh/Pz85yfn//O4r7AyRZSlxOkejosB0+QMaMuanbYsTs4OAAomA7HxsYA7Q5eNTjZQuoi2aGPHhoaAiiYDuOe6ICTLSTWyQ61uqenJ+/40tISACsrK/I1fQcnW0is++zwJ/jT09O8483N8ftBus+OGZYtxLKFSGv2X8fJFmLZQixbiGULsWwhli3EsoVYthDLFmLZQixbiGULsWwhli3EsoVYthDLFmLZQixbiGULsWwhli3EsoX8A1gC6kFIJ2VZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "\n",
    "plt_img = convert_to_plt(inputs[idx].permute(1, 2, 0).cpu().data.numpy())\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(plt_img)\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-M Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import MNIST_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "root_dir = \"./data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4581609321206303, 0.462350402961343, 0.4084781187671726), (1, 1, 1))\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset_m = MNIST_M(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_m = torch.utils.data.DataLoader(trainset_m, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_m = MNIST_M(root_dir, train=False, transform=transform_m, download=True)\n",
    "testloader_m = torch.utils.data.DataLoader(testset_m, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 86th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_m):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "    \n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "\n",
    "plt_img = convert_to_plt(inputs[idx].permute(1, 2, 0).cpu().data.numpy())\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(plt_img)\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "def extract_features(model, dataloader):\n",
    "    features = None\n",
    "    lasthidden = None\n",
    "    labels = None\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels_ = data\n",
    "        if (use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        inputs = Variable(inputs)\n",
    "        _, f, l = model(inputs)\n",
    "        if i == 0:\n",
    "            features = f\n",
    "            lasthidden = l\n",
    "            labels = labels_\n",
    "        else:\n",
    "            features = torch.cat((features, f))\n",
    "            lasthidden = torch.cat((lasthidden, l))\n",
    "            labels = torch.cat((labels, labels_))\n",
    "    features = features.data\n",
    "    lasthidden = lasthidden.data\n",
    "    if use_gpu:\n",
    "        features = features.cpu()\n",
    "        lasthidden = lasthidden.cpu()\n",
    "    return features, lasthidden, labels\n",
    "\n",
    "def visualize_single_dataset(data, labels, perplexity=50, sample_num=None):\n",
    "    total_num = labels.shape[0]\n",
    "    if sample_num:\n",
    "        idx = np.random.choice(total_num, sample_num, replace=False)\n",
    "        data, labels = data[idx, :], labels[idx]\n",
    "        total_num = sample_num\n",
    "    tsne = manifold.TSNE(n_components=2, init='random',\n",
    "                     random_state=0, perplexity=perplexity)\n",
    "    X = tsne.fit_transform(data)\n",
    "    colors = [\"red\", \"orange\", \"goldenrod\", \"yellow\", \"yellowgreen\", \"green\", \"teal\", \"blue\", \"violet\", \"purple\"]\n",
    "    for i in range(10):\n",
    "        plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i], alpha=0.4)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def visualize_da(source, target, perplexity=50, sample_num=None, save=None):\n",
    "    source_num = source.shape[0]\n",
    "    target_num = target.shape[0]\n",
    "    if sample_num:\n",
    "        source, target = source[:sample_num, :], target[:sample_num, :]\n",
    "        \n",
    "    data = np.vstack((source, target))\n",
    "\n",
    "    tsne = manifold.TSNE(n_components=2, init='random',\n",
    "                         random_state=0, perplexity=perplexity)\n",
    "    X = tsne.fit_transform(data)\n",
    "    plt.scatter(X[:sample_num, 0], X[:sample_num, 1], c=\"blue\", edgecolors=None, alpha=0.4)\n",
    "    plt.scatter(X[sample_num:, 0], X[sample_num:, 1], c=\"red\", edgecolors=None, alpha=0.4)\n",
    "    plt.axis(\"off\")\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import ST_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainset_da = ST_Dataset(trainset, trainset_m, batch_size)\n",
    "trainloader_da = torch.utils.data.DataLoader(trainset_da, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 127th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "I don't know :)\n",
      "\n",
      "From domain:\n",
      "Target\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from test set\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_da):\n",
    "    inputs, labels, domain = data\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt_img = convert_to_plt(inputs[idx].permute(1, 2, 0).cpu().data.numpy())\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(plt_img)\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\")\n",
    "if labels[idx].item() == -1:\n",
    "    print(\"I don't know :)\\n\")\n",
    "else:\n",
    "    print(str(labels[idx].item()) + \"\\n\")\n",
    "\n",
    "print(\"From domain:\")\n",
    "if domain[idx].item() == 0:\n",
    "    print(\"Source\")\n",
    "else:\n",
    "    print(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN Algorithm\n",
    "## Definition\n",
    "Define label classifier $L$ has parameter $\\theta_{lc}$, domain classifier $D$ has parameter $\\theta_{dc}$ and feature extractor $F$ has parameter $\\theta_{fe}$  \n",
    "$x_s$ is the data from source domain, while $x_t$ is the data from target domain  \n",
    "$y_s$ is the label for source data  \n",
    "\n",
    "## Loss\n",
    "Loss function $L= D(F(x_t))-D(F(x_s))+L(F(x_s),y_s)$  \n",
    "$D(F(x_t))$ will output 1 if it thinks input is from target, else output 0  \n",
    "\n",
    "## Update  \n",
    "$\\theta_{lc}$ minimize $L(F(x_s),y_s)$  \n",
    "$\\theta_{dc}$ minimize $-D(F(x_t))+D(F(x_s))$  \n",
    "$\\theta_{fe}$ minimize $D(F(x_t))-D(F(x_s))+L(F(x_s),y_s)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # feature extractor\n",
    "        self.C1 = nn.Conv2d(3, 32, 5)\n",
    "        self.C2 = nn.Conv2d(32, 48, 5)\n",
    "\n",
    "        self.C1.weight.data.normal_(0.0, 0.1)\n",
    "        self.C2.weight.data.normal_(0.0, 0.1)    \n",
    "        \n",
    "        # label classifier\n",
    "        self.LC_FC1 = nn.Linear(48 * 8 * 8, 100)\n",
    "        self.LC_FC2 = nn.Linear(100, 100)\n",
    "        self.LC_FC3 = nn.Linear(100, 10)\n",
    "        \n",
    "        self.LC_FC1.weight.data.normal_(0.0, 0.1)\n",
    "        self.LC_FC2.weight.data.normal_(0.0, 0.1)\n",
    "        self.LC_FC3.weight.data.normal_(0.0, 0.1)\n",
    "        \n",
    "        self.LC_FC1.bias.data.normal_(0.0, 0.1)\n",
    "        self.LC_FC2.bias.data.normal_(0.0, 0.1)\n",
    "        self.LC_FC3.bias.data.normal_(0.0, 0.1)\n",
    "        \n",
    "        # domain classifier\n",
    "        self.DC_FC1 = nn.Linear(48 * 8 * 8, 100)\n",
    "        self.DC_FC2 = nn.Linear(100, 100)\n",
    "        self.DC_FC3 = nn.Linear(100, 10)\n",
    "        self.DC_FC4 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.DC_FC1.weight.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC2.weight.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC3.weight.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC4.weight.data.normal_(0.0, 0.1)\n",
    "        \n",
    "        self.DC_FC1.bias.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC2.bias.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC3.bias.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC4.bias.data.normal_(0.0, 0.1)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        \n",
    "        x = x.view(-1 , 48 * 8 * 8)\n",
    "        \n",
    "        f = x\n",
    "        # label classifier\n",
    "        # LC_FC1\n",
    "        x_l = F.relu(self.LC_FC1(f))\n",
    "        # LC_FC2\n",
    "        x_l = F.relu(self.LC_FC2(x_l))\n",
    "        # LC_FC3\n",
    "        x_l = self.LC_FC3(x_l)\n",
    "\n",
    "        \n",
    "        # discriminator classifier\n",
    "        # DC_FC1\n",
    "        x_d = F.relu(self.DC_FC1(f))\n",
    "        # DC_FC2\n",
    "        x_d = F.relu(self.DC_FC2(x_d))\n",
    "        # DC_FC3\n",
    "        x_d = F.relu(self.DC_FC3(x_d))\n",
    "        # DC_FC4\n",
    "        x_d = self.DC_FC4(x_d)\n",
    "        \n",
    "        \n",
    "        eps = 1e-5\n",
    "        x_d = torch.clamp(x_d, min=0, max=1)\n",
    "        \n",
    "        return x_l, x_d\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_da(model, dataloader):\n",
    "    features = None\n",
    "    lasthidden = None\n",
    "    labels = None\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels_ = data\n",
    "        if (use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        inputs = Variable(inputs)\n",
    "        _, _, f, l = model(inputs)\n",
    "        \n",
    "        # from source\n",
    "        if i == 0:\n",
    "            features = f\n",
    "            lasthidden = l\n",
    "            labels = labels_\n",
    "        # from target\n",
    "        else:\n",
    "            features = torch.cat((features, f))\n",
    "            lasthidden = torch.cat((lasthidden, l))\n",
    "            labels = torch.cat((labels, labels_))\n",
    "    features = features.data\n",
    "    lasthidden = lasthidden.data\n",
    "    if use_gpu:\n",
    "        features = features.cpu()\n",
    "        lasthidden = lasthidden.cpu()\n",
    "    return features, lasthidden, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (C2): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (LC_FC1): Linear(in_features=3072, out_features=100, bias=True)\n",
      "  (LC_FC2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (LC_FC3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (DC_FC1): Linear(in_features=3072, out_features=100, bias=True)\n",
      "  (DC_FC2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (DC_FC3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (DC_FC4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CNN()\n",
    "if (use_gpu):\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.001\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "\n",
    "params_FE =[net._modules['C1'].weight,net._modules['C1'].bias, \\\n",
    "            net._modules['C2'].weight,net._modules['C2'].bias]\n",
    "optimizer_FE = optim.SGD(params_FE, lr=lr_init, momentum=0.9)\n",
    "\n",
    "params_LC =[net._modules['LC_FC1'].weight,net._modules['LC_FC1'].bias, \\\n",
    "            net._modules['LC_FC2'].weight,net._modules['LC_FC2'].bias, \\\n",
    "            net._modules['LC_FC3'].weight,net._modules['LC_FC3'].bias]\n",
    "optimizer_LC = optim.SGD(params_LC, lr=lr_init, momentum=0.9)\n",
    "\n",
    "params_DC =[net._modules['DC_FC1'].weight,net._modules['DC_FC1'].bias, \\\n",
    "            net._modules['DC_FC2'].weight,net._modules['DC_FC2'].bias, \\\n",
    "            net._modules['DC_FC3'].weight,net._modules['DC_FC3'].bias, \\\n",
    "            net._modules['DC_FC4'].weight,net._modules['DC_FC4'].bias]\n",
    "optimizer_DC = optim.SGD(params_DC, lr=lr_init, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "        \n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_da = True\n"
     ]
    }
   ],
   "source": [
    "para_file_da = \"./parameters/DA_WGAN.pt\"\n",
    "\n",
    "load_model_da = os.path.isfile(para_file_da)\n",
    "\n",
    "print(\"load_model_da = \" + str(load_model_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip (not $load_model_da)\n",
    "#net.load_state_dict(torch.load(para_file_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 0 DC iter 1 loss: inf -> -0.084\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 0 LC iter 1 loss: inf -> 21.092\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 0 LC iter 1 loss: inf -> 0.971\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 1 DC iter 1 loss: -0.084 -> 0.557\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 1 LC iter 1 loss: 21.092 -> 10.325\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 1 LC iter 1 loss: 0.971 -> 8.560\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 2 DC iter 1 loss: 0.557 -> 0.001\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 2 LC iter 1 loss: 10.325 -> 7.601\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 2 LC iter 1 loss: 8.560 -> 6.910\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 3 DC iter 1 loss: 0.001 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 3 LC iter 1 loss: 7.601 -> 6.428\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 3 LC iter 1 loss: 6.910 -> 6.042\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 4 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 4 LC iter 1 loss: 6.428 -> 5.748\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 4 LC iter 1 loss: 6.042 -> 5.500\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 5 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 5 LC iter 1 loss: 5.748 -> 5.302\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 5 LC iter 1 loss: 5.500 -> 5.128\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 6 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 6 LC iter 1 loss: 5.302 -> 4.983\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 6 LC iter 1 loss: 5.128 -> 4.853\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 7 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 7 LC iter 1 loss: 4.983 -> 4.742\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 7 LC iter 1 loss: 4.853 -> 4.639\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 8 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 8 LC iter 1 loss: 4.742 -> 4.550\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 8 LC iter 1 loss: 4.639 -> 4.467\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 9 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 9 LC iter 1 loss: 4.550 -> 4.394\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 9 LC iter 1 loss: 4.467 -> 4.325\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 10 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 10 LC iter 1 loss: 4.394 -> 4.264\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 10 LC iter 1 loss: 4.325 -> 4.205\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 11 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 11 LC iter 1 loss: 4.264 -> 4.152\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 11 LC iter 1 loss: 4.205 -> 4.102\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 12 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 12 LC iter 1 loss: 4.152 -> 4.056\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 12 LC iter 1 loss: 4.102 -> 4.012\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 13 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 13 LC iter 1 loss: 4.056 -> 3.972\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 13 LC iter 1 loss: 4.012 -> 3.933\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 14 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 14 LC iter 1 loss: 3.972 -> 3.897\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 14 LC iter 1 loss: 3.933 -> 3.863\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 15 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 15 LC iter 1 loss: 3.897 -> 3.830\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 15 LC iter 1 loss: 3.863 -> 3.799\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 16 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 16 LC iter 1 loss: 3.830 -> 3.770\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 16 LC iter 1 loss: 3.799 -> 3.742\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 17 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 17 LC iter 1 loss: 3.770 -> 3.715\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 17 LC iter 1 loss: 3.742 -> 3.690\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 18 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 18 LC iter 1 loss: 3.715 -> 3.665\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 18 LC iter 1 loss: 3.690 -> 3.642\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 19 DC iter 1 loss: -0.000 -> -0.000\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 19 LC iter 1 loss: 3.665 -> 3.619\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 19 LC iter 1 loss: 3.642 -> 3.598\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 20 DC iter 1 loss: -0.000 -> -0.001\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 20 LC iter 1 loss: 3.619 -> 3.576\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 20 LC iter 1 loss: 3.598 -> 3.557\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 21 DC iter 1 loss: -0.001 -> -0.001\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 21 LC iter 1 loss: 3.576 -> 3.537\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 21 LC iter 1 loss: 3.557 -> 3.520\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 22 DC iter 1 loss: -0.001 -> -0.001\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 22 LC iter 1 loss: 3.537 -> 3.500\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 22 LC iter 1 loss: 3.520 -> 3.484\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 23 DC iter 1 loss: -0.001 -> -0.001\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 23 LC iter 1 loss: 3.500 -> 3.466\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 23 LC iter 1 loss: 3.484 -> 3.452\n",
      "\n",
      "-----------------------\n",
      "Start domain classifier training\n",
      "Epoch 24 DC iter 1 loss: -0.001 -> -0.001\n",
      "\n",
      "-----------------------\n",
      "Start label classifier training\n",
      "Epoch 24 LC iter 1 loss: 3.466 -> 3.434\n",
      "\n",
      "-----------------------\n",
      "Start feature extractor training\n",
      "Epoch 24 LC iter 1 loss: 3.452 -> 3.421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%skip $load_model_da_dc\n",
    "\n",
    "\n",
    "total_epoch = 25\n",
    "lamda = 5\n",
    "\n",
    "\n",
    "prev_loss_DC = np.float(\"inf\")\n",
    "prev_loss_LC = np.float(\"inf\")\n",
    "prev_loss_FE = np.float(\"inf\")\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "\n",
    "    \n",
    "    print('-----------------------')\n",
    "    print('Start domain classifier training')    \n",
    "  \n",
    "    # domain classifier \n",
    "    for DC_iter in range(1):\n",
    "        epoch_loss_DC = 0.0\n",
    "        running_loss_DC = 0.0\n",
    "        for batch_idx, data in enumerate(trainloader_da):\n",
    "        \n",
    "            # get inputs\n",
    "            source_size = data[0].size()[0] // 2\n",
    "            inputs, labels, domains = data\n",
    "\n",
    "            domains = domains.to(torch.float32)\n",
    "            if (use_gpu):\n",
    "                inputs, domains = inputs.cuda(), domains.cuda()\n",
    "            inputs, domains = Variable(inputs), Variable(domains)\n",
    "\n",
    "            # forward\n",
    "            source_domain = domains[:source_size]\n",
    "\n",
    "            target_domain = domains[-source_size:]\n",
    "            \n",
    "            optimizer_DC.zero_grad()\n",
    "            _, pred_domains = net(inputs)\n",
    "            pred_source_domain = pred_domains[:source_size]\n",
    "            pred_target_domain = pred_domains[-source_size:]            \n",
    "            \n",
    "            # pred_source_domains should be 0 \n",
    "            \n",
    "            loss_DC_source = -(-torch.mean(pred_source_domain))/batch_size\n",
    "            \n",
    "            # pred_source_domains should be 1\n",
    "            loss_DC_target = -(torch.mean(pred_target_domain))/batch_size\n",
    "                \n",
    "            loss_DC = loss_DC_source + loss_DC_target\n",
    "\n",
    "            loss_DC.backward()\n",
    "            optimizer_DC.step()\n",
    "\n",
    "            # stat\n",
    "            epoch_loss_DC += loss_DC.item()\n",
    "            running_loss_DC += loss_DC.item()\n",
    "            #print(loss_DC.cpu().data.numpy())\n",
    "\n",
    "        print(\"Epoch %d DC iter %d loss: %.3f -> %.3f\\n\" % (epoch , DC_iter + 1, prev_loss_DC, epoch_loss_DC))\n",
    "        if prev_loss_DC - epoch_loss_DC < 0.1:\n",
    "            prev_loss_DC = epoch_loss_DC\n",
    "            pass\n",
    "        else:\n",
    "            prev_loss_DC = epoch_loss_DC                \n",
    "\n",
    " \n",
    "\n",
    "    print('-----------------------')\n",
    "    print('Start label classifier training')\n",
    "    # Label classifier\n",
    "    adjust_lr(optimizer_LC, p)\n",
    "    for LC_iter in range(1):\n",
    "        epoch_loss_LC = 0.0\n",
    "        running_loss_LC = 0.0\n",
    "        for batch_idx, data in enumerate(trainloader_da):\n",
    "\n",
    "            # get inputs\n",
    "            source_size = data[0].size()[0] // 2\n",
    "\n",
    "            inputs, labels, domains = data\n",
    "\n",
    "            domains = domains.to(torch.float32)\n",
    "            if (use_gpu):\n",
    "                inputs, labels, domains = inputs.cuda(), labels.cuda(), domains.cuda()\n",
    "            inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "\n",
    "            optimizer_LC.zero_grad()\n",
    "            # forward\n",
    "            pred_labels, _ = net(inputs)\n",
    "            src_label = pred_labels[:source_size]\n",
    "            loss_LC = criterion_LC(src_label, labels[:source_size])/batch_size\n",
    "            loss_LC.backward()\n",
    "            optimizer_LC.step()\n",
    "\n",
    "            # stat\n",
    "            epoch_loss_LC += loss_LC.item()\n",
    "            running_loss_LC += loss_LC.item()\n",
    "\n",
    "        print(\"Epoch %d LC iter %d loss: %.3f -> %.3f\\n\" % (epoch, LC_iter + 1, prev_loss_LC, epoch_loss_LC))\n",
    "        if prev_loss_LC - epoch_loss_LC < 0.1:\n",
    "            prev_loss_LC = epoch_loss_LC\n",
    "            pass\n",
    "        else:\n",
    "            prev_loss_LC = epoch_loss_LC            \n",
    "\n",
    "    \n",
    "    \n",
    "    print('-----------------------')\n",
    "    print('Start feature extractor training')\n",
    "    # feature extractor\n",
    "    adjust_lr(optimizer_FE, p)\n",
    "    for FE_iter in range(1):\n",
    "        epoch_loss_FE = 0.0\n",
    "        running_loss_FE = 0.0\n",
    "        for batch_idx, data in enumerate(trainloader_da):\n",
    "\n",
    "            # get inputs\n",
    "            source_size = data[0].size()[0] // 2\n",
    "\n",
    "            inputs, labels, domains = data\n",
    "\n",
    "            domains = domains.to(torch.float32)\n",
    "            if (use_gpu):\n",
    "                inputs, labels, domains = inputs.cuda(), labels.cuda(), domains.cuda()\n",
    "            inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "\n",
    "            optimizer_FE.zero_grad()\n",
    "            # forward\n",
    "            pred_labels, pred_domains = net(inputs)\n",
    "            pred_source_domain = pred_domains[:source_size]\n",
    "            pred_target_domain = pred_domains[-source_size:]\n",
    "            src_label = pred_labels[:source_size]\n",
    "            loss_LC = criterion_LC(src_label, labels[:source_size])/batch_size\n",
    "            # pred_source_domains should be 0 \n",
    "            loss_DC_source = (-torch.mean(pred_source_domain))/batch_size            \n",
    "            # pred_source_domains should be 1\n",
    "            loss_DC_target = (torch.mean(pred_target_domain))/batch_size  \n",
    "            loss_FE = loss_LC + lamda*(loss_DC_source + loss_DC_target)\n",
    "            loss_FE.backward()\n",
    "            optimizer_FE.step()\n",
    "\n",
    "            # stat\n",
    "            epoch_loss_FE += loss_FE.item()\n",
    "            running_loss_FE += loss_FE.item()\n",
    "\n",
    "        print(\"Epoch %d LC iter %d loss: %.3f -> %.3f\\n\" % (epoch , FE_iter + 1, prev_loss_FE, epoch_loss_FE))\n",
    "        if prev_loss_FE - epoch_loss_FE < 0.1:\n",
    "            prev_loss_FE = epoch_loss_FE\n",
    "            pass\n",
    "        else:\n",
    "            prev_loss_FE = epoch_loss_FE            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), para_file_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on MNIST and MNIST-M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_da_accuracy(model ,dataloader, source):\n",
    "    model.eval()\n",
    "    correct_LC = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            if (use_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            inputs, labels = Variable(inputs, volatile = True), Variable(labels, volatile = True)\n",
    "\n",
    "            \n",
    "            outputs_LC, _ = model(inputs)\n",
    "            correct_LC += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "\n",
    "            total += labels.size()[0]\n",
    "        acc_LC = correct_LC / total\n",
    "    return acc_LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classifier accuracy on MNIST test set (DA): 0.859500\n"
     ]
    }
   ],
   "source": [
    "print(\"Label classifier accuracy on MNIST test set (DA): %f\"\n",
    "      %evaluate_da_accuracy(net, testloader, source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classifier accuracy on MNIST-M test set (DA): 0.377556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Label classifier accuracy on MNIST-M test set (DA): %f\\n\"\n",
    "      %evaluate_da_accuracy(net, testloader_m, source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "My_CNN_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
