{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTya_dtvpdaX"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e9W0cTFEPmAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1523506225267,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "4BrzNF_oQL77",
    "outputId": "fe8e2212-37c8-4027-d176-ef46991bcd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AX_Msxj4we7"
   },
   "source": [
    "# Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1523506395172,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "d5glpPlhprD4",
    "outputId": "6e9189d3-996c-434c-985e-0e88793fe4eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 64, 5)\n",
    "        self.C2 = nn.Conv2d(64, 64, 5)\n",
    "        self.C3 = nn.Conv2d(64, 128, 5)\n",
    "        self.FC1 = nn.Linear(48 * 4 * 4, 3072)\n",
    "        self.FC2 = nn.Linear(3072, 2048)\n",
    "        self.FC3 = nn.Linear(2048, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (3, 3), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (3, 3), stride=(2, 2))\n",
    "        print(x.size())\n",
    "        # x's size is ()\n",
    "        # flatten\n",
    "        x = x.view(-1, 48 * 4 * 4)\n",
    "        # FC1\n",
    "        x = F.relu(self.FC1(x))\n",
    "        # FC2\n",
    "        x = F.relu(self.FC2(x))\n",
    "        # FC3\n",
    "        x = self.FC3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (C2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (C3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (FC1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (FC2): Linear(in_features=3072, out_features=2048, bias=True)\n",
      "  (FC3): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "if (use_gpu):\n",
    "    cnn.cuda()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNNUM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import SYNNUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root_dir = \"./data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_syn = SYNNUM(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_syn = torch.utils.data.DataLoader(trainset_syn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_syn = SYNNUM(root_dir, train=False, transform=transform_m, download=True)\n",
    "testloader_syn = torch.utils.data.DataLoader(testset_syn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 112th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAACV1JREFUeJztnO9vU9cZxz/cXoxnucaN0tRNvTRKU5pGLKOUMqCUMVSlDFBGGaUVmqYKTZP6P0zTtF/aNE3atDerNNGtjI52XYsopQwQC4FSoMAyylIIIYUkCp4bTOp6bua5zl4851w7IYlvgB5n6vm+uY7vueec++R7nvP8Op41OjqKhRk4lZ7A5wlW2AZhhW0QVtgGYYVtEFbYBmGFbRBW2AZhhW0QrsnBto4cnZ67WigAkMvlSaeGAUj0JQDoPdcDwLnOLs53npP2XR/I9WNgrnyc3XI/AAuWLQSgZckCGpoaAaiqqQIgEAxIY+d67jlAQc0jPZwBoK+7F4DOo6c5p8YefOXIrHKvY5ltELNMxkZ8M1sxaSQ7AkByMMGZY2cA6NjdDsDVN969sUk8/EW+/q31ACxauRiAWF0tAMFg8Dr6OUBuJAdAX28fAHt37AHg1C9e9tqNjo6WZbZRNeILBRhRLzfYNwjAkT3tHPrVi3L/6qc31/+pft7q+Q0A+e89B8DyNSsAiMVjuAFRKVrmBSCXk/kM9IiwT+3YfUNDWzViEDOO2fl8nlRiCIATB44CcOiHL8B/b+EgH8ll/29fAqCmtgaAcCRCtEqJRG+WhQLZTBaA3nOyMdL/7xsa1jLbICrObE83qk0xm8nSfbYbgP1bX5Wbt5LVpbgoFD+mVlB9UwPhSBiAQEBmVsgXPLOzR83rRmGZbRAVZ7ZGIS/MTiWHON1xQr48f9XXs7OW3gfAohWLAAiFQ3Sd7gLgw9ePl33+4oEjACTaVnlmYCAgosnl8wypPeTKybO+5jMZKi7sgrpq8yo5kOCfaln7wYPPrWVl2yoA4vV1ADiuQ8sS8Rh3hUIAXN7+t8k76f8EgMRAwrOpC6EgILb+4KUBaffBR77nNRGsGjGIijG71GkAiX8ApJIpX+rjrm8uBaB10xoamyXWEVQsdhzlDVL0EqdktkI6NUxerTC9YWfS6aLJd5OwzDaIiutsj9r6g+N4ETsmUpHq3oo1KwGoa6wjFNaMLnLHVRtcSJlyfEHd+GSKqeS9yZDPy0obTg57kb2bReWFreSjYxKxeIx7274GwOVtJUt/tlwe/s5TADS2zAMgFA6PETKICtBBrKFEUr6cQsgaoUgY11VWiNook4MJPj1xYXrvNAmsGjGIijG7MO7vgGJ2vCHO6mfWAdCtgvzDqWFicYlfNC2cD0CsNgYU1QV4kVkx11TE8GT7ifKTmSOXmtoar79sRlZGX0/fLfNgLbMNovI6W8F15f8erY7SvLAZgPrGegBy+ZzHfB27CCqnw3Ecz0zLlcTBj+7tAPx5kLev/BIg+4WjdHZ6+NbEQ0phmW0QM4bZOn7sBgKE3bEsLhTwaOGMa0+J5aH1dMeedo7/7E++h168ahkA1bU13maSSqYAuHKLzD6YScIuQdGSc8b9XUShJE85oGIXHbsOAvD2j7eBj2znPU8/BkjGHSASjRTt6yFRI/T5C4b5gVUjBjEjmT3eLBx7c2zmfaB3gPbdwuh3frTNV/+zH5NaktaNqwExNwECwSC5dBooWU1Vc+DKfybv7JF7fY0JltlGMSOZPSFK0mYgjAZo332Q4z/Z7rub25bex8bvbgagSZmY4UgEEDNSu+vVKgn8wLpVnN/xljz8serkLokdPLihlWWrV/ge+/9D2CUZ7r7uSwAc2LkPgL//8i++urj9iS8DsP7ZDcxf3AJAtFrKz0q90IAKzcbrRbW0blpN8yLxWkdGRHVV11QDEgSricd8v4ZVIwYxo5mtQ57ZTNYrZjzwmjD6H79+3VcfOsnQ9m0pOZvX0kSkKgqMZbQMWNwYtY1fP6+BWsVyzc1gKKCuIS9X6QeW2QYxI5ldUI5FJi0lur1dvex7dS8A559/01cfdz75FQDWb9kIQFNLEwDhaNiLfxQHlEs+n/PG1E5NJp3x4jaRqFoREUlWuK47scc1CSyzDWLGMbuQz3tF5z1nJC6x+6Vd9G8/5LuPuWsfYv2zG4BSRivzzi3yS3/KK2pn01l6zkqR/cn2YzKHs90EVIRxgXLrFy6XJHK8Ie4lmf1gxgi7mM3O0nNWhLzz968BcOXPb/vqY+7ahwDYuGUTTQuUDa2E7LpTLGLvZEGaziMnAXj35y9f1+yvHXJPh3uj1VEv1OsHVo0YROWZrTannI519Fxi3yuyGfpl9N1PPwrAus1tgJh34Wh47DCFCSIuanMrTaddUk7ThPiX5MeO7JNytcaWJm/l4EObWGYbRMWZrePHQ0lVAH/wGBdf2O/7+bufepTHN7QCeK5zNpslmxX3vrQcBSQ5ofVsaQUVyOZ5naMzAa69cQqAk4vmE9QnzRaXn6tltkFUhNlytlA+j+gjFMrkOvzHnf46uUMOZ0WrogyqCGBftxwwSg+nvcCVdpD0WcfqWDUNzVLgo2sEo+o8ZDgS9pLM/ZQ3NQ//4A8MXpJU3E8XbynbvnJ1IwVVSDkkub7TyuTSpwHK4prkvd5//k3en+bYXd94BCh6l83KIwyFwzQqu/zw/XdI4wvXJu9olKLK21p+XKtGDKJizNYlwslBqcV7TzPbAD7cIwdWE63LATEVASJVIern1QOwdNMaAN6ZRmKiHCyzDcIos0sL4POK2fokFqrmwwjUW2szT8dLXNelKiZZmCWPSy1JcjA5LVPUx7CVgJgj3i8i6AD91f7PfOR72qReRKe+dCoMx/FOLNQpdbJmcxvtKpHw3u9UwuLGzpxaNWISRpldGp0oFr/LUbivKi/wkHMAOi9Lo+mW6t55m1xrY8xRMQu9ciIqVlLXWO8lfOMNcrpMR/GgqFJ0xr1xfqMXZ5mnCvDPHOsE4MKJM9B9xff0LLMNomK/N6I9u2xaPL2kOo6RHEySGZaqJH02cqoSKccpxjP02ZpQOOwlZR1H7ul6kGA46CVzdXt9b5KiQs9MzWbGpsxSyZS3wb/4zPftL+nMJFTuHKQz9jRXXEXiYnW1Jae2pqz6K+nLGXt13ZLvrmtd/M5PstZxPL0fCEjCN6L2g9q6WvJ5f3OESpp+OqypXjjo6k0qMHH7zwD+xSQY/091x2fpyz0/zfEsbgJGN8jPOyyzDcIK2yCssA3CCtsgrLANwgrbIKywDcIK2yCssA3CCtsgrLANwgrbIKywDcIK2yCssA3CCtsgrLANwgrbIKywDcIK2yCssA3CCtsgrLAN4n8G4fVwaki6/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_syn):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVHN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "root_dir = \"./data/\"\n",
    "data_dir = \"svhn/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_svhn = torchvision.datasets.SVHN(os.path.join(root_dir, data_dir),\n",
    "                                          split=\"train\", transform=transform, download=True)\n",
    "trainloader_svhn = torch.utils.data.DataLoader(trainset_svhn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_svhn = torchvision.datasets.SVHN(os.path.join(root_dir, data_dir),\n",
    "                                          split=\"test\", transform=transform, download=True)\n",
    "testloader_svhn = torch.utils.data.DataLoader(testset_svhn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 30th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_svhn):\n",
    "    inputs, labels = data\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73257"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_svhn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WaXlbBWN44Gd"
   },
   "source": [
    "# MNIST Dataset\n",
    "\n",
    "Load the dataset using $\\texttt{torchvision}$.\n",
    "\n",
    "* Usage of $\\texttt{transforms.Normalize(mean, std)}$:<br>\n",
    "Assume that the input has $d$ channels, $\\texttt{mean}$ should be a tuple of $d$ values, so should $\\texttt{std}$. Let $\\texttt{mean = (m1, m2, ..., md)}$ and $\\texttt{std = (s1, s2, ..., sd)}$. Then the transform does $$\\texttt{input[channel] = (input[channel] - mean[channel]) / std[channel]}\\text{.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3W2qqi9azoVs"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30198,
     "status": "ok",
     "timestamp": 1523506265222,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "e5G7cSEk0V2S",
    "outputId": "621eee0f-8ee7-490e-db5f-f82317d18f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "root_dir = \"./data/mnist/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, )),\n",
    "    #transforms.Normalize((0.13066047712053577, ), (1, )),\n",
    "    transforms.Lambda(lambda x: torch.cat((x, x, x), dim=0))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root_dir, train=True, transform=transform,\n",
    "                                      target_transform=None, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset = torchvision.datasets.MNIST(root_dir, train=False, transform=transform,\n",
    "                                      target_transform=None, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1523506265637,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "VTlEvJBY-ziX",
    "outputId": "54f6931a-ca67-452c-81fc-6c4cfae9e89a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 18th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAApZJREFUeJzt2r9K6nEcxvG3dhpEkMJBXCMQWsqxrRZvwFWvwMErCNRFcO82XFrcvIPAJYKmhmqISARpkPRMnjNo5Tnq8/vj8xrVn98PDw9fft+fJmazGaaRDHqAXeKwhRy2kMMWcthCDlvIYQs5bCGHLfRLuVgikYjtcXU2myV++oybLeSwhRy2kMMWcthCOxd2uVymXC4zHo8Zj8fStXcu7CBJ77OD9PT0BMDd3R0AJycn8hncbKHYN/v9/R2As7MzAB4fHwObxc0Wim2zX19fAWi1WkCwjZ5zs4USyv+NqJ76NZtNDg4OAKjX64ol/dQvbGLV7FwuB8D9/T2Hh4fbXGqBmx0ysbob6fV6ADQajWAH+YKbLRSrPfvl5QWAfD6/zWWW8p4dMrHYs9vtNgCdTifgSb7nZgvFYs/++PgAIJVKbePrV+I9O2QivWcXi0UAut3ul595eHgA4OjoCIDRaAQgP2FCxLeR+Q+26XR64b1+vw/A5eXl0mvf3t4AyGazG5nF20jIRLrZn5+fAOzt7S28d3p6CsBgMFh6baFQAKBSqQBwdXW11ixudsjEttmrmkwmAOzv7681i5sdMg5byGELRfpQk0yu35Xn5+cNTLIaN1so0s2eTqf/fW2tVgOgVCptapwfudlCkW727e3tP19zfn4OwMXFBQDX19ebHOlbbrZQpE+Qc6s8wbu5uQFgOBwCUK1WNzqDT5AhE4tmz80bnslk/rw2v48+Pj4G/j4L2TQ3O2Ri1ewgudkh47CFHLaQwxZy2EIOW8hhCzlsIYctJD1B7jo3W8hhCzlsIYct5LCFHLaQwxZy2EIOW8hhCzlsIYct5LCFHLaQwxZy2EIOW8hhCzlsIYct5LCFHLaQwxb6DTz+vbFcyOrcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcSb16SIFgZt"
   },
   "source": [
    "## Training on MNIST\n",
    "\n",
    "Or you can load the parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model = True\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "para_file = \"./cnn_mnist\"\n",
    "load_model = os.path.isfile(para_file)\n",
    "print(\"load_model = \" + str(load_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model)\n",
    "cnn.load_state_dict(torch.load(para_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YRHBSIQ2FkOL"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgUSfM0KF4iT"
   },
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 10050
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 606703,
     "status": "ok",
     "timestamp": 1523144380194,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "IlstjDLbFkQ6",
    "outputId": "59a56d48-bdaa-4e0e-bc20-0feaecd74fa5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50] batch loss: 2.258\n",
      "[100] batch loss: 1.116\n",
      "[150] batch loss: 0.344\n",
      "[200] batch loss: 0.208\n",
      "[250] batch loss: 0.179\n",
      "[300] batch loss: 0.129\n",
      "[350] batch loss: 0.126\n",
      "[400] batch loss: 0.146\n",
      "[450] batch loss: 0.096\n",
      "epoch 1 loss: inf -> 231.561\n",
      "\n",
      "[ 50] batch loss: 0.090\n",
      "[100] batch loss: 0.089\n",
      "[150] batch loss: 0.076\n",
      "[200] batch loss: 0.063\n",
      "[250] batch loss: 0.070\n",
      "[300] batch loss: 0.069\n",
      "[350] batch loss: 0.067\n",
      "[400] batch loss: 0.080\n",
      "[450] batch loss: 0.057\n",
      "epoch 2 loss: 231.561 -> 34.103\n",
      "\n",
      "[ 50] batch loss: 0.062\n",
      "[100] batch loss: 0.057\n",
      "[150] batch loss: 0.046\n",
      "[200] batch loss: 0.042\n",
      "[250] batch loss: 0.048\n",
      "[300] batch loss: 0.051\n",
      "[350] batch loss: 0.045\n",
      "[400] batch loss: 0.051\n",
      "[450] batch loss: 0.042\n",
      "epoch 3 loss: 34.103 -> 22.953\n",
      "\n",
      "[ 50] batch loss: 0.048\n",
      "[100] batch loss: 0.043\n",
      "[150] batch loss: 0.032\n",
      "[200] batch loss: 0.032\n",
      "[250] batch loss: 0.038\n",
      "[300] batch loss: 0.041\n",
      "[350] batch loss: 0.036\n",
      "[400] batch loss: 0.040\n",
      "[450] batch loss: 0.033\n",
      "epoch 4 loss: 22.953 -> 17.787\n",
      "\n",
      "[ 50] batch loss: 0.039\n",
      "[100] batch loss: 0.035\n",
      "[150] batch loss: 0.026\n",
      "[200] batch loss: 0.026\n",
      "[250] batch loss: 0.031\n",
      "[300] batch loss: 0.034\n",
      "[350] batch loss: 0.031\n",
      "[400] batch loss: 0.033\n",
      "[450] batch loss: 0.028\n",
      "epoch 5 loss: 17.787 -> 14.769\n",
      "\n",
      "[ 50] batch loss: 0.033\n",
      "[100] batch loss: 0.030\n",
      "[150] batch loss: 0.021\n",
      "[200] batch loss: 0.022\n",
      "[250] batch loss: 0.026\n",
      "[300] batch loss: 0.029\n",
      "[350] batch loss: 0.027\n",
      "[400] batch loss: 0.029\n",
      "[450] batch loss: 0.024\n",
      "epoch 6 loss: 14.769 -> 12.590\n",
      "\n",
      "[ 50] batch loss: 0.029\n",
      "[100] batch loss: 0.026\n",
      "[150] batch loss: 0.018\n",
      "[200] batch loss: 0.019\n",
      "[250] batch loss: 0.023\n",
      "[300] batch loss: 0.026\n",
      "[350] batch loss: 0.024\n",
      "[400] batch loss: 0.025\n",
      "[450] batch loss: 0.021\n",
      "epoch 7 loss: 12.590 -> 10.993\n",
      "\n",
      "[ 50] batch loss: 0.025\n",
      "[100] batch loss: 0.023\n",
      "[150] batch loss: 0.016\n",
      "[200] batch loss: 0.016\n",
      "[250] batch loss: 0.020\n",
      "[300] batch loss: 0.023\n",
      "[350] batch loss: 0.021\n",
      "[400] batch loss: 0.021\n",
      "[450] batch loss: 0.019\n",
      "epoch 8 loss: 10.993 -> 9.687\n",
      "\n",
      "[ 50] batch loss: 0.023\n",
      "[100] batch loss: 0.020\n",
      "[150] batch loss: 0.014\n",
      "[200] batch loss: 0.014\n",
      "[250] batch loss: 0.018\n",
      "[300] batch loss: 0.021\n",
      "[350] batch loss: 0.019\n",
      "[400] batch loss: 0.019\n",
      "[450] batch loss: 0.017\n",
      "epoch 9 loss: 9.687 -> 8.600\n",
      "\n",
      "[ 50] batch loss: 0.020\n",
      "[100] batch loss: 0.017\n",
      "[150] batch loss: 0.012\n",
      "[200] batch loss: 0.013\n",
      "[250] batch loss: 0.017\n",
      "[300] batch loss: 0.019\n",
      "[350] batch loss: 0.017\n",
      "[400] batch loss: 0.017\n",
      "[450] batch loss: 0.015\n",
      "epoch 10 loss: 8.600 -> 7.665\n",
      "\n",
      "[ 50] batch loss: 0.018\n",
      "[100] batch loss: 0.016\n",
      "[150] batch loss: 0.010\n",
      "[200] batch loss: 0.011\n",
      "[250] batch loss: 0.015\n",
      "[300] batch loss: 0.017\n",
      "[350] batch loss: 0.015\n",
      "[400] batch loss: 0.015\n",
      "[450] batch loss: 0.013\n",
      "epoch 11 loss: 7.665 -> 6.918\n",
      "\n",
      "[ 50] batch loss: 0.016\n",
      "[100] batch loss: 0.014\n",
      "[150] batch loss: 0.009\n",
      "[200] batch loss: 0.010\n",
      "[250] batch loss: 0.014\n",
      "[300] batch loss: 0.016\n",
      "[350] batch loss: 0.014\n",
      "[400] batch loss: 0.013\n",
      "[450] batch loss: 0.012\n",
      "epoch 12 loss: 6.918 -> 6.273\n",
      "\n",
      "[ 50] batch loss: 0.014\n",
      "[100] batch loss: 0.013\n",
      "[150] batch loss: 0.008\n",
      "[200] batch loss: 0.009\n",
      "[250] batch loss: 0.013\n",
      "[300] batch loss: 0.014\n",
      "[350] batch loss: 0.013\n",
      "[400] batch loss: 0.012\n",
      "[450] batch loss: 0.011\n",
      "epoch 13 loss: 6.273 -> 5.725\n",
      "\n",
      "[ 50] batch loss: 0.013\n",
      "[100] batch loss: 0.012\n",
      "[150] batch loss: 0.007\n",
      "[200] batch loss: 0.008\n",
      "[250] batch loss: 0.012\n",
      "[300] batch loss: 0.013\n",
      "[350] batch loss: 0.012\n",
      "[400] batch loss: 0.011\n",
      "[450] batch loss: 0.010\n",
      "epoch 14 loss: 5.725 -> 5.239\n",
      "\n",
      "[ 50] batch loss: 0.012\n",
      "[100] batch loss: 0.011\n",
      "[150] batch loss: 0.007\n",
      "[200] batch loss: 0.008\n",
      "[250] batch loss: 0.011\n",
      "[300] batch loss: 0.012\n",
      "[350] batch loss: 0.011\n",
      "[400] batch loss: 0.010\n",
      "[450] batch loss: 0.010\n",
      "epoch 15 loss: 5.239 -> 4.815\n",
      "\n",
      "[ 50] batch loss: 0.010\n",
      "[100] batch loss: 0.010\n",
      "[150] batch loss: 0.006\n",
      "[200] batch loss: 0.007\n",
      "[250] batch loss: 0.010\n",
      "[300] batch loss: 0.011\n",
      "[350] batch loss: 0.010\n",
      "[400] batch loss: 0.009\n",
      "[450] batch loss: 0.009\n",
      "epoch 16 loss: 4.815 -> 4.424\n",
      "\n",
      "[ 50] batch loss: 0.009\n",
      "[100] batch loss: 0.010\n",
      "[150] batch loss: 0.006\n",
      "[200] batch loss: 0.007\n",
      "[250] batch loss: 0.009\n",
      "[300] batch loss: 0.010\n",
      "[350] batch loss: 0.010\n",
      "[400] batch loss: 0.008\n",
      "[450] batch loss: 0.008\n",
      "epoch 17 loss: 4.424 -> 4.083\n",
      "\n",
      "[ 50] batch loss: 0.008\n",
      "[100] batch loss: 0.009\n",
      "[150] batch loss: 0.005\n",
      "[200] batch loss: 0.006\n",
      "[250] batch loss: 0.009\n",
      "[300] batch loss: 0.010\n",
      "[350] batch loss: 0.009\n",
      "[400] batch loss: 0.007\n",
      "[450] batch loss: 0.007\n",
      "epoch 18 loss: 4.083 -> 3.779\n",
      "\n",
      "[ 50] batch loss: 0.008\n",
      "[100] batch loss: 0.008\n",
      "[150] batch loss: 0.005\n",
      "[200] batch loss: 0.006\n",
      "[250] batch loss: 0.008\n",
      "[300] batch loss: 0.009\n",
      "[350] batch loss: 0.008\n",
      "[400] batch loss: 0.007\n",
      "[450] batch loss: 0.007\n",
      "epoch 19 loss: 3.779 -> 3.504\n",
      "\n",
      "[ 50] batch loss: 0.007\n",
      "[100] batch loss: 0.008\n",
      "[150] batch loss: 0.004\n",
      "[200] batch loss: 0.005\n",
      "[250] batch loss: 0.007\n",
      "[300] batch loss: 0.008\n",
      "[350] batch loss: 0.008\n",
      "[400] batch loss: 0.006\n",
      "[450] batch loss: 0.006\n",
      "epoch 20 loss: 3.504 -> 3.260\n",
      "\n",
      "[ 50] batch loss: 0.007\n",
      "[100] batch loss: 0.007\n",
      "[150] batch loss: 0.004\n",
      "[200] batch loss: 0.005\n",
      "[250] batch loss: 0.007\n",
      "[300] batch loss: 0.008\n",
      "[350] batch loss: 0.007\n",
      "[400] batch loss: 0.006\n",
      "[450] batch loss: 0.006\n",
      "epoch 21 loss: 3.260 -> 3.044\n",
      "\n",
      "[ 50] batch loss: 0.006\n",
      "[100] batch loss: 0.006\n",
      "[150] batch loss: 0.004\n",
      "[200] batch loss: 0.005\n",
      "[250] batch loss: 0.007\n",
      "[300] batch loss: 0.007\n",
      "[350] batch loss: 0.007\n",
      "[400] batch loss: 0.006\n",
      "[450] batch loss: 0.006\n",
      "epoch 22 loss: 3.044 -> 2.851\n",
      "\n",
      "[ 50] batch loss: 0.006\n",
      "[100] batch loss: 0.006\n",
      "[150] batch loss: 0.004\n",
      "[200] batch loss: 0.004\n",
      "[250] batch loss: 0.006\n",
      "[300] batch loss: 0.007\n",
      "[350] batch loss: 0.006\n",
      "[400] batch loss: 0.005\n",
      "[450] batch loss: 0.005\n",
      "epoch 23 loss: 2.851 -> 2.675\n",
      "\n",
      "[ 50] batch loss: 0.005\n",
      "[100] batch loss: 0.006\n",
      "[150] batch loss: 0.004\n",
      "[200] batch loss: 0.004\n",
      "[250] batch loss: 0.006\n",
      "[300] batch loss: 0.006\n",
      "[350] batch loss: 0.006\n",
      "[400] batch loss: 0.005\n",
      "[450] batch loss: 0.005\n",
      "epoch 24 loss: 2.675 -> 2.523\n",
      "\n",
      "[ 50] batch loss: 0.005\n",
      "[100] batch loss: 0.005\n",
      "[150] batch loss: 0.003\n",
      "[200] batch loss: 0.004\n",
      "[250] batch loss: 0.005\n",
      "[300] batch loss: 0.006\n",
      "[350] batch loss: 0.006\n",
      "[400] batch loss: 0.005\n",
      "[450] batch loss: 0.005\n",
      "epoch 25 loss: 2.523 -> 2.383\n",
      "\n",
      "[ 50] batch loss: 0.005\n",
      "[100] batch loss: 0.005\n",
      "[150] batch loss: 0.003\n",
      "[200] batch loss: 0.004\n",
      "[250] batch loss: 0.005\n",
      "[300] batch loss: 0.006\n",
      "[350] batch loss: 0.005\n",
      "[400] batch loss: 0.004\n",
      "[450] batch loss: 0.004\n",
      "epoch 26 loss: 2.383 -> 2.255\n",
      "\n",
      "[ 50] batch loss: 0.004\n",
      "[100] batch loss: 0.005\n",
      "[150] batch loss: 0.003\n",
      "[200] batch loss: 0.003\n",
      "[250] batch loss: 0.005\n",
      "[300] batch loss: 0.005\n",
      "[350] batch loss: 0.005\n",
      "[400] batch loss: 0.004\n",
      "[450] batch loss: 0.004\n",
      "epoch 27 loss: 2.255 -> 2.138\n",
      "\n",
      "[ 50] batch loss: 0.004\n",
      "[100] batch loss: 0.004\n",
      "[150] batch loss: 0.003\n",
      "[200] batch loss: 0.003\n",
      "[250] batch loss: 0.005\n",
      "[300] batch loss: 0.005\n",
      "[350] batch loss: 0.005\n",
      "[400] batch loss: 0.004\n",
      "[450] batch loss: 0.004\n",
      "epoch 28 loss: 2.138 -> 2.029\n",
      "\n",
      "[ 50] batch loss: 0.004\n",
      "[100] batch loss: 0.004\n",
      "[150] batch loss: 0.003\n",
      "[200] batch loss: 0.003\n",
      "[250] batch loss: 0.004\n",
      "[300] batch loss: 0.005\n",
      "[350] batch loss: 0.005\n",
      "[400] batch loss: 0.004\n",
      "[450] batch loss: 0.004\n",
      "epoch 29 loss: 2.029 -> 1.931\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%skip $load_model\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 30\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%3d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        break\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model\n",
    "\n",
    "torch.save(cnn.state_dict(), para_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oBeLwnRLrcY"
   },
   "source": [
    "## Testing on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs = model(inputs)\n",
    "        correct += (torch.max(outputs.data, 1)[1] == labels.data).sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1632,
     "status": "ok",
     "timestamp": 1523144440407,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "Rn4sOe0iLuKI",
    "outputId": "f31ce94e-ef18-4564-9772-a3a9b22c1272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST test set (source only): 0.9912\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on MNIST test set (source only): \" + str(evaluate_accuracy(cnn, testloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-M Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import MNIST_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.pt.gz\n",
      "Extracting train.pt.gz\n",
      "Downloading test.pt.gz\n",
      "Extracting test.pt.gz\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "root_dir = \"./data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4581609321206303, 0.462350402961343, 0.4084781187671726), (1, 1, 1))\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset_m = MNIST_M(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_m = torch.utils.data.DataLoader(trainset_m, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_m = MNIST_M(root_dir, train=False, transform=transform_m, download=True)\n",
    "testloader_m = torch.utils.data.DataLoader(testset_m, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 117th image in the first 128 images in the test set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_m):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "    \n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on MNIST-M Dataset (trained on source only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST-M test set (source only): 0.5806666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on MNIST-M test set (source only): \" + str(evaluate_accuracy(cnn, testloader_m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on MNIST-M Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (C2): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (FC1): Linear(in_features=768, out_features=100, bias=True)\n",
      "  (FC2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (FC3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_m = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_m.cuda()\n",
    "print(cnn_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_m = False\n"
     ]
    }
   ],
   "source": [
    "para_file_m = \"./cnn_mnist_m\"\n",
    "load_model_m = os.path.isfile(para_file_m)\n",
    "print(\"load_model_m = \" + str(load_model_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_m)\n",
    "cnn_m.load_state_dict(torch.load(para_file_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.01/((1+10)**0.75)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_m.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50] batch loss: 2.302\n",
      "[100] batch loss: 2.302\n",
      "[150] batch loss: 2.301\n",
      "[200] batch loss: 2.300\n",
      "[250] batch loss: 2.298\n",
      "[300] batch loss: 2.297\n",
      "[350] batch loss: 2.295\n",
      "[400] batch loss: 2.294\n",
      "[450] batch loss: 2.292\n",
      "epoch 1 loss: inf -> 1059.286\n",
      "\n",
      "[ 50] batch loss: 2.288\n",
      "[100] batch loss: 2.286\n",
      "[150] batch loss: 2.282\n",
      "[200] batch loss: 2.277\n",
      "[250] batch loss: 2.271\n",
      "[300] batch loss: 2.264\n",
      "[350] batch loss: 2.253\n",
      "[400] batch loss: 2.237\n",
      "[450] batch loss: 2.212\n",
      "epoch 2 loss: 1059.286 -> 1042.553\n",
      "\n",
      "[ 50] batch loss: 2.169\n",
      "[100] batch loss: 2.129\n",
      "[150] batch loss: 2.069\n",
      "[200] batch loss: 1.986\n",
      "[250] batch loss: 1.898\n",
      "[300] batch loss: 1.813\n",
      "[350] batch loss: 1.720\n",
      "[400] batch loss: 1.611\n",
      "[450] batch loss: 1.480\n",
      "epoch 3 loss: 1042.553 -> 859.559\n",
      "\n",
      "[ 50] batch loss: 1.385\n",
      "[100] batch loss: 1.321\n",
      "[150] batch loss: 1.231\n",
      "[200] batch loss: 1.149\n",
      "[250] batch loss: 1.072\n",
      "[300] batch loss: 1.011\n",
      "[350] batch loss: 0.996\n",
      "[400] batch loss: 0.922\n",
      "[450] batch loss: 0.838\n",
      "epoch 4 loss: 859.559 -> 504.832\n",
      "\n",
      "[ 50] batch loss: 0.811\n",
      "[100] batch loss: 0.797\n",
      "[150] batch loss: 0.766\n",
      "[200] batch loss: 0.721\n",
      "[250] batch loss: 0.684\n",
      "[300] batch loss: 0.658\n",
      "[350] batch loss: 0.693\n",
      "[400] batch loss: 0.649\n",
      "[450] batch loss: 0.588\n",
      "epoch 5 loss: 504.832 -> 324.257\n",
      "\n",
      "[ 50] batch loss: 0.585\n",
      "[100] batch loss: 0.595\n",
      "[150] batch loss: 0.576\n",
      "[200] batch loss: 0.555\n",
      "[250] batch loss: 0.534\n",
      "[300] batch loss: 0.519\n",
      "[350] batch loss: 0.561\n",
      "[400] batch loss: 0.524\n",
      "[450] batch loss: 0.472\n",
      "epoch 6 loss: 324.257 -> 250.786\n",
      "\n",
      "[ 50] batch loss: 0.481\n",
      "[100] batch loss: 0.495\n",
      "[150] batch loss: 0.481\n",
      "[200] batch loss: 0.470\n",
      "[250] batch loss: 0.455\n",
      "[300] batch loss: 0.444\n",
      "[350] batch loss: 0.485\n",
      "[400] batch loss: 0.448\n",
      "[450] batch loss: 0.404\n",
      "epoch 7 loss: 250.786 -> 212.100\n",
      "\n",
      "[ 50] batch loss: 0.415\n",
      "[100] batch loss: 0.431\n",
      "[150] batch loss: 0.420\n",
      "[200] batch loss: 0.410\n",
      "[250] batch loss: 0.401\n",
      "[300] batch loss: 0.393\n",
      "[350] batch loss: 0.428\n",
      "[400] batch loss: 0.392\n",
      "[450] batch loss: 0.356\n",
      "epoch 8 loss: 212.100 -> 185.834\n",
      "\n",
      "[ 50] batch loss: 0.366\n",
      "[100] batch loss: 0.384\n",
      "[150] batch loss: 0.377\n",
      "[200] batch loss: 0.364\n",
      "[250] batch loss: 0.361\n",
      "[300] batch loss: 0.354\n",
      "[350] batch loss: 0.384\n",
      "[400] batch loss: 0.352\n",
      "[450] batch loss: 0.319\n",
      "epoch 9 loss: 185.834 -> 166.169\n",
      "\n",
      "[ 50] batch loss: 0.329\n",
      "[100] batch loss: 0.347\n",
      "[150] batch loss: 0.343\n",
      "[200] batch loss: 0.328\n",
      "[250] batch loss: 0.329\n",
      "[300] batch loss: 0.324\n",
      "[350] batch loss: 0.350\n",
      "[400] batch loss: 0.322\n",
      "[450] batch loss: 0.291\n",
      "epoch 10 loss: 166.169 -> 150.907\n",
      "\n",
      "[ 50] batch loss: 0.299\n",
      "[100] batch loss: 0.319\n",
      "[150] batch loss: 0.315\n",
      "[200] batch loss: 0.300\n",
      "[250] batch loss: 0.304\n",
      "[300] batch loss: 0.300\n",
      "[350] batch loss: 0.322\n",
      "[400] batch loss: 0.298\n",
      "[450] batch loss: 0.268\n",
      "epoch 11 loss: 150.907 -> 138.926\n",
      "\n",
      "[ 50] batch loss: 0.276\n",
      "[100] batch loss: 0.297\n",
      "[150] batch loss: 0.294\n",
      "[200] batch loss: 0.278\n",
      "[250] batch loss: 0.284\n",
      "[300] batch loss: 0.281\n",
      "[350] batch loss: 0.300\n",
      "[400] batch loss: 0.280\n",
      "[450] batch loss: 0.251\n",
      "epoch 12 loss: 138.926 -> 129.426\n",
      "\n",
      "[ 50] batch loss: 0.258\n",
      "[100] batch loss: 0.280\n",
      "[150] batch loss: 0.275\n",
      "[200] batch loss: 0.260\n",
      "[250] batch loss: 0.267\n",
      "[300] batch loss: 0.265\n",
      "[350] batch loss: 0.283\n",
      "[400] batch loss: 0.266\n",
      "[450] batch loss: 0.237\n",
      "epoch 13 loss: 129.426 -> 121.766\n",
      "\n",
      "[ 50] batch loss: 0.243\n",
      "[100] batch loss: 0.265\n",
      "[150] batch loss: 0.260\n",
      "[200] batch loss: 0.245\n",
      "[250] batch loss: 0.254\n",
      "[300] batch loss: 0.252\n",
      "[350] batch loss: 0.268\n",
      "[400] batch loss: 0.253\n",
      "[450] batch loss: 0.225\n",
      "epoch 14 loss: 121.766 -> 115.408\n",
      "\n",
      "[ 50] batch loss: 0.231\n",
      "[100] batch loss: 0.253\n",
      "[150] batch loss: 0.247\n",
      "[200] batch loss: 0.234\n",
      "[250] batch loss: 0.242\n",
      "[300] batch loss: 0.240\n",
      "[350] batch loss: 0.255\n",
      "[400] batch loss: 0.243\n",
      "[450] batch loss: 0.215\n",
      "epoch 15 loss: 115.408 -> 110.032\n",
      "\n",
      "[ 50] batch loss: 0.221\n",
      "[100] batch loss: 0.243\n",
      "[150] batch loss: 0.236\n",
      "[200] batch loss: 0.224\n",
      "[250] batch loss: 0.232\n",
      "[300] batch loss: 0.230\n",
      "[350] batch loss: 0.245\n",
      "[400] batch loss: 0.233\n",
      "[450] batch loss: 0.206\n",
      "epoch 16 loss: 110.032 -> 105.439\n",
      "\n",
      "[ 50] batch loss: 0.212\n",
      "[100] batch loss: 0.234\n",
      "[150] batch loss: 0.227\n",
      "[200] batch loss: 0.215\n",
      "[250] batch loss: 0.224\n",
      "[300] batch loss: 0.222\n",
      "[350] batch loss: 0.235\n",
      "[400] batch loss: 0.225\n",
      "[450] batch loss: 0.198\n",
      "epoch 17 loss: 105.439 -> 101.449\n",
      "\n",
      "[ 50] batch loss: 0.204\n",
      "[100] batch loss: 0.226\n",
      "[150] batch loss: 0.219\n",
      "[200] batch loss: 0.208\n",
      "[250] batch loss: 0.216\n",
      "[300] batch loss: 0.214\n",
      "[350] batch loss: 0.227\n",
      "[400] batch loss: 0.218\n",
      "[450] batch loss: 0.192\n",
      "epoch 18 loss: 101.449 -> 97.931\n",
      "\n",
      "[ 50] batch loss: 0.197\n",
      "[100] batch loss: 0.219\n",
      "[150] batch loss: 0.211\n",
      "[200] batch loss: 0.201\n",
      "[250] batch loss: 0.210\n",
      "[300] batch loss: 0.207\n",
      "[350] batch loss: 0.219\n",
      "[400] batch loss: 0.212\n",
      "[450] batch loss: 0.186\n",
      "epoch 19 loss: 97.931 -> 94.811\n",
      "\n",
      "[ 50] batch loss: 0.191\n",
      "[100] batch loss: 0.213\n",
      "[150] batch loss: 0.205\n",
      "[200] batch loss: 0.195\n",
      "[250] batch loss: 0.204\n",
      "[300] batch loss: 0.201\n",
      "[350] batch loss: 0.212\n",
      "[400] batch loss: 0.206\n",
      "[450] batch loss: 0.181\n",
      "epoch 20 loss: 94.811 -> 91.998\n",
      "\n",
      "[ 50] batch loss: 0.185\n",
      "[100] batch loss: 0.207\n",
      "[150] batch loss: 0.198\n",
      "[200] batch loss: 0.190\n",
      "[250] batch loss: 0.198\n",
      "[300] batch loss: 0.195\n",
      "[350] batch loss: 0.206\n",
      "[400] batch loss: 0.201\n",
      "[450] batch loss: 0.176\n",
      "epoch 21 loss: 91.998 -> 89.448\n",
      "\n",
      "[ 50] batch loss: 0.180\n",
      "[100] batch loss: 0.202\n",
      "[150] batch loss: 0.193\n",
      "[200] batch loss: 0.185\n",
      "[250] batch loss: 0.194\n",
      "[300] batch loss: 0.190\n",
      "[350] batch loss: 0.200\n",
      "[400] batch loss: 0.196\n",
      "[450] batch loss: 0.171\n",
      "epoch 22 loss: 89.448 -> 87.121\n",
      "\n",
      "[ 50] batch loss: 0.175\n",
      "[100] batch loss: 0.197\n",
      "[150] batch loss: 0.188\n",
      "[200] batch loss: 0.180\n",
      "[250] batch loss: 0.189\n",
      "[300] batch loss: 0.186\n",
      "[350] batch loss: 0.195\n",
      "[400] batch loss: 0.192\n",
      "[450] batch loss: 0.168\n",
      "epoch 23 loss: 87.121 -> 84.989\n",
      "\n",
      "[ 50] batch loss: 0.171\n",
      "[100] batch loss: 0.193\n",
      "[150] batch loss: 0.183\n",
      "[200] batch loss: 0.176\n",
      "[250] batch loss: 0.185\n",
      "[300] batch loss: 0.181\n",
      "[350] batch loss: 0.190\n",
      "[400] batch loss: 0.188\n",
      "[450] batch loss: 0.164\n",
      "epoch 24 loss: 84.989 -> 83.021\n",
      "\n",
      "[ 50] batch loss: 0.167\n",
      "[100] batch loss: 0.189\n",
      "[150] batch loss: 0.178\n",
      "[200] batch loss: 0.172\n",
      "[250] batch loss: 0.181\n",
      "[300] batch loss: 0.177\n",
      "[350] batch loss: 0.185\n",
      "[400] batch loss: 0.184\n",
      "[450] batch loss: 0.161\n",
      "epoch 25 loss: 83.021 -> 81.213\n",
      "\n",
      "[ 50] batch loss: 0.164\n",
      "[100] batch loss: 0.185\n",
      "[150] batch loss: 0.174\n",
      "[200] batch loss: 0.169\n",
      "[250] batch loss: 0.178\n",
      "[300] batch loss: 0.174\n",
      "[350] batch loss: 0.181\n",
      "[400] batch loss: 0.181\n",
      "[450] batch loss: 0.157\n",
      "epoch 26 loss: 81.213 -> 79.536\n",
      "\n",
      "[ 50] batch loss: 0.161\n",
      "[100] batch loss: 0.181\n",
      "[150] batch loss: 0.171\n",
      "[200] batch loss: 0.165\n",
      "[250] batch loss: 0.174\n",
      "[300] batch loss: 0.170\n",
      "[350] batch loss: 0.177\n",
      "[400] batch loss: 0.177\n",
      "[450] batch loss: 0.154\n",
      "epoch 27 loss: 79.536 -> 77.973\n",
      "\n",
      "[ 50] batch loss: 0.158\n",
      "[100] batch loss: 0.178\n",
      "[150] batch loss: 0.167\n",
      "[200] batch loss: 0.162\n",
      "[250] batch loss: 0.171\n",
      "[300] batch loss: 0.167\n",
      "[350] batch loss: 0.174\n",
      "[400] batch loss: 0.174\n",
      "[450] batch loss: 0.152\n",
      "epoch 28 loss: 77.973 -> 76.515\n",
      "\n",
      "[ 50] batch loss: 0.155\n",
      "[100] batch loss: 0.175\n",
      "[150] batch loss: 0.164\n",
      "[200] batch loss: 0.159\n",
      "[250] batch loss: 0.168\n",
      "[300] batch loss: 0.164\n",
      "[350] batch loss: 0.170\n",
      "[400] batch loss: 0.172\n",
      "[450] batch loss: 0.149\n",
      "epoch 29 loss: 76.515 -> 75.139\n",
      "\n",
      "[ 50] batch loss: 0.152\n",
      "[100] batch loss: 0.172\n",
      "[150] batch loss: 0.161\n",
      "[200] batch loss: 0.157\n",
      "[250] batch loss: 0.166\n",
      "[300] batch loss: 0.161\n",
      "[350] batch loss: 0.167\n",
      "[400] batch loss: 0.169\n",
      "[450] batch loss: 0.147\n",
      "epoch 30 loss: 75.139 -> 73.851\n",
      "\n",
      "[ 50] batch loss: 0.149\n",
      "[100] batch loss: 0.169\n",
      "[150] batch loss: 0.158\n",
      "[200] batch loss: 0.154\n",
      "[250] batch loss: 0.163\n",
      "[300] batch loss: 0.159\n",
      "[350] batch loss: 0.164\n",
      "[400] batch loss: 0.166\n",
      "[450] batch loss: 0.144\n",
      "epoch 31 loss: 73.851 -> 72.629\n",
      "\n",
      "[ 50] batch loss: 0.147\n",
      "[100] batch loss: 0.166\n",
      "[150] batch loss: 0.155\n",
      "[200] batch loss: 0.152\n",
      "[250] batch loss: 0.161\n",
      "[300] batch loss: 0.156\n",
      "[350] batch loss: 0.161\n",
      "[400] batch loss: 0.164\n",
      "[450] batch loss: 0.142\n",
      "epoch 32 loss: 72.629 -> 71.471\n",
      "\n",
      "[ 50] batch loss: 0.145\n",
      "[100] batch loss: 0.164\n",
      "[150] batch loss: 0.153\n",
      "[200] batch loss: 0.149\n",
      "[250] batch loss: 0.158\n",
      "[300] batch loss: 0.154\n",
      "[350] batch loss: 0.159\n",
      "[400] batch loss: 0.162\n",
      "[450] batch loss: 0.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 loss: 71.471 -> 70.375\n",
      "\n",
      "[ 50] batch loss: 0.143\n",
      "[100] batch loss: 0.161\n",
      "[150] batch loss: 0.150\n",
      "[200] batch loss: 0.147\n",
      "[250] batch loss: 0.156\n",
      "[300] batch loss: 0.152\n",
      "[350] batch loss: 0.156\n",
      "[400] batch loss: 0.159\n",
      "[450] batch loss: 0.138\n",
      "epoch 34 loss: 70.375 -> 69.330\n",
      "\n",
      "[ 50] batch loss: 0.141\n",
      "[100] batch loss: 0.159\n",
      "[150] batch loss: 0.148\n",
      "[200] batch loss: 0.145\n",
      "[250] batch loss: 0.154\n",
      "[300] batch loss: 0.150\n",
      "[350] batch loss: 0.154\n",
      "[400] batch loss: 0.157\n",
      "[450] batch loss: 0.136\n",
      "epoch 35 loss: 69.330 -> 68.330\n",
      "\n",
      "[ 50] batch loss: 0.139\n",
      "[100] batch loss: 0.157\n",
      "[150] batch loss: 0.146\n",
      "[200] batch loss: 0.143\n",
      "[250] batch loss: 0.152\n",
      "[300] batch loss: 0.147\n",
      "[350] batch loss: 0.151\n",
      "[400] batch loss: 0.155\n",
      "[450] batch loss: 0.134\n",
      "epoch 36 loss: 68.330 -> 67.378\n",
      "\n",
      "[ 50] batch loss: 0.137\n",
      "[100] batch loss: 0.155\n",
      "[150] batch loss: 0.143\n",
      "[200] batch loss: 0.141\n",
      "[250] batch loss: 0.150\n",
      "[300] batch loss: 0.146\n",
      "[350] batch loss: 0.149\n",
      "[400] batch loss: 0.153\n",
      "[450] batch loss: 0.133\n",
      "epoch 37 loss: 67.378 -> 66.457\n",
      "\n",
      "[ 50] batch loss: 0.135\n",
      "[100] batch loss: 0.153\n",
      "[150] batch loss: 0.141\n",
      "[200] batch loss: 0.139\n",
      "[250] batch loss: 0.148\n",
      "[300] batch loss: 0.144\n",
      "[350] batch loss: 0.147\n",
      "[400] batch loss: 0.151\n",
      "[450] batch loss: 0.131\n",
      "epoch 38 loss: 66.457 -> 65.582\n",
      "\n",
      "[ 50] batch loss: 0.133\n",
      "[100] batch loss: 0.151\n",
      "[150] batch loss: 0.139\n",
      "[200] batch loss: 0.137\n",
      "[250] batch loss: 0.146\n",
      "[300] batch loss: 0.142\n",
      "[350] batch loss: 0.145\n",
      "[400] batch loss: 0.149\n",
      "[450] batch loss: 0.129\n",
      "epoch 39 loss: 65.582 -> 64.739\n",
      "\n",
      "[ 50] batch loss: 0.132\n",
      "[100] batch loss: 0.149\n",
      "[150] batch loss: 0.137\n",
      "[200] batch loss: 0.135\n",
      "[250] batch loss: 0.145\n",
      "[300] batch loss: 0.140\n",
      "[350] batch loss: 0.143\n",
      "[400] batch loss: 0.148\n",
      "[450] batch loss: 0.128\n",
      "epoch 40 loss: 64.739 -> 63.924\n",
      "\n",
      "[ 50] batch loss: 0.130\n",
      "[100] batch loss: 0.147\n",
      "[150] batch loss: 0.136\n",
      "[200] batch loss: 0.134\n",
      "[250] batch loss: 0.143\n",
      "[300] batch loss: 0.139\n",
      "[350] batch loss: 0.141\n",
      "[400] batch loss: 0.146\n",
      "[450] batch loss: 0.126\n",
      "epoch 41 loss: 63.924 -> 63.141\n",
      "\n",
      "[ 50] batch loss: 0.129\n",
      "[100] batch loss: 0.146\n",
      "[150] batch loss: 0.134\n",
      "[200] batch loss: 0.132\n",
      "[250] batch loss: 0.141\n",
      "[300] batch loss: 0.137\n",
      "[350] batch loss: 0.139\n",
      "[400] batch loss: 0.144\n",
      "[450] batch loss: 0.125\n",
      "epoch 42 loss: 63.141 -> 62.388\n",
      "\n",
      "[ 50] batch loss: 0.127\n",
      "[100] batch loss: 0.144\n",
      "[150] batch loss: 0.132\n",
      "[200] batch loss: 0.130\n",
      "[250] batch loss: 0.140\n",
      "[300] batch loss: 0.135\n",
      "[350] batch loss: 0.138\n",
      "[400] batch loss: 0.143\n",
      "[450] batch loss: 0.123\n",
      "epoch 43 loss: 62.388 -> 61.662\n",
      "\n",
      "[ 50] batch loss: 0.126\n",
      "[100] batch loss: 0.142\n",
      "[150] batch loss: 0.130\n",
      "[200] batch loss: 0.129\n",
      "[250] batch loss: 0.138\n",
      "[300] batch loss: 0.134\n",
      "[350] batch loss: 0.136\n",
      "[400] batch loss: 0.141\n",
      "[450] batch loss: 0.122\n",
      "epoch 44 loss: 61.662 -> 60.954\n",
      "\n",
      "[ 50] batch loss: 0.124\n",
      "[100] batch loss: 0.141\n",
      "[150] batch loss: 0.129\n",
      "[200] batch loss: 0.127\n",
      "[250] batch loss: 0.137\n",
      "[300] batch loss: 0.132\n",
      "[350] batch loss: 0.134\n",
      "[400] batch loss: 0.139\n",
      "[450] batch loss: 0.120\n",
      "epoch 45 loss: 60.954 -> 60.278\n",
      "\n",
      "[ 50] batch loss: 0.123\n",
      "[100] batch loss: 0.139\n",
      "[150] batch loss: 0.127\n",
      "[200] batch loss: 0.126\n",
      "[250] batch loss: 0.135\n",
      "[300] batch loss: 0.131\n",
      "[350] batch loss: 0.133\n",
      "[400] batch loss: 0.138\n",
      "[450] batch loss: 0.119\n",
      "epoch 46 loss: 60.278 -> 59.622\n",
      "\n",
      "[ 50] batch loss: 0.122\n",
      "[100] batch loss: 0.138\n",
      "[150] batch loss: 0.126\n",
      "[200] batch loss: 0.124\n",
      "[250] batch loss: 0.134\n",
      "[300] batch loss: 0.130\n",
      "[350] batch loss: 0.131\n",
      "[400] batch loss: 0.137\n",
      "[450] batch loss: 0.118\n",
      "epoch 47 loss: 59.622 -> 58.985\n",
      "\n",
      "[ 50] batch loss: 0.121\n",
      "[100] batch loss: 0.137\n",
      "[150] batch loss: 0.124\n",
      "[200] batch loss: 0.123\n",
      "[250] batch loss: 0.132\n",
      "[300] batch loss: 0.128\n",
      "[350] batch loss: 0.130\n",
      "[400] batch loss: 0.135\n",
      "[450] batch loss: 0.117\n",
      "epoch 48 loss: 58.985 -> 58.372\n",
      "\n",
      "[ 50] batch loss: 0.119\n",
      "[100] batch loss: 0.135\n",
      "[150] batch loss: 0.123\n",
      "[200] batch loss: 0.121\n",
      "[250] batch loss: 0.131\n",
      "[300] batch loss: 0.127\n",
      "[350] batch loss: 0.129\n",
      "[400] batch loss: 0.134\n",
      "[450] batch loss: 0.116\n",
      "epoch 49 loss: 58.372 -> 57.772\n",
      "\n",
      "[ 50] batch loss: 0.118\n",
      "[100] batch loss: 0.134\n",
      "[150] batch loss: 0.122\n",
      "[200] batch loss: 0.120\n",
      "[250] batch loss: 0.130\n",
      "[300] batch loss: 0.126\n",
      "[350] batch loss: 0.127\n",
      "[400] batch loss: 0.133\n",
      "[450] batch loss: 0.114\n",
      "epoch 50 loss: 57.772 -> 57.191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%skip $load_model_m\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 50\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_m):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_m(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%3d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        prev_loss = epoch_loss\n",
    "        break\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model_m\n",
    "\n",
    "torch.save(cnn_m.state_dict(), para_file_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST-M test set: 0.9513333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on MNIST-M test set (train on target): \" + str(evaluate_accuracy(cnn_m, testloader_m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation\n",
    "\n",
    "## Structure (GRL)\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import ST_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainset_da = ST_Dataset(trainset, trainset_m, batch_size)\n",
    "trainloader_da = torch.utils.data.DataLoader(trainset_da, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 105th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABBpJREFUeJztmk9oHFUcxz+7HcZtjMu6LkGDSpGo2FZrjCDVlEottIgH68H2JKX1VIoIIhSxFC8lVZBSBC+eBS9FehAPBSlCkPqnFRURLS0almVJlzgZp+MyzHr4vUnYXtxC9reZ6e9zeTubeZPffvPZl5n3XqnX62HoUB51AbcTFrYiFrYiFrYiFrYiFrYiFrYiFrYiFrYinuYvK5VKhX1c7fV6pf87x8xWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWRHU+e1i8sGM/AC/vfcO9UyZJuiuvAc6d/wiAr77+TLm6VcxsRUqae/3WaqVm2+PPAHDk4Om+98fGxgBoNBok3QQAv+ID0HXHpCkAURwCsP/wlrUoyVZq1hu5HLPPzJ1zr8SVIBBLoygC4M139tJsXQPgya2zABw59D4AExMNAGr1SaVqVzGzFcmV2dtn9gEQhnKnETuToygG4OixXQD8vXx9pU+tKgZ3FpdcX+lT8SsKFfdjZiuSK7Nf3PU6AJd/+BkAz5M7jTOfHAT6jd4+Ld+C3bOHAAgCsT/pRK7v8Ou9GTNbkVyZXamMAxDFYufc6VcBWP7net95e3Ye5tmnDwCw8GcLWL3PHh+Xa3heOvyCbyJXYY9XawB4vjy8VN3xI1NPAfDKS8cASBOP2P1BOqGE/eD9W+UaLuy3TswoVb2KDSOK5Mrsb777HIDdz78GwMl3v+z7eRzLUBEEHT49OwfAr79fAODjD34BwPNG55eZrUguJ6JmpncCUHaulFdslXZxscWVqz8BcOq4mO25ez03D8Xb7z23FqWsYBNR64xcjdkZ31+6MPC5nU4HAN/3h1XOwJjZiuTS7Fshe6QnFa9S9B9mMsxsRQpvdtcZHccyEfXF+ZMjq8XMVqTwZrc7smiQbW24cvXbkdViZitSWLP37TkFQDkRn+bnPxxlOYCZrUphza5UqgAEYQBAGIWjLAcws1UprNntoA1AmsqqThiurdkbXDt13+B9zGxFCmv2xcsXAfB8GbsTklvqf6drG3dL63atUZPLkX1R6rXBr2lmK1Ios6cflXZ2dhvXFuQupN36AwBvUj5qNqudOtE3PXSPnLcg2yHG/I0ALC3dACBx57lpcdquLTtNm0uD12dmK1Iosy/9Ju1i+0fqdXmdfcDNTzwMQLcrbjebfwHg9sTTakpbrYjRmbn12h1ynP4LQLbgU6/fBUAQLQ9cn5mtSC5X14fBjsekndr0AAD3TspW44mG3IakbpAPA/e/oC07rZotac/OR7a6vp5QNft2x8xWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJWxMJW5D86NgQh8i1oQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "I don't know :)\n",
      "\n",
      "From domain:\n",
      "Target\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from test set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_da):\n",
    "    inputs, labels, domain = data\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\")\n",
    "if labels[idx].item() == -1:\n",
    "    print(\"I don't know :)\\n\")\n",
    "else:\n",
    "    print(str(labels[idx].item()) + \"\\n\")\n",
    "\n",
    "print(\"From domain:\")\n",
    "if domain[idx].item() == 0:\n",
    "    print(\"Source\")\n",
    "else:\n",
    "    print(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, lamda):\n",
    "        ctx.save_for_backward(lamda)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        lamda, = ctx.saved_tensors\n",
    "        return -lamda * grad_outputs, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init):\n",
    "        super(GRL, self).__init__()\n",
    "        self.GRL_func = GRL_func.apply\n",
    "        self.lamda = nn.Parameter(torch.Tensor(1), requires_grad=False)\n",
    "        self.set_lamda(lamda_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.GRL_func(x, self.lamda)\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.lamda[0] = lamda_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_DA(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init=0):\n",
    "        super(CNN_DA, self).__init__()\n",
    "        # lamda\n",
    "        self.lamda = lamda_init\n",
    "        # feature extractor\n",
    "        self.C1 = nn.Conv2d(3, 32, 5)\n",
    "        self.C2 = nn.Conv2d(32, 48, 5)\n",
    "        # label classifier\n",
    "        self.LC_FC1 = nn.Linear(48 * 4 * 4, 100)\n",
    "        self.LC_FC2 = nn.Linear(100, 100)\n",
    "        self.LC_FC3 = nn.Linear(100, 10)\n",
    "        # domain classifier\n",
    "        self.GRL_layer = GRL(lamda_init)\n",
    "        self.DC_FC1 = nn.Linear(48 * 4 * 4, 100)\n",
    "        self.DC_FC2 = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # x's size is (128, 48, 4, 4)\n",
    "        # flatten\n",
    "        x = x.view(-1, 48 * 4 * 4)\n",
    "        # label classifier\n",
    "        # LC_FC1\n",
    "        x_l = F.relu(self.LC_FC1(x))\n",
    "        # LC_FC2\n",
    "        x_l = F.relu(self.LC_FC2(x_l))\n",
    "        # LC_FC3\n",
    "        x_l = self.LC_FC3(x_l)\n",
    "        # domain classifier\n",
    "        # GRL\n",
    "        x_d = self.GRL_layer(x)\n",
    "        # DC_FC1\n",
    "        x_d = F.relu(self.DC_FC1(x_d))\n",
    "        # DC_FC2\n",
    "        x_d = F.sigmoid(self.DC_FC2(x_d))\n",
    "        return x_l, x_d\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.GRL_layer.set_lamda(lamda_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_DA(\n",
      "  (C1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (C2): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (LC_FC1): Linear(in_features=768, out_features=100, bias=True)\n",
      "  (LC_FC2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (LC_FC3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (GRL_layer): GRL()\n",
      "  (DC_FC1): Linear(in_features=768, out_features=100, bias=True)\n",
      "  (DC_FC2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_da = CNN_DA(0)\n",
    "if (use_gpu):\n",
    "    cnn_da.cuda()\n",
    "print(cnn_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_da = False\n"
     ]
    }
   ],
   "source": [
    "para_file_da = \"./cnn_mnist_to_mnist_m\"\n",
    "load_model_da = os.path.isfile(para_file_da)\n",
    "print(\"load_model_da = \" + str(load_model_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_da)\n",
    "cnn.load_state_dict(torch.load(para_file_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "criterion_DC = nn.BCELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "        \n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50] batch loss: 2.837\n",
      "[100] batch loss: 1.463\n",
      "[150] batch loss: 0.734\n",
      "[200] batch loss: 0.475\n",
      "[250] batch loss: 0.443\n",
      "[300] batch loss: 0.368\n",
      "[350] batch loss: 0.267\n",
      "[400] batch loss: 0.334\n",
      "[450] batch loss: 0.256\n",
      "[500] batch loss: 0.298\n",
      "[550] batch loss: 0.274\n",
      "[600] batch loss: 0.259\n",
      "[650] batch loss: 0.209\n",
      "[700] batch loss: 0.185\n",
      "[750] batch loss: 0.219\n",
      "[800] batch loss: 0.201\n",
      "[850] batch loss: 0.170\n",
      "[900] batch loss: 0.144\n",
      "epoch 1 loss: inf -> 459.455\n",
      "\n",
      "[ 50] batch loss: 0.392\n",
      "[100] batch loss: 0.257\n",
      "[150] batch loss: 0.289\n",
      "[200] batch loss: 0.347\n",
      "[250] batch loss: 0.226\n",
      "[300] batch loss: 0.190\n",
      "[350] batch loss: 0.167\n",
      "[400] batch loss: 0.159\n",
      "[450] batch loss: 0.172\n",
      "[500] batch loss: 0.165\n",
      "[550] batch loss: 0.160\n",
      "[600] batch loss: 0.165\n",
      "[650] batch loss: 0.158\n",
      "[700] batch loss: 0.150\n",
      "[750] batch loss: 0.166\n",
      "[800] batch loss: 0.147\n",
      "[850] batch loss: 0.149\n",
      "[900] batch loss: 0.128\n",
      "epoch 2 loss: 459.455 -> 181.733\n",
      "\n",
      "[ 50] batch loss: 0.149\n",
      "[100] batch loss: 0.142\n",
      "[150] batch loss: 0.139\n",
      "[200] batch loss: 0.138\n",
      "[250] batch loss: 0.141\n",
      "[300] batch loss: 0.135\n",
      "[350] batch loss: 0.122\n",
      "[400] batch loss: 0.124\n",
      "[450] batch loss: 0.141\n",
      "[500] batch loss: 0.142\n",
      "[550] batch loss: 0.140\n",
      "[600] batch loss: 0.164\n",
      "[650] batch loss: 0.157\n",
      "[700] batch loss: 0.150\n",
      "[750] batch loss: 0.165\n",
      "[800] batch loss: 0.140\n",
      "[850] batch loss: 0.146\n",
      "[900] batch loss: 0.148\n",
      "epoch 3 loss: 181.733 -> 132.150\n",
      "\n",
      "[ 50] batch loss: 0.179\n",
      "[100] batch loss: 0.190\n",
      "[150] batch loss: 0.236\n",
      "[200] batch loss: 0.209\n",
      "[250] batch loss: 0.218\n",
      "[300] batch loss: 0.234\n",
      "[350] batch loss: 0.198\n",
      "[400] batch loss: 0.232\n",
      "[450] batch loss: 0.274\n",
      "[500] batch loss: 0.281\n",
      "[550] batch loss: 0.276\n",
      "[600] batch loss: 0.278\n",
      "[650] batch loss: 0.275\n",
      "[700] batch loss: 0.276\n",
      "[750] batch loss: 0.349\n",
      "[800] batch loss: 0.274\n",
      "[850] batch loss: 0.276\n",
      "[900] batch loss: 0.366\n",
      "epoch 4 loss: 132.150 -> 236.654\n",
      "\n",
      "[ 50] batch loss: 0.276\n",
      "[100] batch loss: 0.282\n",
      "[150] batch loss: 0.261\n",
      "[200] batch loss: 0.273\n",
      "[250] batch loss: 0.332\n",
      "[300] batch loss: 0.407\n",
      "[350] batch loss: 0.367\n",
      "[400] batch loss: 0.403\n",
      "[450] batch loss: 0.478\n",
      "[500] batch loss: 0.435\n",
      "[550] batch loss: 0.489\n",
      "[600] batch loss: 0.443\n",
      "[650] batch loss: 0.492\n",
      "[700] batch loss: 0.474\n",
      "[750] batch loss: 0.509\n",
      "[800] batch loss: 0.487\n",
      "[850] batch loss: 0.471\n",
      "[900] batch loss: 0.431\n",
      "epoch 5 loss: 236.654 -> 375.485\n",
      "\n",
      "[ 50] batch loss: 0.460\n",
      "[100] batch loss: 0.535\n",
      "[150] batch loss: 0.502\n",
      "[200] batch loss: 0.496\n",
      "[250] batch loss: 0.450\n",
      "[300] batch loss: 0.535\n",
      "[350] batch loss: 0.553\n",
      "[400] batch loss: 0.463\n",
      "[450] batch loss: 0.480\n",
      "[500] batch loss: 0.508\n",
      "[550] batch loss: 0.390\n",
      "[600] batch loss: 0.465\n",
      "[650] batch loss: 0.526\n",
      "[700] batch loss: 0.525\n",
      "[750] batch loss: 0.532\n",
      "[800] batch loss: 0.599\n",
      "[850] batch loss: 0.637\n",
      "[900] batch loss: 0.616\n",
      "epoch 6 loss: 375.485 -> 475.188\n",
      "\n",
      "[ 50] batch loss: 0.611\n",
      "[100] batch loss: 0.652\n",
      "[150] batch loss: 0.678\n",
      "[200] batch loss: 0.523\n",
      "[250] batch loss: 0.557\n",
      "[300] batch loss: 0.567\n",
      "[350] batch loss: 0.685\n",
      "[400] batch loss: 0.656\n",
      "[450] batch loss: 0.678\n",
      "[500] batch loss: 0.611\n",
      "[550] batch loss: 0.700\n",
      "[600] batch loss: 0.623\n",
      "[650] batch loss: 0.766\n",
      "[700] batch loss: 0.614\n",
      "[750] batch loss: 0.644\n",
      "[800] batch loss: 0.711\n",
      "[850] batch loss: 0.619\n",
      "[900] batch loss: 0.568\n",
      "epoch 7 loss: 475.188 -> 585.082\n",
      "\n",
      "[ 50] batch loss: 0.756\n",
      "[100] batch loss: 0.746\n",
      "[150] batch loss: 0.715\n",
      "[200] batch loss: 0.651\n",
      "[250] batch loss: 0.693\n",
      "[300] batch loss: 0.696\n",
      "[350] batch loss: 0.674\n",
      "[400] batch loss: 0.616\n",
      "[450] batch loss: 0.659\n",
      "[500] batch loss: 0.672\n",
      "[550] batch loss: 0.605\n",
      "[600] batch loss: 0.740\n",
      "[650] batch loss: 0.622\n",
      "[700] batch loss: 0.661\n",
      "[750] batch loss: 0.706\n",
      "[800] batch loss: 0.681\n",
      "[850] batch loss: 0.664\n",
      "[900] batch loss: 0.668\n",
      "epoch 8 loss: 585.082 -> 626.055\n",
      "\n",
      "[ 50] batch loss: 0.670\n",
      "[100] batch loss: 0.701\n",
      "[150] batch loss: 0.667\n",
      "[200] batch loss: 0.665\n",
      "[250] batch loss: 0.628\n",
      "[300] batch loss: 0.666\n",
      "[350] batch loss: 0.628\n",
      "[400] batch loss: 0.603\n",
      "[450] batch loss: 0.675\n",
      "[500] batch loss: 0.614\n",
      "[550] batch loss: 0.646\n",
      "[600] batch loss: 0.637\n",
      "[650] batch loss: 0.622\n",
      "[700] batch loss: 0.626\n",
      "[750] batch loss: 0.677\n",
      "[800] batch loss: 0.666\n",
      "[850] batch loss: 0.633\n",
      "[900] batch loss: 0.610\n",
      "epoch 9 loss: 626.055 -> 595.888\n",
      "\n",
      "[ 50] batch loss: 0.628\n",
      "[100] batch loss: 0.650\n",
      "[150] batch loss: 0.605\n",
      "[200] batch loss: 0.647\n",
      "[250] batch loss: 0.627\n",
      "[300] batch loss: 0.617\n",
      "[350] batch loss: 0.620\n",
      "[400] batch loss: 0.604\n",
      "[450] batch loss: 0.604\n",
      "[500] batch loss: 0.634\n",
      "[550] batch loss: 0.628\n",
      "[600] batch loss: 0.630\n",
      "[650] batch loss: 0.638\n",
      "[700] batch loss: 0.710\n",
      "[750] batch loss: 0.584\n",
      "[800] batch loss: 0.566\n",
      "[850] batch loss: 0.589\n",
      "[900] batch loss: 0.676\n",
      "epoch 10 loss: 595.888 -> 577.788\n",
      "\n",
      "[ 50] batch loss: 0.625\n",
      "[100] batch loss: 0.606\n",
      "[150] batch loss: 0.675\n",
      "[200] batch loss: 0.622\n",
      "[250] batch loss: 0.649\n",
      "[300] batch loss: 0.646\n",
      "[350] batch loss: 0.656\n",
      "[400] batch loss: 0.605\n",
      "[450] batch loss: 0.642\n",
      "[500] batch loss: 0.634\n",
      "[550] batch loss: 0.639\n",
      "[600] batch loss: 0.675\n",
      "[650] batch loss: 0.672\n",
      "[700] batch loss: 0.636\n",
      "[750] batch loss: 0.675\n",
      "[800] batch loss: 0.632\n",
      "[850] batch loss: 0.655\n",
      "[900] batch loss: 0.673\n",
      "epoch 11 loss: 577.788 -> 595.137\n",
      "\n",
      "[ 50] batch loss: 0.673\n",
      "[100] batch loss: 0.624\n",
      "[150] batch loss: 0.639\n",
      "[200] batch loss: 0.661\n",
      "[250] batch loss: 0.650\n",
      "[300] batch loss: 0.656\n",
      "[350] batch loss: 0.636\n",
      "[400] batch loss: 0.663\n",
      "[450] batch loss: 0.654\n",
      "[500] batch loss: 0.650\n",
      "[550] batch loss: 0.646\n",
      "[600] batch loss: 0.658\n",
      "[650] batch loss: 0.660\n",
      "[700] batch loss: 0.659\n",
      "[750] batch loss: 0.658\n",
      "[800] batch loss: 0.626\n",
      "[850] batch loss: 0.676\n",
      "[900] batch loss: 0.631\n",
      "epoch 12 loss: 595.137 -> 599.056\n",
      "\n",
      "[ 50] batch loss: 0.657\n",
      "[100] batch loss: 0.659\n",
      "[150] batch loss: 0.657\n",
      "[200] batch loss: 0.663\n",
      "[250] batch loss: 0.638\n",
      "[300] batch loss: 0.645\n",
      "[350] batch loss: 0.637\n",
      "[400] batch loss: 0.645\n",
      "[450] batch loss: 0.673\n",
      "[500] batch loss: 0.658\n",
      "[550] batch loss: 0.653\n",
      "[600] batch loss: 0.674\n",
      "[650] batch loss: 0.651\n",
      "[700] batch loss: 0.637\n",
      "[750] batch loss: 0.651\n",
      "[800] batch loss: 0.618\n",
      "[850] batch loss: 0.647\n",
      "[900] batch loss: 0.635\n",
      "epoch 13 loss: 599.056 -> 598.882\n",
      "\n",
      "[ 50] batch loss: 0.648\n",
      "[100] batch loss: 0.618\n",
      "[150] batch loss: 0.627\n",
      "[200] batch loss: 0.632\n",
      "[250] batch loss: 0.663\n",
      "[300] batch loss: 0.635\n",
      "[350] batch loss: 0.666\n",
      "[400] batch loss: 0.618\n",
      "[450] batch loss: 0.700\n",
      "[500] batch loss: 0.642\n",
      "[550] batch loss: 0.658\n",
      "[600] batch loss: 0.657\n",
      "[650] batch loss: 0.632\n",
      "[700] batch loss: 0.634\n",
      "[750] batch loss: 0.667\n",
      "[800] batch loss: 0.638\n",
      "[850] batch loss: 0.664\n",
      "[900] batch loss: 0.745\n",
      "epoch 14 loss: 598.882 -> 599.998\n",
      "\n",
      "[ 50] batch loss: 0.671\n",
      "[100] batch loss: 0.669\n",
      "[150] batch loss: 0.658\n",
      "[200] batch loss: 0.673\n",
      "[250] batch loss: 0.699\n",
      "[300] batch loss: 0.686\n",
      "[350] batch loss: 0.662\n",
      "[400] batch loss: 0.665\n",
      "[450] batch loss: 0.675\n",
      "[500] batch loss: 0.678\n",
      "[550] batch loss: 0.679\n",
      "[600] batch loss: 0.685\n",
      "[650] batch loss: 0.697\n",
      "[700] batch loss: 0.688\n",
      "[750] batch loss: 0.689\n",
      "[800] batch loss: 0.642\n",
      "[850] batch loss: 0.675\n",
      "[900] batch loss: 0.649\n",
      "epoch 15 loss: 599.998 -> 621.519\n",
      "\n",
      "[ 50] batch loss: 0.678\n",
      "[100] batch loss: 0.674\n",
      "[150] batch loss: 0.646\n",
      "[200] batch loss: 0.702\n",
      "[250] batch loss: 0.689\n",
      "[300] batch loss: 0.680\n",
      "[350] batch loss: 0.645\n",
      "[400] batch loss: 0.686\n",
      "[450] batch loss: 0.666\n",
      "[500] batch loss: 0.694\n",
      "[550] batch loss: 0.719\n",
      "[600] batch loss: 0.689\n",
      "[650] batch loss: 0.682\n",
      "[700] batch loss: 0.693\n",
      "[750] batch loss: 0.670\n",
      "[800] batch loss: 0.666\n",
      "[850] batch loss: 0.701\n",
      "[900] batch loss: 0.697\n",
      "epoch 16 loss: 621.519 -> 627.734\n",
      "\n",
      "[ 50] batch loss: 0.674\n",
      "[100] batch loss: 0.690\n",
      "[150] batch loss: 0.673\n",
      "[200] batch loss: 0.680\n",
      "[250] batch loss: 0.661\n",
      "[300] batch loss: 0.663\n",
      "[350] batch loss: 0.675\n",
      "[400] batch loss: 0.662\n",
      "[450] batch loss: 0.658\n",
      "[500] batch loss: 0.672\n",
      "[550] batch loss: 0.665\n",
      "[600] batch loss: 0.665\n",
      "[650] batch loss: 0.672\n",
      "[700] batch loss: 0.660\n",
      "[750] batch loss: 0.679\n",
      "[800] batch loss: 0.649\n",
      "[850] batch loss: 0.672\n",
      "[900] batch loss: 0.673\n",
      "epoch 17 loss: 627.734 -> 615.709\n",
      "\n",
      "[ 50] batch loss: 0.651\n",
      "[100] batch loss: 0.694\n",
      "[150] batch loss: 0.658\n",
      "[200] batch loss: 0.686\n",
      "[250] batch loss: 0.651\n",
      "[300] batch loss: 0.676\n",
      "[350] batch loss: 0.629\n",
      "[400] batch loss: 0.665\n",
      "[450] batch loss: 0.704\n",
      "[500] batch loss: 0.675\n",
      "[550] batch loss: 0.686\n",
      "[600] batch loss: 0.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650] batch loss: 0.680\n",
      "[700] batch loss: 0.661\n",
      "[750] batch loss: 0.690\n",
      "[800] batch loss: 0.639\n",
      "[850] batch loss: 0.669\n",
      "[900] batch loss: 0.653\n",
      "epoch 18 loss: 615.709 -> 616.432\n",
      "\n",
      "[ 50] batch loss: 0.688\n",
      "[100] batch loss: 0.652\n",
      "[150] batch loss: 0.679\n",
      "[200] batch loss: 0.650\n",
      "[250] batch loss: 0.664\n",
      "[300] batch loss: 0.662\n",
      "[350] batch loss: 0.645\n",
      "[400] batch loss: 0.660\n",
      "[450] batch loss: 0.663\n",
      "[500] batch loss: 0.655\n",
      "[550] batch loss: 0.667\n",
      "[600] batch loss: 0.661\n",
      "[650] batch loss: 0.645\n",
      "[700] batch loss: 0.637\n",
      "[750] batch loss: 0.743\n",
      "[800] batch loss: 0.629\n",
      "[850] batch loss: 0.664\n",
      "[900] batch loss: 0.680\n",
      "epoch 19 loss: 616.432 -> 611.634\n",
      "\n",
      "[ 50] batch loss: 0.668\n",
      "[100] batch loss: 0.686\n",
      "[150] batch loss: 0.666\n",
      "[200] batch loss: 0.659\n",
      "[250] batch loss: 0.655\n",
      "[300] batch loss: 0.651\n",
      "[350] batch loss: 0.637\n",
      "[400] batch loss: 0.663\n",
      "[450] batch loss: 0.669\n",
      "[500] batch loss: 0.669\n",
      "[550] batch loss: 0.661\n",
      "[600] batch loss: 0.667\n",
      "[650] batch loss: 0.647\n",
      "[700] batch loss: 0.629\n",
      "[750] batch loss: 0.662\n",
      "[800] batch loss: 0.667\n",
      "[850] batch loss: 0.648\n",
      "[900] batch loss: 0.653\n",
      "epoch 20 loss: 611.634 -> 607.019\n",
      "\n",
      "[ 50] batch loss: 0.659\n",
      "[100] batch loss: 0.656\n",
      "[150] batch loss: 0.650\n",
      "[200] batch loss: 0.637\n",
      "[250] batch loss: 0.642\n",
      "[300] batch loss: 0.643\n",
      "[350] batch loss: 0.648\n",
      "[400] batch loss: 0.655\n",
      "[450] batch loss: 0.646\n",
      "[500] batch loss: 0.652\n",
      "[550] batch loss: 0.662\n",
      "[600] batch loss: 0.653\n",
      "[650] batch loss: 0.623\n",
      "[700] batch loss: 0.625\n",
      "[750] batch loss: 0.655\n",
      "[800] batch loss: 0.637\n",
      "[850] batch loss: 0.638\n",
      "[900] batch loss: 0.616\n",
      "epoch 21 loss: 607.019 -> 593.469\n",
      "\n",
      "[ 50] batch loss: 0.658\n",
      "[100] batch loss: 0.649\n",
      "[150] batch loss: 0.641\n",
      "[200] batch loss: 0.643\n",
      "[250] batch loss: 0.671\n",
      "[300] batch loss: 0.596\n",
      "[350] batch loss: 0.648\n",
      "[400] batch loss: 0.639\n",
      "[450] batch loss: 0.637\n",
      "[500] batch loss: 0.642\n",
      "[550] batch loss: 0.651\n",
      "[600] batch loss: 0.650\n",
      "[650] batch loss: 0.656\n",
      "[700] batch loss: 0.654\n",
      "[750] batch loss: 0.673\n",
      "[800] batch loss: 0.655\n",
      "[850] batch loss: 0.650\n",
      "[900] batch loss: 0.641\n",
      "epoch 22 loss: 593.469 -> 597.002\n",
      "\n",
      "[ 50] batch loss: 0.658\n",
      "[100] batch loss: 0.654\n",
      "[150] batch loss: 0.658\n",
      "[200] batch loss: 0.679\n",
      "[250] batch loss: 0.646\n",
      "[300] batch loss: 0.665\n",
      "[350] batch loss: 0.647\n",
      "[400] batch loss: 0.662\n",
      "[450] batch loss: 0.646\n",
      "[500] batch loss: 0.675\n",
      "[550] batch loss: 0.650\n",
      "[600] batch loss: 0.675\n",
      "[650] batch loss: 0.639\n",
      "[700] batch loss: 0.618\n",
      "[750] batch loss: 0.664\n",
      "[800] batch loss: 0.663\n",
      "[850] batch loss: 0.662\n",
      "[900] batch loss: 0.631\n",
      "epoch 23 loss: 597.002 -> 603.497\n",
      "\n",
      "[ 50] batch loss: 0.646\n",
      "[100] batch loss: 0.660\n",
      "[150] batch loss: 0.654\n",
      "[200] batch loss: 0.654\n",
      "[250] batch loss: 0.677\n",
      "[300] batch loss: 0.662\n",
      "[350] batch loss: 0.655\n",
      "[400] batch loss: 0.674\n",
      "[450] batch loss: 0.654\n",
      "[500] batch loss: 0.663\n",
      "[550] batch loss: 0.632\n",
      "[600] batch loss: 0.633\n",
      "[650] batch loss: 0.643\n",
      "[700] batch loss: 0.641\n",
      "[750] batch loss: 0.698\n",
      "[800] batch loss: 0.669\n",
      "[850] batch loss: 0.663\n",
      "[900] batch loss: 0.623\n",
      "epoch 24 loss: 603.497 -> 603.744\n",
      "\n",
      "[ 50] batch loss: 0.627\n",
      "[100] batch loss: 0.660\n",
      "[150] batch loss: 0.639\n",
      "[200] batch loss: 0.649\n",
      "[250] batch loss: 0.647\n",
      "[300] batch loss: 0.650\n",
      "[350] batch loss: 0.650\n",
      "[400] batch loss: 0.655\n",
      "[450] batch loss: 0.640\n",
      "[500] batch loss: 0.677\n",
      "[550] batch loss: 0.640\n",
      "[600] batch loss: 0.654\n",
      "[650] batch loss: 0.636\n",
      "[700] batch loss: 0.626\n",
      "[750] batch loss: 0.654\n",
      "[800] batch loss: 0.629\n",
      "[850] batch loss: 0.637\n",
      "[900] batch loss: 0.646\n",
      "epoch 25 loss: 603.744 -> 594.894\n",
      "\n",
      "[ 50] batch loss: 0.631\n",
      "[100] batch loss: 0.655\n",
      "[150] batch loss: 0.637\n",
      "[200] batch loss: 0.644\n",
      "[250] batch loss: 0.657\n",
      "[300] batch loss: 0.637\n",
      "[350] batch loss: 0.639\n",
      "[400] batch loss: 0.640\n",
      "[450] batch loss: 0.639\n",
      "[500] batch loss: 0.667\n",
      "[550] batch loss: 0.648\n",
      "[600] batch loss: 0.641\n",
      "[650] batch loss: 0.626\n",
      "[700] batch loss: 0.633\n",
      "[750] batch loss: 0.653\n",
      "[800] batch loss: 0.634\n",
      "[850] batch loss: 0.642\n",
      "[900] batch loss: 0.618\n",
      "epoch 26 loss: 594.894 -> 590.289\n",
      "\n",
      "[ 50] batch loss: 0.631\n",
      "[100] batch loss: 0.629\n",
      "[150] batch loss: 0.639\n",
      "[200] batch loss: 0.646\n",
      "[250] batch loss: 0.673\n",
      "[300] batch loss: 0.660\n",
      "[350] batch loss: 0.654\n",
      "[400] batch loss: 0.663\n",
      "[450] batch loss: 0.666\n",
      "[500] batch loss: 0.663\n",
      "[550] batch loss: 0.629\n",
      "[600] batch loss: 0.676\n",
      "[650] batch loss: 0.655\n",
      "[700] batch loss: 0.639\n",
      "[750] batch loss: 0.663\n",
      "[800] batch loss: 0.644\n",
      "[850] batch loss: 0.648\n",
      "[900] batch loss: 0.635\n",
      "epoch 27 loss: 590.289 -> 599.671\n",
      "\n",
      "[ 50] batch loss: 0.651\n",
      "[100] batch loss: 0.637\n",
      "[150] batch loss: 0.642\n",
      "[200] batch loss: 0.630\n",
      "[250] batch loss: 0.655\n",
      "[300] batch loss: 0.630\n",
      "[350] batch loss: 0.638\n",
      "[400] batch loss: 0.646\n",
      "[450] batch loss: 0.642\n",
      "[500] batch loss: 0.647\n",
      "[550] batch loss: 0.645\n",
      "[600] batch loss: 0.644\n",
      "[650] batch loss: 0.657\n",
      "[700] batch loss: 0.632\n",
      "[750] batch loss: 0.660\n",
      "[800] batch loss: 0.650\n",
      "[850] batch loss: 0.654\n",
      "[900] batch loss: 0.626\n",
      "epoch 28 loss: 599.671 -> 593.092\n",
      "\n",
      "[ 50] batch loss: 0.664\n",
      "[100] batch loss: 0.639\n",
      "[150] batch loss: 0.651\n",
      "[200] batch loss: 0.643\n",
      "[250] batch loss: 0.655\n",
      "[300] batch loss: 0.648\n",
      "[350] batch loss: 0.648\n",
      "[400] batch loss: 0.633\n",
      "[450] batch loss: 0.640\n",
      "[500] batch loss: 0.647\n",
      "[550] batch loss: 0.629\n",
      "[600] batch loss: 0.661\n",
      "[650] batch loss: 0.622\n",
      "[700] batch loss: 0.679\n",
      "[750] batch loss: 0.662\n",
      "[800] batch loss: 0.651\n",
      "[850] batch loss: 0.639\n",
      "[900] batch loss: 0.634\n",
      "epoch 29 loss: 593.092 -> 595.685\n",
      "\n",
      "[ 50] batch loss: 0.680\n",
      "[100] batch loss: 0.650\n",
      "[150] batch loss: 0.662\n",
      "[200] batch loss: 0.656\n",
      "[250] batch loss: 0.636\n",
      "[300] batch loss: 0.633\n",
      "[350] batch loss: 0.647\n",
      "[400] batch loss: 0.656\n",
      "[450] batch loss: 0.661\n",
      "[500] batch loss: 0.645\n",
      "[550] batch loss: 0.630\n",
      "[600] batch loss: 0.657\n",
      "[650] batch loss: 0.658\n",
      "[700] batch loss: 0.646\n",
      "[750] batch loss: 0.666\n",
      "[800] batch loss: 0.632\n",
      "[850] batch loss: 0.675\n",
      "[900] batch loss: 0.638\n",
      "epoch 30 loss: 595.685 -> 600.788\n",
      "\n",
      "[ 50] batch loss: 0.660\n",
      "[100] batch loss: 0.665\n",
      "[150] batch loss: 0.685\n",
      "[200] batch loss: 0.664\n",
      "[250] batch loss: 0.679\n",
      "[300] batch loss: 0.673\n",
      "[350] batch loss: 0.653\n",
      "[400] batch loss: 0.661\n",
      "[450] batch loss: 0.654\n",
      "[500] batch loss: 0.661\n",
      "[550] batch loss: 0.679\n",
      "[600] batch loss: 0.685\n",
      "[650] batch loss: 0.705\n",
      "[700] batch loss: 0.676\n",
      "[750] batch loss: 0.679\n",
      "[800] batch loss: 0.661\n",
      "[850] batch loss: 0.637\n",
      "[900] batch loss: 0.671\n",
      "epoch 31 loss: 600.788 -> 616.285\n",
      "\n",
      "[ 50] batch loss: 0.649\n",
      "[100] batch loss: 0.678\n",
      "[150] batch loss: 0.664\n",
      "[200] batch loss: 0.680\n",
      "[250] batch loss: 0.667\n",
      "[300] batch loss: 0.643\n",
      "[350] batch loss: 0.668\n",
      "[400] batch loss: 0.687\n",
      "[450] batch loss: 0.673\n",
      "[500] batch loss: 0.668\n",
      "[550] batch loss: 0.661\n",
      "[600] batch loss: 0.665\n",
      "[650] batch loss: 0.681\n",
      "[700] batch loss: 0.642\n",
      "[750] batch loss: 0.684\n",
      "[800] batch loss: 0.659\n",
      "[850] batch loss: 0.645\n",
      "[900] batch loss: 0.663\n",
      "epoch 32 loss: 616.285 -> 613.221\n",
      "\n",
      "[ 50] batch loss: 0.663\n",
      "[100] batch loss: 0.657\n",
      "[150] batch loss: 0.659\n",
      "[200] batch loss: 0.684\n",
      "[250] batch loss: 0.679\n",
      "[300] batch loss: 0.664\n",
      "[350] batch loss: 0.656\n",
      "[400] batch loss: 0.647\n",
      "[450] batch loss: 0.674\n",
      "[500] batch loss: 0.673\n",
      "[550] batch loss: 0.659\n",
      "[600] batch loss: 0.710\n",
      "[650] batch loss: 0.659\n",
      "[700] batch loss: 0.670\n",
      "[750] batch loss: 0.685\n",
      "[800] batch loss: 0.647\n",
      "[850] batch loss: 0.657\n",
      "[900] batch loss: 0.671\n",
      "epoch 33 loss: 613.221 -> 614.891\n",
      "\n",
      "[ 50] batch loss: 0.683\n",
      "[100] batch loss: 0.667\n",
      "[150] batch loss: 0.652\n",
      "[200] batch loss: 0.666\n",
      "[250] batch loss: 0.663\n",
      "[300] batch loss: 0.644\n",
      "[350] batch loss: 0.641\n",
      "[400] batch loss: 0.653\n",
      "[450] batch loss: 0.671\n",
      "[500] batch loss: 0.679\n",
      "[550] batch loss: 0.669\n",
      "[600] batch loss: 0.684\n",
      "[650] batch loss: 0.693\n",
      "[700] batch loss: 0.667\n",
      "[750] batch loss: 0.713\n",
      "[800] batch loss: 0.668\n",
      "[850] batch loss: 0.652\n",
      "[900] batch loss: 0.677\n",
      "epoch 34 loss: 614.891 -> 616.044\n",
      "\n",
      "[ 50] batch loss: 0.699\n",
      "[100] batch loss: 0.654\n",
      "[150] batch loss: 0.667\n",
      "[200] batch loss: 0.680\n",
      "[250] batch loss: 0.689\n",
      "[300] batch loss: 0.663\n",
      "[350] batch loss: 0.671\n",
      "[400] batch loss: 0.643\n",
      "[450] batch loss: 0.663\n",
      "[500] batch loss: 0.668\n",
      "[550] batch loss: 0.655\n",
      "[600] batch loss: 0.681\n",
      "[650] batch loss: 0.661\n",
      "[700] batch loss: 0.657\n",
      "[750] batch loss: 0.681\n",
      "[800] batch loss: 0.672\n",
      "[850] batch loss: 0.646\n",
      "[900] batch loss: 0.652\n",
      "epoch 35 loss: 616.044 -> 614.287\n",
      "\n",
      "[ 50] batch loss: 0.667\n",
      "[100] batch loss: 0.676\n",
      "[150] batch loss: 0.645\n",
      "[200] batch loss: 0.657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250] batch loss: 0.671\n",
      "[300] batch loss: 0.656\n",
      "[350] batch loss: 0.639\n",
      "[400] batch loss: 0.647\n",
      "[450] batch loss: 0.656\n",
      "[500] batch loss: 0.667\n",
      "[550] batch loss: 0.662\n",
      "[600] batch loss: 0.668\n",
      "[650] batch loss: 0.668\n",
      "[700] batch loss: 0.666\n",
      "[750] batch loss: 0.666\n",
      "[800] batch loss: 0.660\n",
      "[850] batch loss: 0.637\n",
      "[900] batch loss: 0.647\n",
      "epoch 36 loss: 614.287 -> 606.828\n",
      "\n",
      "[ 50] batch loss: 0.673\n",
      "[100] batch loss: 0.655\n",
      "[150] batch loss: 0.659\n",
      "[200] batch loss: 0.686\n",
      "[250] batch loss: 0.658\n",
      "[300] batch loss: 0.664\n",
      "[350] batch loss: 0.652\n",
      "[400] batch loss: 0.653\n",
      "[450] batch loss: 0.657\n",
      "[500] batch loss: 0.670\n",
      "[550] batch loss: 0.689\n",
      "[600] batch loss: 0.710\n",
      "[650] batch loss: 0.689\n",
      "[700] batch loss: 0.667\n",
      "[750] batch loss: 0.645\n",
      "[800] batch loss: 0.636\n",
      "[850] batch loss: 0.632\n",
      "[900] batch loss: 0.645\n",
      "epoch 37 loss: 606.828 -> 611.176\n",
      "\n",
      "[ 50] batch loss: 0.668\n",
      "[100] batch loss: 0.653\n",
      "[150] batch loss: 0.652\n",
      "[200] batch loss: 0.664\n",
      "[250] batch loss: 0.670\n",
      "[300] batch loss: 0.659\n",
      "[350] batch loss: 0.653\n",
      "[400] batch loss: 0.662\n",
      "[450] batch loss: 0.639\n",
      "[500] batch loss: 0.678\n",
      "[550] batch loss: 0.677\n",
      "[600] batch loss: 0.664\n",
      "[650] batch loss: 0.668\n",
      "[700] batch loss: 0.667\n",
      "[750] batch loss: 0.660\n",
      "[800] batch loss: 0.655\n",
      "[850] batch loss: 0.654\n",
      "[900] batch loss: 0.637\n",
      "epoch 38 loss: 611.176 -> 608.451\n",
      "\n",
      "[ 50] batch loss: 0.658\n",
      "[100] batch loss: 0.652\n",
      "[150] batch loss: 0.662\n",
      "[200] batch loss: 0.637\n",
      "[250] batch loss: 0.652\n",
      "[300] batch loss: 0.655\n",
      "[350] batch loss: 0.636\n",
      "[400] batch loss: 0.651\n",
      "[450] batch loss: 0.651\n",
      "[500] batch loss: 0.675\n",
      "[550] batch loss: 0.653\n",
      "[600] batch loss: 0.683\n",
      "[650] batch loss: 0.671\n",
      "[700] batch loss: 0.657\n",
      "[750] batch loss: 0.667\n",
      "[800] batch loss: 0.646\n",
      "[850] batch loss: 0.647\n",
      "[900] batch loss: 0.627\n",
      "epoch 39 loss: 608.451 -> 603.074\n",
      "\n",
      "[ 50] batch loss: 0.657\n",
      "[100] batch loss: 0.647\n",
      "[150] batch loss: 0.658\n",
      "[200] batch loss: 0.669\n",
      "[250] batch loss: 0.649\n",
      "[300] batch loss: 0.650\n",
      "[350] batch loss: 0.650\n",
      "[400] batch loss: 0.658\n",
      "[450] batch loss: 0.657\n",
      "[500] batch loss: 0.683\n",
      "[550] batch loss: 0.676\n",
      "[600] batch loss: 0.676\n",
      "[650] batch loss: 0.664\n",
      "[700] batch loss: 0.670\n",
      "[750] batch loss: 0.664\n",
      "[800] batch loss: 0.652\n",
      "[850] batch loss: 0.661\n",
      "[900] batch loss: 0.651\n",
      "epoch 40 loss: 603.074 -> 608.451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%skip $load_model_da\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 40\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    lamda = adjust_lamda(cnn_da, p)\n",
    "    for i, data in enumerate(trainloader_da):\n",
    "        source_size = data[0].size()[0] // 2\n",
    "        inputs, labels, domains = data\n",
    "        domains = domains.to(torch.float32)\n",
    "        if (use_gpu):\n",
    "            inputs, labels, domains = inputs.cuda(), labels.cuda(), domains.cuda()\n",
    "        inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, outputs_DC = cnn_da(inputs)\n",
    "        outputs_DC = outputs_DC.view(-1)\n",
    "        loss_LC = criterion_LC(outputs_LC[:source_size], labels[:source_size])\n",
    "        loss_DC = criterion_DC(outputs_DC, domains)\n",
    "        loss = loss_LC + loss_DC\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%3d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        prev_loss = epoch_loss\n",
    "        pass\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model_da\n",
    "\n",
    "torch.save(cnn.state_dict(), para_file_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_da_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs_LC, _ = model(inputs)\n",
    "        correct += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST-M test set (source only): 0.8186666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on MNIST-M test set (source only): \" + str(evaluate_da_accuracy(cnn_da, testloader_m)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "My_CNN_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
