{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTya_dtvpdaX"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e9W0cTFEPmAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold\n",
    "from math import exp\n",
    "from torch.autograd import Variable\n",
    "from my_dataset import MNIST_M\n",
    "from my_dataset import ST_Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the numpy input image with dim 3 x height x width\n",
    "def convert_to_plt(input_image):\n",
    "    input_image = input_image*np.asarray([0.5,0.5,0.5]) + np.asarray([0.5,0.5,0.5])\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1523506225267,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "4BrzNF_oQL77",
    "outputId": "fe8e2212-37c8-4027-d176-ef46991bcd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3W2qqi9azoVs"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            #inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs, _, _ = model(inputs)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels.data).sum().item()\n",
    "            total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oBeLwnRLrcY"
   },
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "root_dir = \"./data/mnist/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, )),\n",
    "    #transforms.Normalize((0.13066047712053577, ), (1, )),\n",
    "    transforms.Lambda(lambda x: torch.cat((x, x, x), dim=0))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root_dir, train=True, transform=transform,\n",
    "                                      target_transform=None, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset = torchvision.datasets.MNIST(root_dir, train=False, transform=transform,\n",
    "                                      target_transform=None, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 101th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAA6RJREFUeJztm78rdXEcx18HAzaDgRUppa5kYGDwMwYpilCSRVksUiiDIspfIIWRRcQig1+LGAxikzLalN/uM+g4z+V6rofrfY/j81rUued7vl+v3j6+P851wuEwhoakRA/gN2GyhZhsISZbiMkWYrKFmGwhJluIyRaSouzMcZzALlfD4bAT6x5LthCTLcRkCzHZQky2EJMtxGQLMdlCTLYQky3EZAsx2UJMthDprp8fSUp6zltKSqSKx8fHiJ9x6StuTzJiEvhkJycnA5CWlgZAX18fABkZGQAUFRUBUFVVFdFucnISgMHBwbiNxZItJFDJdutuKBSivb0dgNzcXAAaGhqitnGc5wOW1+88lpaWxn18lmwhPyrZbv3NzMwEICsrC4Cenh4AsrOzAWhsbPxyX4uLi19+xmss2UIc5fvZnz1d7+rqAqC+vh6A5ubmmG2ur68B2NraAmBpaSni81AoBEBvby/gzbf39vYAKC8vB+Dp6elDY7TTdZ/hi5rt1uLp6WnAmwO7uDODnJycqO1vbm4A2NnZAWBhYYGLiwsANjc3o7YZGRkB4O7uDoDU1FQAVlZWgLezk3hgyRbii2S7NbimpgaA/Pz8f95/eHgIwNTUFACXl5cAbGxsxOyrsLAQgLq6OsBL9NraGgDr6+vA9yTbF7KbmpoAT/Ls7CwAq6urUe/f398HeCkVn+nL/QfpLmrGxsYAODo6+u9nfhQrI0J8key2tjYA5ufnATg+Pgbg7Owsbn0UFBQA3vTR3Zjq6OgA4ODgIG59vYclW8iPWNTEg5mZGcD7Kzo5OQGgoqICgKurqy893xY1PsMXNfs7GR4eBqC1tRXwavXExAQAt7e3srFYsoUENtllZWUAdHZ2ApCeng7A0NAQAMvLywDc39/LxmTJFhLYZA8MDACQl5cXcX17exvwNqCUWLKFBDLZc3NzVFdXA3B6egpAd3c3oFkpvoclW0igkl1SUgJAS0vLy9bp+fk54CU6EbXaxZItJFDJrq2tBbwDAYDx8XEgsYl2sWQLCUSyi4uLAejv73+5Njo6CniHwH7Aki0kEMmurKwEIl+BcHfzHh4eEjKmaFiyhQQi2dHY3d1N9BDeYMkWEogzSPfdj7+/hOTWatXvZ2eQPiMQyfYDlmyfIU32b8eSLcRkCzHZQky2EJMtxGQLMdlCTLYQky3EZAsx2UJMthCTLcRkCzHZQky2EJMtxGQLMdlCTLYQky3EZAsx2UL+AM5D5jhHGHldAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "\n",
    "plt_img = convert_to_plt(inputs[idx].permute(1, 2, 0).cpu().data.numpy())\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(plt_img)\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-M Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import MNIST_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "root_dir = \"./data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4581609321206303, 0.462350402961343, 0.4084781187671726), (1, 1, 1))\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset_m = MNIST_M(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_m = torch.utils.data.DataLoader(trainset_m, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_m = MNIST_M(root_dir, train=False, transform=transform_m, download=True)\n",
    "testloader_m = torch.utils.data.DataLoader(testset_m, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 82th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_m):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "    \n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "\n",
    "plt_img = convert_to_plt(inputs[idx].permute(1, 2, 0).cpu().data.numpy())\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(plt_img)\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "def extract_features(model, dataloader):\n",
    "    features = None\n",
    "    lasthidden = None\n",
    "    labels = None\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels_ = data\n",
    "        if (use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        inputs = Variable(inputs)\n",
    "        _, f, l = model(inputs)\n",
    "        if i == 0:\n",
    "            features = f\n",
    "            lasthidden = l\n",
    "            labels = labels_\n",
    "        else:\n",
    "            features = torch.cat((features, f))\n",
    "            lasthidden = torch.cat((lasthidden, l))\n",
    "            labels = torch.cat((labels, labels_))\n",
    "    features = features.data\n",
    "    lasthidden = lasthidden.data\n",
    "    if use_gpu:\n",
    "        features = features.cpu()\n",
    "        lasthidden = lasthidden.cpu()\n",
    "    return features, lasthidden, labels\n",
    "\n",
    "def visualize_single_dataset(data, labels, perplexity=50, sample_num=None):\n",
    "    total_num = labels.shape[0]\n",
    "    if sample_num:\n",
    "        idx = np.random.choice(total_num, sample_num, replace=False)\n",
    "        data, labels = data[idx, :], labels[idx]\n",
    "        total_num = sample_num\n",
    "    tsne = manifold.TSNE(n_components=2, init='random',\n",
    "                     random_state=0, perplexity=perplexity)\n",
    "    X = tsne.fit_transform(data)\n",
    "    colors = [\"red\", \"orange\", \"goldenrod\", \"yellow\", \"yellowgreen\", \"green\", \"teal\", \"blue\", \"violet\", \"purple\"]\n",
    "    for i in range(10):\n",
    "        plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i], alpha=0.4)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def visualize_da(source, target, perplexity=50, sample_num=None, save=None):\n",
    "    source_num = source.shape[0]\n",
    "    target_num = target.shape[0]\n",
    "    if sample_num:\n",
    "        source, target = source[:sample_num, :], target[:sample_num, :]\n",
    "        \n",
    "    data = np.vstack((source, target))\n",
    "\n",
    "    tsne = manifold.TSNE(n_components=2, init='random',\n",
    "                         random_state=0, perplexity=perplexity)\n",
    "    X = tsne.fit_transform(data)\n",
    "    plt.scatter(X[:sample_num, 0], X[:sample_num, 1], c=\"blue\", edgecolors=None, alpha=0.4)\n",
    "    plt.scatter(X[sample_num:, 0], X[sample_num:, 1], c=\"red\", edgecolors=None, alpha=0.4)\n",
    "    plt.axis(\"off\")\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation\n",
    "\n",
    "## Structure (GRL)\n",
    "\n",
    "![MNIST Structure](https://c1.staticflickr.com/1/978/41270649404_41480327ce_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import ST_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainset_da = ST_Dataset(trainset, trainset_m, batch_size)\n",
    "trainloader_da = torch.utils.data.DataLoader(trainset_da, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 112th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "I don't know :)\n",
      "\n",
      "From domain:\n",
      "Target\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from test set\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_da):\n",
    "    inputs, labels, domain = data\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt_img = convert_to_plt(inputs[idx].permute(1, 2, 0).cpu().data.numpy())\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(plt_img)\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\")\n",
    "if labels[idx].item() == -1:\n",
    "    print(\"I don't know :)\\n\")\n",
    "else:\n",
    "    print(str(labels[idx].item()) + \"\\n\")\n",
    "\n",
    "print(\"From domain:\")\n",
    "if domain[idx].item() == 0:\n",
    "    print(\"Source\")\n",
    "else:\n",
    "    print(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class label_classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(label_classifier, self).__init__()\n",
    "        \n",
    "        # feature extractor\n",
    "        self.C1 = nn.Conv2d(3, 32, 5)\n",
    "        self.C2 = nn.Conv2d(32, 48, 5)\n",
    "        # label classifier\n",
    "        self.LC_FC1 = nn.Linear(48 * 4 * 4, 100)\n",
    "        self.LC_FC2 = nn.Linear(100, 100)\n",
    "        self.LC_FC3 = nn.Linear(100, 10)\n",
    "\n",
    "        self.C1.weight.data.normal_(0.0, 0.1)\n",
    "        self.C1.weight.data.normal_(0.0, 0.1)        \n",
    "        self.LC_FC1.weight.data.normal_(0.0, 0.1)\n",
    "        self.LC_FC2.weight.data.normal_(0.0, 0.1)\n",
    "        self.LC_FC3.weight.data.normal_(0.0, 0.1)\n",
    "        \n",
    "        self.LC_FC1.bias.data.normal_(0.0, 0.1)\n",
    "        self.LC_FC2.bias.data.normal_(0.0, 0.1)\n",
    "        self.LC_FC3.bias.data.normal_(0.0, 0.1)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # x's size is (128, 48, 4, 4)\n",
    "        # flatten\n",
    "        x = x.view(-1, 48 * 4 * 4)        \n",
    "        f = x\n",
    "        \n",
    "        \n",
    "        # label classifier\n",
    "        # LC_FC1\n",
    "        x_l = F.relu(self.LC_FC1(x))\n",
    "        # LC_FC2\n",
    "        x_l = F.relu(self.LC_FC2(x_l))\n",
    "        # LC_FC3\n",
    "        x_l = self.LC_FC3(x_l)\n",
    "\n",
    "        return x_l, f\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        # domain classifier\n",
    "        self.DC_FC1 = nn.Linear(48 * 4 * 4, 100)\n",
    "        self.DC_FC2 = nn.Linear(100, 100)\n",
    "        self.DC_FC3 = nn.Linear(100, 10)\n",
    "        self.DC_FC4 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.DC_FC1.weight.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC2.weight.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC3.weight.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC4.weight.data.normal_(0.0, 0.1)\n",
    "        \n",
    "        self.DC_FC1.bias.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC2.bias.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC3.bias.data.normal_(0.0, 0.1)\n",
    "        self.DC_FC4.bias.data.normal_(0.0, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # discriminator classifier\n",
    "        # DC_FC1\n",
    "        x_d = F.relu(self.DC_FC1(x))\n",
    "        # DC_FC2\n",
    "        x_d = F.relu(self.DC_FC2(x_d))\n",
    "        # DC_FC3\n",
    "        x_d = F.relu(self.DC_FC3(x_d))\n",
    "        # DC_FC4\n",
    "        x_d = F.sigmoid(self.DC_FC4(x_d))\n",
    "        \n",
    "        return x_d\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_da(model, dataloader):\n",
    "    features = None\n",
    "    lasthidden = None\n",
    "    labels = None\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels_ = data\n",
    "        if (use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        inputs = Variable(inputs)\n",
    "        _, _, f, l = model(inputs)\n",
    "        \n",
    "        # from source\n",
    "        if i == 0:\n",
    "            features = f\n",
    "            lasthidden = l\n",
    "            labels = labels_\n",
    "        # from target\n",
    "        else:\n",
    "            features = torch.cat((features, f))\n",
    "            lasthidden = torch.cat((lasthidden, l))\n",
    "            labels = torch.cat((labels, labels_))\n",
    "    features = features.data\n",
    "    lasthidden = lasthidden.data\n",
    "    if use_gpu:\n",
    "        features = features.cpu()\n",
    "        lasthidden = lasthidden.cpu()\n",
    "    return features, lasthidden, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_classifier(\n",
      "  (C1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (C2): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (LC_FC1): Linear(in_features=768, out_features=100, bias=True)\n",
      "  (LC_FC2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (LC_FC3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "LC = label_classifier()\n",
    "if (use_gpu):\n",
    "    LC.cuda()\n",
    "print(LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator(\n",
      "  (DC_FC1): Linear(in_features=768, out_features=100, bias=True)\n",
      "  (DC_FC2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (DC_FC3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (DC_FC4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "DC = discriminator()\n",
    "if (use_gpu):\n",
    "    DC.cuda()\n",
    "print(DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.001\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "criterion_DC = nn.BCELoss()\n",
    "\n",
    "optimizer_LC = optim.SGD(LC.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "\n",
    "optimizer_DC = optim.SGD(DC.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "        \n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_da = True\n",
      "load_model_da = True\n"
     ]
    }
   ],
   "source": [
    "para_file_da_dc = \"./parameters/DA_GAN_DC.pt\"\n",
    "para_file_da_lc = \"./parameters/DA_GAN_LC.pt\"\n",
    "load_model_da_dc = os.path.isfile(para_file_da_dc)\n",
    "load_model_da_lc = os.path.isfile(para_file_da_lc)\n",
    "print(\"load_model_da = \" + str(load_model_da_dc))\n",
    "print(\"load_model_da = \" + str(load_model_da_lc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip (not $load_model_da_dc)\n",
    "#DC.load_state_dict(torch.load(para_file_da_dc))\n",
    "#%%skip (not $load_model_da_lc)\n",
    "#LC.load_state_dict(torch.load(para_file_da_lc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50] batch loss: 0.076\n",
      "[100] batch loss: 0.029\n",
      "[150] batch loss: 0.017\n",
      "[200] batch loss: 0.024\n",
      "[250] batch loss: 0.024\n",
      "[300] batch loss: 0.007\n",
      "[350] batch loss: 0.016\n",
      "[400] batch loss: 0.003\n",
      "[450] batch loss: 0.043\n",
      "[500] batch loss: 0.006\n",
      "[550] batch loss: 0.007\n",
      "[600] batch loss: 0.011\n",
      "[650] batch loss: 0.004\n",
      "[700] batch loss: 0.014\n",
      "[750] batch loss: 0.009\n",
      "[800] batch loss: 0.005\n",
      "[850] batch loss: 0.041\n",
      "[900] batch loss: 0.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([56, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC epoch 1 loss: inf -> 1138.700\n",
      "\n",
      "[ 50] batch loss: 0.007\n",
      "[100] batch loss: 0.001\n",
      "[150] batch loss: 0.002\n",
      "[200] batch loss: 0.012\n",
      "[250] batch loss: 0.008\n",
      "[300] batch loss: 0.013\n",
      "[350] batch loss: 0.005\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.006\n",
      "[500] batch loss: 0.002\n",
      "[550] batch loss: 0.003\n",
      "[600] batch loss: 0.007\n",
      "[650] batch loss: 0.003\n",
      "[700] batch loss: 0.011\n",
      "[750] batch loss: 0.001\n",
      "[800] batch loss: 0.008\n",
      "[850] batch loss: 0.001\n",
      "[900] batch loss: 0.003\n",
      "DC epoch 2 loss: 1138.700 -> 352.508\n",
      "\n",
      "[ 50] batch loss: 0.005\n",
      "[100] batch loss: 0.000\n",
      "[150] batch loss: 0.002\n",
      "[200] batch loss: 0.010\n",
      "[250] batch loss: 0.010\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.003\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.012\n",
      "[500] batch loss: 0.001\n",
      "[550] batch loss: 0.005\n",
      "[600] batch loss: 0.006\n",
      "[650] batch loss: 0.002\n",
      "[700] batch loss: 0.008\n",
      "[750] batch loss: 0.002\n",
      "[800] batch loss: 0.006\n",
      "[850] batch loss: 0.000\n",
      "[900] batch loss: 0.000\n",
      "DC epoch 3 loss: 352.508 -> 258.775\n",
      "\n",
      "[ 50] batch loss: 0.001\n",
      "[100] batch loss: 0.000\n",
      "[150] batch loss: 0.004\n",
      "[200] batch loss: 0.014\n",
      "[250] batch loss: 0.014\n",
      "[300] batch loss: 0.002\n",
      "[350] batch loss: 0.005\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.001\n",
      "[500] batch loss: 0.000\n",
      "[550] batch loss: 0.003\n",
      "[600] batch loss: 0.005\n",
      "[650] batch loss: 0.001\n",
      "[700] batch loss: 0.010\n",
      "[750] batch loss: 0.001\n",
      "[800] batch loss: 0.001\n",
      "[850] batch loss: 0.000\n",
      "[900] batch loss: 0.000\n",
      "DC epoch 4 loss: 258.775 -> 229.558\n",
      "\n",
      "[ 50] batch loss: 0.000\n",
      "[100] batch loss: 0.000\n",
      "[150] batch loss: 0.000\n",
      "[200] batch loss: 0.006\n",
      "[250] batch loss: 0.012\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.004\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.001\n",
      "[500] batch loss: 0.001\n",
      "[550] batch loss: 0.003\n",
      "[600] batch loss: 0.003\n",
      "[650] batch loss: 0.000\n",
      "[700] batch loss: 0.011\n",
      "[750] batch loss: 0.001\n",
      "[800] batch loss: 0.001\n",
      "[850] batch loss: 0.000\n",
      "[900] batch loss: 0.000\n",
      "DC epoch 5 loss: 229.558 -> 182.126\n",
      "\n",
      "[ 50] batch loss: 0.000\n",
      "[100] batch loss: 0.002\n",
      "[150] batch loss: 0.001\n",
      "[200] batch loss: 0.002\n",
      "[250] batch loss: 0.014\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.001\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.001\n",
      "[500] batch loss: 0.002\n",
      "[550] batch loss: 0.003\n",
      "[600] batch loss: 0.009\n",
      "[650] batch loss: 0.000\n",
      "[700] batch loss: 0.012\n",
      "[750] batch loss: 0.001\n",
      "[800] batch loss: 0.000\n",
      "[850] batch loss: 0.000\n",
      "[900] batch loss: 0.000\n",
      "DC epoch 6 loss: 182.126 -> 168.172\n",
      "\n",
      "[ 50] batch loss: 0.000\n",
      "[100] batch loss: 0.002\n",
      "[150] batch loss: 0.001\n",
      "[200] batch loss: 0.004\n",
      "[250] batch loss: 0.013\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.001\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.001\n",
      "[500] batch loss: 0.007\n",
      "[550] batch loss: 0.005\n",
      "[600] batch loss: 0.007\n",
      "[650] batch loss: 0.001\n",
      "[700] batch loss: 0.006\n",
      "[750] batch loss: 0.003\n",
      "[800] batch loss: 0.001\n",
      "[850] batch loss: 0.000\n",
      "[900] batch loss: 0.001\n",
      "DC epoch 7 loss: 168.172 -> 148.307\n",
      "\n",
      "[ 50] batch loss: 0.000\n",
      "[100] batch loss: 0.003\n",
      "[150] batch loss: 0.002\n",
      "[200] batch loss: 0.004\n",
      "[250] batch loss: 0.009\n",
      "[300] batch loss: 0.000\n",
      "[350] batch loss: 0.000\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.000\n",
      "[500] batch loss: 0.000\n",
      "[550] batch loss: 0.002\n",
      "[600] batch loss: 0.004\n",
      "[650] batch loss: 0.000\n",
      "[700] batch loss: 0.004\n",
      "[750] batch loss: 0.002\n",
      "[800] batch loss: 0.000\n",
      "[850] batch loss: 0.000\n",
      "[900] batch loss: 0.002\n",
      "DC epoch 8 loss: 148.307 -> 96.276\n",
      "\n",
      "[ 50] batch loss: 0.000\n",
      "[100] batch loss: 0.000\n",
      "[150] batch loss: 0.003\n",
      "[200] batch loss: 0.006\n",
      "[250] batch loss: 0.003\n",
      "[300] batch loss: 0.000\n",
      "[350] batch loss: 0.000\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.000\n",
      "[500] batch loss: 0.000\n",
      "[550] batch loss: 0.001\n",
      "[600] batch loss: 0.003\n",
      "[650] batch loss: 0.000\n",
      "[700] batch loss: 0.001\n",
      "[750] batch loss: 0.001\n",
      "[800] batch loss: 0.000\n",
      "[850] batch loss: 0.000\n",
      "[900] batch loss: 0.000\n",
      "DC epoch 9 loss: 96.276 -> 79.598\n",
      "\n",
      "[ 50] batch loss: 0.000\n",
      "[100] batch loss: 0.001\n",
      "[150] batch loss: 0.002\n",
      "[200] batch loss: 0.001\n",
      "[250] batch loss: 0.005\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.000\n",
      "[400] batch loss: 0.000\n",
      "[450] batch loss: 0.000\n",
      "[500] batch loss: 0.000\n",
      "[550] batch loss: 0.001\n",
      "[600] batch loss: 0.002\n",
      "[650] batch loss: 0.000\n",
      "[700] batch loss: 0.002\n",
      "[750] batch loss: 0.001\n",
      "[800] batch loss: 0.000\n",
      "[850] batch loss: 0.000\n",
      "[900] batch loss: 0.000\n",
      "DC epoch 10 loss: 79.598 -> 82.913\n",
      "\n",
      "[ 50] batch loss: 54.918\n",
      "[100] batch loss: 31.454\n",
      "[150] batch loss: 29.234\n",
      "[200] batch loss: 28.534\n",
      "[250] batch loss: 28.443\n",
      "[300] batch loss: 25.697\n",
      "[350] batch loss: 26.694\n",
      "[400] batch loss: 26.245\n",
      "[450] batch loss: 24.562\n",
      "[500] batch loss: 24.856\n",
      "[550] batch loss: 24.277\n",
      "[600] batch loss: 25.604\n",
      "[650] batch loss: 24.391\n",
      "[700] batch loss: 22.301\n",
      "[750] batch loss: 24.028\n",
      "[800] batch loss: 23.628\n",
      "[850] batch loss: 22.616\n",
      "[900] batch loss: 20.903\n",
      "LC epoch 1 loss: inf -> 24860.139\n",
      "\n",
      "[ 50] batch loss: 20.931\n",
      "[100] batch loss: 22.400\n",
      "[150] batch loss: 22.994\n",
      "[200] batch loss: 21.976\n",
      "[250] batch loss: 24.095\n",
      "[300] batch loss: 21.520\n",
      "[350] batch loss: 21.619\n",
      "[400] batch loss: 21.847\n",
      "[450] batch loss: 21.254\n",
      "[500] batch loss: 21.736\n",
      "[550] batch loss: 21.376\n",
      "[600] batch loss: 21.699\n",
      "[650] batch loss: 21.791\n",
      "[700] batch loss: 20.062\n",
      "[750] batch loss: 21.982\n",
      "[800] batch loss: 22.018\n",
      "[850] batch loss: 20.752\n",
      "[900] batch loss: 18.883\n",
      "LC epoch 1 loss: 24860.139 -> 19835.914\n",
      "\n",
      "[ 50] batch loss: 19.512\n",
      "[100] batch loss: 20.420\n",
      "[150] batch loss: 20.880\n",
      "[200] batch loss: 20.143\n",
      "[250] batch loss: 22.387\n",
      "[300] batch loss: 20.360\n",
      "[350] batch loss: 20.800\n",
      "[400] batch loss: 21.101\n",
      "[450] batch loss: 20.077\n",
      "[500] batch loss: 20.974\n",
      "[550] batch loss: 20.357\n",
      "[600] batch loss: 20.863\n",
      "[650] batch loss: 20.439\n",
      "[700] batch loss: 19.301\n",
      "[750] batch loss: 20.503\n",
      "[800] batch loss: 21.322\n",
      "[850] batch loss: 19.547\n",
      "[900] batch loss: 17.748\n",
      "LC epoch 1 loss: 19835.914 -> 18712.309\n",
      "\n",
      "[ 50] batch loss: 19.126\n",
      "[100] batch loss: 18.935\n",
      "[150] batch loss: 19.650\n",
      "[200] batch loss: 19.234\n",
      "[250] batch loss: 21.861\n",
      "[300] batch loss: 19.150\n",
      "[350] batch loss: 19.908\n",
      "[400] batch loss: 20.454\n",
      "[450] batch loss: 19.209\n",
      "[500] batch loss: 20.244\n",
      "[550] batch loss: 19.530\n",
      "[600] batch loss: 20.132\n",
      "[650] batch loss: 19.816\n",
      "[700] batch loss: 18.396\n",
      "[750] batch loss: 19.859\n",
      "[800] batch loss: 20.710\n",
      "[850] batch loss: 18.823\n",
      "[900] batch loss: 17.415\n",
      "LC epoch 1 loss: 18712.309 -> 17986.531\n",
      "\n",
      "[ 50] batch loss: 18.716\n",
      "[100] batch loss: 18.630\n",
      "[150] batch loss: 19.182\n",
      "[200] batch loss: 19.320\n",
      "[250] batch loss: 21.215\n",
      "[300] batch loss: 18.827\n",
      "[350] batch loss: 19.324\n",
      "[400] batch loss: 19.154\n",
      "[450] batch loss: 18.567\n",
      "[500] batch loss: 19.533\n",
      "[550] batch loss: 19.063\n",
      "[600] batch loss: 19.682\n",
      "[650] batch loss: 19.601\n",
      "[700] batch loss: 18.057\n",
      "[750] batch loss: 19.519\n",
      "[800] batch loss: 20.465\n",
      "[850] batch loss: 18.689\n",
      "[900] batch loss: 17.072\n",
      "LC epoch 1 loss: 17986.531 -> 17590.919\n",
      "\n",
      "[ 50] batch loss: 17.970\n",
      "[100] batch loss: 18.368\n",
      "[150] batch loss: 18.762\n",
      "[200] batch loss: 18.589\n",
      "[250] batch loss: 21.083\n",
      "[300] batch loss: 18.315\n",
      "[350] batch loss: 18.749\n",
      "[400] batch loss: 18.944\n",
      "[450] batch loss: 18.538\n",
      "[500] batch loss: 19.140\n",
      "[550] batch loss: 18.862\n",
      "[600] batch loss: 19.145\n",
      "[650] batch loss: 19.202\n",
      "[700] batch loss: 17.398\n",
      "[750] batch loss: 19.041\n",
      "[800] batch loss: 19.819\n",
      "[850] batch loss: 18.020\n",
      "[900] batch loss: 16.995\n",
      "LC epoch 1 loss: 17590.919 -> 17198.098\n",
      "\n",
      "[ 50] batch loss: 17.667\n",
      "[100] batch loss: 17.947\n",
      "[150] batch loss: 18.493\n",
      "[200] batch loss: 17.740\n",
      "[250] batch loss: 20.409\n",
      "[300] batch loss: 18.453\n",
      "[350] batch loss: 18.271\n",
      "[400] batch loss: 18.850\n",
      "[450] batch loss: 18.219\n",
      "[500] batch loss: 18.917\n",
      "[550] batch loss: 18.379\n",
      "[600] batch loss: 18.774\n",
      "[650] batch loss: 18.962\n",
      "[700] batch loss: 17.304\n",
      "[750] batch loss: 18.962\n",
      "[800] batch loss: 19.463\n",
      "[850] batch loss: 18.060\n",
      "[900] batch loss: 16.690\n",
      "LC epoch 1 loss: 17198.098 -> 16922.501\n",
      "\n",
      "[ 50] batch loss: 17.621\n",
      "[100] batch loss: 17.634\n",
      "[150] batch loss: 18.567\n",
      "[200] batch loss: 18.054\n",
      "[250] batch loss: 19.946\n",
      "[300] batch loss: 17.826\n",
      "[350] batch loss: 18.459\n",
      "[400] batch loss: 18.581\n",
      "[450] batch loss: 17.985\n",
      "[500] batch loss: 18.677\n",
      "[550] batch loss: 18.065\n",
      "[600] batch loss: 18.564\n",
      "[650] batch loss: 18.752\n",
      "[700] batch loss: 17.260\n",
      "[750] batch loss: 18.743\n",
      "[800] batch loss: 19.121\n",
      "[850] batch loss: 17.947\n",
      "[900] batch loss: 16.549\n",
      "LC epoch 1 loss: 16922.501 -> 16756.102\n",
      "\n",
      "[ 50] batch loss: 17.354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] batch loss: 17.206\n",
      "[150] batch loss: 18.147\n",
      "[200] batch loss: 17.019\n",
      "[250] batch loss: 19.563\n",
      "[300] batch loss: 17.560\n",
      "[350] batch loss: 17.904\n",
      "[400] batch loss: 18.293\n",
      "[450] batch loss: 17.596\n",
      "[500] batch loss: 18.476\n",
      "[550] batch loss: 17.896\n",
      "[600] batch loss: 18.503\n",
      "[650] batch loss: 18.434\n",
      "[700] batch loss: 16.897\n",
      "[750] batch loss: 18.440\n",
      "[800] batch loss: 18.906\n",
      "[850] batch loss: 17.458\n",
      "[900] batch loss: 16.245\n",
      "LC epoch 1 loss: 16756.102 -> 16433.176\n",
      "\n",
      "[ 50] batch loss: 17.111\n",
      "[100] batch loss: 17.242\n",
      "[150] batch loss: 18.134\n",
      "[200] batch loss: 16.983\n",
      "[250] batch loss: 19.282\n",
      "[300] batch loss: 17.305\n",
      "[350] batch loss: 17.525\n",
      "[400] batch loss: 17.939\n",
      "[450] batch loss: 17.301\n",
      "[500] batch loss: 18.190\n",
      "[550] batch loss: 17.823\n",
      "[600] batch loss: 18.154\n",
      "[650] batch loss: 18.188\n",
      "[700] batch loss: 16.895\n",
      "[750] batch loss: 18.297\n",
      "[800] batch loss: 18.739\n",
      "[850] batch loss: 17.229\n",
      "[900] batch loss: 16.072\n",
      "LC epoch 1 loss: 16433.176 -> 16253.101\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 1 loss: inf -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 2 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 3 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 4 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 5 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 6 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 7 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 8 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 9 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 10 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 16.700\n",
      "[100] batch loss: 16.365\n",
      "[150] batch loss: 17.384\n",
      "[200] batch loss: 16.343\n",
      "[250] batch loss: 18.748\n",
      "[300] batch loss: 16.927\n",
      "[350] batch loss: 16.718\n",
      "[400] batch loss: 17.476\n",
      "[450] batch loss: 16.875\n",
      "[500] batch loss: 17.558\n",
      "[550] batch loss: 17.051\n",
      "[600] batch loss: 17.425\n",
      "[650] batch loss: 18.033\n",
      "[700] batch loss: 16.473\n",
      "[750] batch loss: 17.822\n",
      "[800] batch loss: 18.122\n",
      "[850] batch loss: 16.666\n",
      "[900] batch loss: 15.587\n",
      "LC epoch 2 loss: inf -> 15736.097\n",
      "\n",
      "[ 50] batch loss: 16.229\n",
      "[100] batch loss: 16.195\n",
      "[150] batch loss: 17.289\n",
      "[200] batch loss: 16.269\n",
      "[250] batch loss: 18.630\n",
      "[300] batch loss: 16.858\n",
      "[350] batch loss: 16.773\n",
      "[400] batch loss: 17.317\n",
      "[450] batch loss: 16.686\n",
      "[500] batch loss: 17.424\n",
      "[550] batch loss: 17.031\n",
      "[600] batch loss: 17.274\n",
      "[650] batch loss: 17.900\n",
      "[700] batch loss: 16.365\n",
      "[750] batch loss: 17.579\n",
      "[800] batch loss: 17.929\n",
      "[850] batch loss: 16.632\n",
      "[900] batch loss: 15.607\n",
      "LC epoch 2 loss: 15736.097 -> 15618.532\n",
      "\n",
      "[ 50] batch loss: 16.174\n",
      "[100] batch loss: 16.204\n",
      "[150] batch loss: 17.111\n",
      "[200] batch loss: 16.246\n",
      "[250] batch loss: 18.391\n",
      "[300] batch loss: 16.729\n",
      "[350] batch loss: 16.583\n",
      "[400] batch loss: 17.209\n",
      "[450] batch loss: 16.588\n",
      "[500] batch loss: 17.206\n",
      "[550] batch loss: 16.762\n",
      "[600] batch loss: 17.386\n",
      "[650] batch loss: 17.731\n",
      "[700] batch loss: 16.240\n",
      "[750] batch loss: 17.442\n",
      "[800] batch loss: 17.911\n",
      "[850] batch loss: 16.679\n",
      "[900] batch loss: 15.523\n",
      "LC epoch 2 loss: 15618.532 -> 15522.455\n",
      "\n",
      "[ 50] batch loss: 15.976\n",
      "[100] batch loss: 16.045\n",
      "[150] batch loss: 17.026\n",
      "[200] batch loss: 16.058\n",
      "[250] batch loss: 18.235\n",
      "[300] batch loss: 16.654\n",
      "[350] batch loss: 16.539\n",
      "[400] batch loss: 17.081\n",
      "[450] batch loss: 16.472\n",
      "[500] batch loss: 17.090\n",
      "[550] batch loss: 16.632\n",
      "[600] batch loss: 17.261\n",
      "[650] batch loss: 17.739\n",
      "[700] batch loss: 16.255\n",
      "[750] batch loss: 17.256\n",
      "[800] batch loss: 17.868\n",
      "[850] batch loss: 16.422\n",
      "[900] batch loss: 15.210\n",
      "LC epoch 2 loss: 15522.455 -> 15404.649\n",
      "\n",
      "[ 50] batch loss: 15.960\n",
      "[100] batch loss: 16.072\n",
      "[150] batch loss: 16.992\n",
      "[200] batch loss: 15.722\n",
      "[250] batch loss: 18.182\n",
      "[300] batch loss: 16.673\n",
      "[350] batch loss: 16.426\n",
      "[400] batch loss: 17.145\n",
      "[450] batch loss: 16.457\n",
      "[500] batch loss: 17.009\n",
      "[550] batch loss: 16.670\n",
      "[600] batch loss: 17.022\n",
      "[650] batch loss: 17.507\n",
      "[700] batch loss: 16.145\n",
      "[750] batch loss: 17.309\n",
      "[800] batch loss: 17.749\n",
      "[850] batch loss: 16.521\n",
      "[900] batch loss: 15.307\n",
      "LC epoch 2 loss: 15404.649 -> 15354.283\n",
      "\n",
      "[ 50] batch loss: 15.957\n",
      "[100] batch loss: 16.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150] batch loss: 16.954\n",
      "[200] batch loss: 15.854\n",
      "[250] batch loss: 18.063\n",
      "[300] batch loss: 16.657\n",
      "[350] batch loss: 16.572\n",
      "[400] batch loss: 16.880\n",
      "[450] batch loss: 16.230\n",
      "[500] batch loss: 16.967\n",
      "[550] batch loss: 16.637\n",
      "[600] batch loss: 16.915\n",
      "[650] batch loss: 17.421\n",
      "[700] batch loss: 16.098\n",
      "[750] batch loss: 17.282\n",
      "[800] batch loss: 17.568\n",
      "[850] batch loss: 16.469\n",
      "[900] batch loss: 15.299\n",
      "LC epoch 2 loss: 15354.283 -> 15299.618\n",
      "\n",
      "[ 50] batch loss: 15.834\n",
      "[100] batch loss: 16.040\n",
      "[150] batch loss: 16.896\n",
      "[200] batch loss: 15.689\n",
      "[250] batch loss: 18.028\n",
      "[300] batch loss: 16.619\n",
      "[350] batch loss: 16.440\n",
      "[400] batch loss: 17.037\n",
      "[450] batch loss: 16.392\n",
      "[500] batch loss: 16.936\n",
      "[550] batch loss: 16.724\n",
      "[600] batch loss: 16.940\n",
      "[650] batch loss: 17.414\n",
      "[700] batch loss: 16.103\n",
      "[750] batch loss: 17.226\n",
      "[800] batch loss: 17.465\n",
      "[850] batch loss: 16.369\n",
      "[900] batch loss: 15.109\n",
      "LC epoch 2 loss: 15299.618 -> 15275.083\n",
      "\n",
      "[ 50] batch loss: 15.712\n",
      "[100] batch loss: 16.056\n",
      "[150] batch loss: 16.835\n",
      "[200] batch loss: 15.719\n",
      "[250] batch loss: 18.036\n",
      "[300] batch loss: 16.478\n",
      "[350] batch loss: 16.284\n",
      "[400] batch loss: 16.773\n",
      "[450] batch loss: 16.382\n",
      "[500] batch loss: 16.881\n",
      "[550] batch loss: 16.599\n",
      "[600] batch loss: 16.760\n",
      "[650] batch loss: 17.172\n",
      "[700] batch loss: 15.952\n",
      "[750] batch loss: 17.222\n",
      "[800] batch loss: 17.417\n",
      "[850] batch loss: 16.287\n",
      "[900] batch loss: 15.201\n",
      "LC epoch 2 loss: 15275.083 -> 15193.042\n",
      "\n",
      "[ 50] batch loss: 15.653\n",
      "[100] batch loss: 16.123\n",
      "[150] batch loss: 16.899\n",
      "[200] batch loss: 15.836\n",
      "[250] batch loss: 18.015\n",
      "[300] batch loss: 16.752\n",
      "[350] batch loss: 16.270\n",
      "[400] batch loss: 16.641\n",
      "[450] batch loss: 16.265\n",
      "[500] batch loss: 16.862\n",
      "[550] batch loss: 16.777\n",
      "[600] batch loss: 16.695\n",
      "[650] batch loss: 17.135\n",
      "[700] batch loss: 15.936\n",
      "[750] batch loss: 17.165\n",
      "[800] batch loss: 17.411\n",
      "[850] batch loss: 16.283\n",
      "[900] batch loss: 15.233\n",
      "LC epoch 2 loss: 15193.042 -> 15202.906\n",
      "\n",
      "[ 50] batch loss: 15.596\n",
      "[100] batch loss: 16.137\n",
      "[150] batch loss: 16.813\n",
      "[200] batch loss: 15.693\n",
      "[250] batch loss: 17.864\n",
      "[300] batch loss: 16.397\n",
      "[350] batch loss: 16.196\n",
      "[400] batch loss: 16.767\n",
      "[450] batch loss: 16.170\n",
      "[500] batch loss: 16.789\n",
      "[550] batch loss: 16.625\n",
      "[600] batch loss: 16.806\n",
      "[650] batch loss: 17.019\n",
      "[700] batch loss: 15.805\n",
      "[750] batch loss: 17.267\n",
      "[800] batch loss: 17.452\n",
      "[850] batch loss: 16.176\n",
      "[900] batch loss: 15.173\n",
      "LC epoch 2 loss: 15202.906 -> 15146.788\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 1 loss: inf -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 2 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 3 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 4 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 5 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 6 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 7 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 8 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 9 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 4.317\n",
      "[100] batch loss: 4.317\n",
      "[150] batch loss: 4.317\n",
      "[200] batch loss: 4.317\n",
      "[250] batch loss: 4.317\n",
      "[300] batch loss: 4.317\n",
      "[350] batch loss: 4.317\n",
      "[400] batch loss: 4.317\n",
      "[450] batch loss: 4.317\n",
      "[500] batch loss: 4.317\n",
      "[550] batch loss: 4.317\n",
      "[600] batch loss: 4.317\n",
      "[650] batch loss: 4.317\n",
      "[700] batch loss: 4.317\n",
      "[750] batch loss: 4.317\n",
      "[800] batch loss: 4.317\n",
      "[850] batch loss: 4.317\n",
      "[900] batch loss: 4.317\n",
      "DC epoch 10 loss: 199029.702 -> 199029.702\n",
      "\n",
      "[ 50] batch loss: 15.307\n",
      "[100] batch loss: 15.813\n",
      "[150] batch loss: 16.726\n",
      "[200] batch loss: 15.254\n",
      "[250] batch loss: 17.570\n",
      "[300] batch loss: 16.152\n",
      "[350] batch loss: 15.757\n",
      "[400] batch loss: 16.330\n",
      "[450] batch loss: 15.869\n",
      "[500] batch loss: 16.600\n",
      "[550] batch loss: 16.300\n",
      "[600] batch loss: 16.336\n",
      "[650] batch loss: 16.772\n",
      "[700] batch loss: 15.444\n",
      "[750] batch loss: 16.875\n",
      "[800] batch loss: 17.061\n",
      "[850] batch loss: 15.787\n",
      "[900] batch loss: 14.795\n",
      "LC epoch 3 loss: inf -> 14839.363\n",
      "\n",
      "[ 50] batch loss: 15.229\n",
      "[100] batch loss: 15.748\n",
      "[150] batch loss: 16.617\n",
      "[200] batch loss: 15.242\n",
      "[250] batch loss: 17.592\n",
      "[300] batch loss: 16.137\n",
      "[350] batch loss: 15.512\n",
      "[400] batch loss: 16.273\n",
      "[450] batch loss: 15.912\n",
      "[500] batch loss: 16.585\n",
      "[550] batch loss: 16.372\n",
      "[600] batch loss: 16.269\n",
      "[650] batch loss: 16.540\n",
      "[700] batch loss: 15.420\n",
      "[750] batch loss: 16.823\n",
      "[800] batch loss: 17.108\n",
      "[850] batch loss: 15.691\n",
      "[900] batch loss: 14.714\n",
      "LC epoch 3 loss: 14839.363 -> 14792.962\n",
      "\n",
      "[ 50] batch loss: 15.182\n",
      "[100] batch loss: 15.691\n",
      "[150] batch loss: 16.563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200] batch loss: 15.225\n",
      "[250] batch loss: 17.537\n",
      "[300] batch loss: 16.181\n",
      "[350] batch loss: 15.521\n",
      "[400] batch loss: 16.043\n",
      "[450] batch loss: 15.929\n",
      "[500] batch loss: 16.615\n",
      "[550] batch loss: 16.240\n",
      "[600] batch loss: 16.248\n",
      "[650] batch loss: 16.624\n",
      "[700] batch loss: 15.415\n",
      "[750] batch loss: 16.799\n",
      "[800] batch loss: 17.145\n",
      "[850] batch loss: 15.708\n",
      "[900] batch loss: 14.712\n",
      "LC epoch 3 loss: 14792.962 -> 14770.654\n",
      "\n",
      "[ 50] batch loss: 15.143\n",
      "[100] batch loss: 15.636\n",
      "[150] batch loss: 16.481\n",
      "[200] batch loss: 15.303\n",
      "[250] batch loss: 17.379\n",
      "[300] batch loss: 16.139\n",
      "[350] batch loss: 15.366\n",
      "[400] batch loss: 16.043\n",
      "[450] batch loss: 15.884\n",
      "[500] batch loss: 16.628\n",
      "[550] batch loss: 16.320\n",
      "[600] batch loss: 16.203\n",
      "[650] batch loss: 16.627\n",
      "[700] batch loss: 15.342\n",
      "[750] batch loss: 16.710\n",
      "[800] batch loss: 16.969\n",
      "[850] batch loss: 15.656\n",
      "[900] batch loss: 14.784\n",
      "LC epoch 3 loss: 14770.654 -> 14732.778\n",
      "\n",
      "[ 50] batch loss: 15.121\n",
      "[100] batch loss: 15.633\n",
      "[150] batch loss: 16.436\n",
      "[200] batch loss: 15.258\n",
      "[250] batch loss: 17.416\n",
      "[300] batch loss: 16.160\n",
      "[350] batch loss: 15.486\n",
      "[400] batch loss: 15.997\n",
      "[450] batch loss: 15.858\n",
      "[500] batch loss: 16.607\n",
      "[550] batch loss: 16.277\n",
      "[600] batch loss: 16.125\n",
      "[650] batch loss: 16.582\n",
      "[700] batch loss: 15.314\n",
      "[750] batch loss: 16.745\n",
      "[800] batch loss: 17.009\n",
      "[850] batch loss: 15.580\n",
      "[900] batch loss: 14.724\n",
      "LC epoch 3 loss: 14732.778 -> 14721.271\n",
      "\n",
      "[ 50] batch loss: 15.125\n",
      "[100] batch loss: 15.690\n",
      "[150] batch loss: 16.535\n",
      "[200] batch loss: 15.208\n",
      "[250] batch loss: 17.423\n",
      "[300] batch loss: 16.008\n",
      "[350] batch loss: 15.544\n",
      "[400] batch loss: 15.969\n",
      "[450] batch loss: 15.822\n",
      "[500] batch loss: 16.640\n",
      "[550] batch loss: 16.220\n",
      "[600] batch loss: 16.118\n",
      "[650] batch loss: 16.520\n",
      "[700] batch loss: 15.351\n",
      "[750] batch loss: 16.702\n",
      "[800] batch loss: 16.980\n",
      "[850] batch loss: 15.569\n",
      "[900] batch loss: 14.798\n",
      "LC epoch 3 loss: 14721.271 -> 14715.541\n",
      "\n",
      "[ 50] batch loss: 15.142\n",
      "[100] batch loss: 15.671\n",
      "[150] batch loss: 16.534\n",
      "[200] batch loss: 15.160\n",
      "[250] batch loss: 17.376\n",
      "[300] batch loss: 16.020\n",
      "[350] batch loss: 15.558\n",
      "[400] batch loss: 16.012\n",
      "[450] batch loss: 15.837\n",
      "[500] batch loss: 16.581\n",
      "[550] batch loss: 16.223\n",
      "[600] batch loss: 16.075\n",
      "[650] batch loss: 16.443\n",
      "[700] batch loss: 15.274\n",
      "[750] batch loss: 16.721\n",
      "[800] batch loss: 16.900\n",
      "[850] batch loss: 15.572\n",
      "[900] batch loss: 14.830\n",
      "LC epoch 3 loss: 14715.541 -> 14699.645\n",
      "\n",
      "[ 50] batch loss: 15.112\n",
      "[100] batch loss: 15.646\n",
      "[150] batch loss: 16.457\n",
      "[200] batch loss: 15.182\n",
      "[250] batch loss: 17.386\n",
      "[300] batch loss: 15.961\n",
      "[350] batch loss: 15.441\n",
      "[400] batch loss: 16.065\n",
      "[450] batch loss: 15.845\n",
      "[500] batch loss: 16.602\n",
      "[550] batch loss: 16.215\n",
      "[600] batch loss: 16.096\n",
      "[650] batch loss: 16.564\n",
      "[700] batch loss: 15.285\n",
      "[750] batch loss: 16.700\n",
      "[800] batch loss: 16.975\n",
      "[850] batch loss: 15.491\n",
      "[900] batch loss: 14.779\n",
      "LC epoch 3 loss: 14699.645 -> 14689.446\n",
      "\n",
      "[ 50] batch loss: 15.061\n",
      "[100] batch loss: 15.639\n",
      "[150] batch loss: 16.498\n",
      "[200] batch loss: 15.037\n",
      "[250] batch loss: 17.251\n",
      "[300] batch loss: 15.990\n",
      "[350] batch loss: 15.498\n",
      "[400] batch loss: 15.990\n",
      "[450] batch loss: 15.837\n",
      "[500] batch loss: 16.567\n",
      "[550] batch loss: 16.263\n",
      "[600] batch loss: 16.116\n",
      "[650] batch loss: 16.464\n",
      "[700] batch loss: 15.314\n",
      "[750] batch loss: 16.642\n",
      "[800] batch loss: 16.933\n",
      "[850] batch loss: 15.481\n",
      "[900] batch loss: 14.788\n",
      "LC epoch 3 loss: 14689.446 -> 14671.157\n",
      "\n",
      "[ 50] batch loss: 15.026\n",
      "[100] batch loss: 15.645\n",
      "[150] batch loss: 16.471\n",
      "[200] batch loss: 15.044\n",
      "[250] batch loss: 17.327\n",
      "[300] batch loss: 16.032\n",
      "[350] batch loss: 15.455\n",
      "[400] batch loss: 15.981\n",
      "[450] batch loss: 15.879\n",
      "[500] batch loss: 16.484\n",
      "[550] batch loss: 16.199\n",
      "[600] batch loss: 16.114\n",
      "[650] batch loss: 16.541\n",
      "[700] batch loss: 15.433\n",
      "[750] batch loss: 16.648\n"
     ]
    }
   ],
   "source": [
    "#%%skip $load_model_da_dc\n",
    "\n",
    "\n",
    "total_epoch = 25\n",
    "lamda = 2\n",
    "const = 1000\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    \n",
    "    DC.train()\n",
    "    LC.eval()\n",
    "\n",
    "    \n",
    "    prev_loss_DC = np.float(\"inf\")\n",
    "    for DC_iter in range(10):\n",
    "        epoch_loss_DC = 0.0\n",
    "        running_loss_DC = 0.0\n",
    "        for batch_idx, data in enumerate(trainloader_da):\n",
    "\n",
    "            \n",
    "            # get inputs\n",
    "            source_size = data[0].size()[0] // 2\n",
    "            inputs, labels, domains = data\n",
    "\n",
    "            domains = domains.to(torch.float32)\n",
    "            if (use_gpu):\n",
    "                inputs, domains = inputs.cuda(), domains.cuda()\n",
    "            inputs, domains = Variable(inputs), Variable(domains)\n",
    "\n",
    "            # forward\n",
    "            optimizer_LC.zero_grad()\n",
    "            _, all_features = LC(inputs)\n",
    "\n",
    "            source_feature = all_features[:source_size]\n",
    "            source_domain = domains[:source_size]\n",
    "\n",
    "            target_feature = all_features[-source_size:]\n",
    "            target_domain = domains[-source_size:]\n",
    "            optimizer_DC.zero_grad()\n",
    "            pred_source_domains = DC(Variable(source_feature))\n",
    "            pred_target_domains = DC(Variable(target_feature))\n",
    "\n",
    "            loss_DC_source = const*criterion_DC(pred_source_domains, source_domain)/batch_size\n",
    "            loss_DC_target = const*criterion_DC(pred_target_domains, target_domain)/batch_size\n",
    "            \n",
    "            \n",
    "            loss_DC = loss_DC_source + loss_DC_target\n",
    "\n",
    "            loss_DC.backward()\n",
    "            optimizer_DC.step()\n",
    "\n",
    "            # stat\n",
    "            epoch_loss_DC += loss_DC.item()\n",
    "            running_loss_DC += loss_DC.item()\n",
    "            #print(loss_DC.cpu().data.numpy())\n",
    "            # stat\n",
    "            if batch_idx % 50 == 49:    # print every 50 mini-batches\n",
    "                 print('[%3d] batch loss: %.3f' %\n",
    "                  (batch_idx + 1, running_loss_DC / 50))\n",
    "            running_loss_DC = 0.0\n",
    "                \n",
    "        print(\"DC epoch %d loss: %.3f -> %.3f\\n\" % (DC_iter + 1, prev_loss_DC, epoch_loss_DC))\n",
    "        if prev_loss_DC - epoch_loss_DC < 0.1:\n",
    "            prev_loss_DC = epoch_loss_DC\n",
    "            pass\n",
    "        else:\n",
    "            prev_loss_DC = epoch_loss_DC                \n",
    "    \n",
    "    \n",
    "    DC.eval()\n",
    "    LC.train()\n",
    "    adjust_lr(optimizer_LC, p)\n",
    "    prev_loss_LC = np.float(\"inf\")\n",
    "    for LC_iter in range(10):\n",
    "        epoch_loss_LC = 0.0\n",
    "        running_loss_LC = 0.0\n",
    "        for batch_idx, data in enumerate(trainloader_da):\n",
    "\n",
    "            # get inputs\n",
    "            source_size = data[0].size()[0] // 2\n",
    "\n",
    "            inputs, labels, domains = data\n",
    "\n",
    "            domains = domains.to(torch.float32)\n",
    "            if (use_gpu):\n",
    "                inputs, labels, domains = inputs.cuda(), labels.cuda(), domains.cuda()\n",
    "            inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "\n",
    "            optimizer_LC.zero_grad()\n",
    "            # forward\n",
    "            pred_labels, all_features = LC(inputs)\n",
    "\n",
    "            src_label = pred_labels[:source_size]\n",
    "            source_feature = all_features[:source_size]\n",
    "            source_domain = domains[:source_size]\n",
    "\n",
    "            target_feature = all_features[-source_size:]\n",
    "            target_domain = domains[-source_size:]\n",
    "\n",
    "            pred_source_domains = DC(source_feature)\n",
    "            pred_target_domains = DC(target_feature)\n",
    "\n",
    "            loss_LC = const*criterion_LC(all_features[:source_size], labels[:source_size])/batch_size\n",
    "            loss_DC_source = const*criterion_DC(pred_source_domains, source_domain)/batch_size\n",
    "            loss_DC_target = const*criterion_DC(pred_target_domains, source_domain)/batch_size\n",
    "\n",
    "            loss = lamda*loss_LC + loss_DC_target + loss_DC_source \n",
    "            loss.backward()\n",
    "            optimizer_LC.step()\n",
    "\n",
    "            # stat\n",
    "            epoch_loss_LC += loss.item()\n",
    "            running_loss_LC += loss.item()\n",
    "\n",
    "            if batch_idx % 50 == 49:    # print every 50 mini-batches\n",
    "                print('[%3d] batch loss: %.3f' %\n",
    "                      (batch_idx + 1, running_loss_LC / 50))\n",
    "                running_loss_LC = 0.0     \n",
    "\n",
    "        print(\"LC epoch %d loss: %.3f -> %.3f\\n\" % (LC_iter + 1, prev_loss_LC, epoch_loss_LC))\n",
    "        if prev_loss_LC - epoch_loss_LC < 0.1:\n",
    "            prev_loss_LC = epoch_loss_LC\n",
    "            pass\n",
    "        else:\n",
    "            prev_loss_LC = epoch_loss_LC            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DC.state_dict(), para_file_da_dc)\n",
    "torch.save(LC.state_dict(), para_file_da_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on MNIST and MNIST-M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_da_accuracy(model_LC, model_DC ,dataloader, source):\n",
    "    model_LC.eval()\n",
    "    model_DC.eval()\n",
    "    correct_LC = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            if (use_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            inputs, labels = Variable(inputs, volatile = True), Variable(labels, volatile = True)\n",
    "            \n",
    "            #print(inputs.shape)\n",
    "            #print(labels.shape)\n",
    "            \n",
    "            outputs_LC, _ = model_LC(inputs)\n",
    "            correct_LC += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "\n",
    "            total += labels.size()[0]\n",
    "        acc_LC = correct_LC / total\n",
    "    return acc_LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label classifier accuracy on MNIST test set (DA): %f\"\n",
    "      %evaluate_da_accuracy(LC, DC, testloader, source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label classifier accuracy on MNIST-M test set (DA): %f\\n\"\n",
    "      %evaluate_da_accuracy(LC, DC, testloader_m, source=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_da, lh_da, l_da = extract_features_da(cnn_da, testloader)\n",
    "f_m_da, lh_m_da, l_m_da = extract_features_da(cnn_da, testloader_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of MNIST test set (extracted by DA model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_da(f_da.numpy(), f_m_da.numpy(), perplexity=50, sample_num=500,\n",
    "             save=\"./pics/MNIST_to_MNIST_M_features_Adapted.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of MNIST-M test set (extracted by DA model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_da(lh_da.numpy(), lh_m_da.numpy(), perplexity=50, sample_num=500,\n",
    "             save=\"./pics/MNIST_to_MNIST_M_lasthidden_Adapted.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "My_CNN_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
