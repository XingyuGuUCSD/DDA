{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTya_dtvpdaX"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e9W0cTFEPmAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold\n",
    "from math import exp, sqrt\n",
    "from torch.autograd import Variable\n",
    "from my_dataset import MNIST_M\n",
    "from my_dataset import ST_Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1523506225267,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "4BrzNF_oQL77",
    "outputId": "fe8e2212-37c8-4027-d176-ef46991bcd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNNUM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import SYNNUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root_dir = \"./data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_syn = SYNNUM(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_syn = torch.utils.data.DataLoader(trainset_syn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_syn = SYNNUM(root_dir, train=False, transform=transform_m, download=True)\n",
    "testloader_syn = torch.utils.data.DataLoader(testset_syn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 14th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAXZJREFUeJzt2s1ugkAUQOE7+NPENMb3f8pui5TpAkjUNlEwc6aV820wRII5uRIYTTnnEKOp/QHWxNggY4OMDTI2yNggY4OMDTI2aEueLKX0so+rOed07z1ONsjYIGODjA0yNsjYIGODjA1CH2p+tRu3/bj9qvVByqsXe3zeenu/3v35ERElnzNTxO44vGyai3NG4fOGlxFUvckep6hth21zs7/kefv+5z6Ckw1K5P9GXPUTxtggY4OMDTI2yNggYz/j7s3etfoLUf/RtK5ziui6xw9zskFO9hL7YXM4bGYd5mSDnOwndN28XzpciFpiugvZRsR5eOlC1B/jZWSJ6ft5nneYkw0yNsjYIGODjA0yNsjYIGODjA0yNsjYIGODjA0yNsjYIGODjA0yNsjYIGODjA0yNsjYIGODjA1C/+u3dk42yNggY4OMDTI2yNggY4OMDTI2yNggY4OMDTI2yNggY4OMDTI2yNggY4OMDTI2yNggY4O+AWvWMLrnECXTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_syn):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVHN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "root_dir = \"./data/\"\n",
    "data_dir = \"svhn/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_svhn = torchvision.datasets.SVHN(os.path.join(root_dir, data_dir),\n",
    "                                          split=\"train\", transform=transform, download=True)\n",
    "trainloader_svhn = torch.utils.data.DataLoader(trainset_svhn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_svhn = torchvision.datasets.SVHN(os.path.join(root_dir, data_dir),\n",
    "                                          split=\"test\", transform=transform, download=True)\n",
    "testloader_svhn = torch.utils.data.DataLoader(testset_svhn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 72th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_svhn):\n",
    "    inputs, labels = data\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_svhn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AX_Msxj4we7"
   },
   "source": [
    "# Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1523506395172,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "d5glpPlhprD4",
    "outputId": "6e9189d3-996c-434c-985e-0e88793fe4eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.C2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.C3 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
    "        self.FC1 = nn.Linear(128 * 7 * 7, 3072)\n",
    "        self.FC2 = nn.Linear(3072, 2048)\n",
    "        self.FC3 = nn.Linear(2048, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (3, 3), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (3, 3), stride=(2, 2))\n",
    "        # C3\n",
    "        x = F.relu(self.C3(x))\n",
    "        # x's size is (128, 128, 7, 7)\n",
    "        # flatten\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        f = x\n",
    "        # FC1\n",
    "        x = F.relu(self.FC1(x))\n",
    "        # FC2\n",
    "        x = F.relu(self.FC2(x))\n",
    "        lh = x\n",
    "        # FC3\n",
    "        x = self.FC3(x)\n",
    "        return x, f, lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (FC1): Linear(in_features=6272, out_features=3072, bias=True)\n",
      "  (FC2): Linear(in_features=3072, out_features=2048, bias=True)\n",
      "  (FC3): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_syn = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_syn.cuda()\n",
    "print(cnn_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, sqrt(2 / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            size = m.weight.size()\n",
    "            fan_out = size[0] # number of rows\n",
    "            fan_in = size[1] # number of columns\n",
    "            m.weight.data.normal_(0, sqrt(2 / (fan_in + fan_out)))\n",
    "            m.bias.data.zero_()\n",
    "        elif hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcSb16SIFgZt"
   },
   "source": [
    "# Training on SYNNUM\n",
    "\n",
    "Or you can load the parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model = False\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "para_file = \"./parameters/cnn_syn\"\n",
    "load_model = os.path.isfile(para_file)\n",
    "print(\"load_model = \" + str(load_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model)\n",
    "cnn_syn.load_state_dict(torch.load(para_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YRHBSIQ2FkOL"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_syn.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgUSfM0KF4iT"
   },
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 10050
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 606703,
     "status": "ok",
     "timestamp": 1523144380194,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "IlstjDLbFkQ6",
    "outputId": "59a56d48-bdaa-4e0e-bc20-0feaecd74fa5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 300] batch loss: 0.608\n",
      "[ 600] batch loss: 0.160\n",
      "[ 900] batch loss: 0.122\n",
      "[1200] batch loss: 0.098\n",
      "[1500] batch loss: 0.081\n",
      "[1800] batch loss: 0.070\n",
      "[2100] batch loss: 0.064\n",
      "[2400] batch loss: 0.051\n",
      "[2700] batch loss: 0.051\n",
      "[3000] batch loss: 0.048\n",
      "[3300] batch loss: 0.048\n",
      "[3600] batch loss: 0.044\n",
      "epoch 1 loss: inf -> 439.908\n",
      "\n",
      "[ 300] batch loss: 0.033\n",
      "[ 600] batch loss: 0.026\n",
      "[ 900] batch loss: 0.025\n",
      "[1200] batch loss: 0.027\n",
      "[1500] batch loss: 0.022\n",
      "[1800] batch loss: 0.019\n",
      "[2100] batch loss: 0.019\n",
      "[2400] batch loss: 0.016\n",
      "[2700] batch loss: 0.016\n",
      "[3000] batch loss: 0.016\n",
      "[3300] batch loss: 0.015\n",
      "[3600] batch loss: 0.015\n",
      "epoch 2 loss: 439.908 -> 76.717\n",
      "\n",
      "[ 300] batch loss: 0.014\n",
      "[ 600] batch loss: 0.012\n",
      "[ 900] batch loss: 0.011\n",
      "[1200] batch loss: 0.012\n",
      "[1500] batch loss: 0.010\n",
      "[1800] batch loss: 0.009\n",
      "[2100] batch loss: 0.008\n",
      "[2400] batch loss: 0.007\n",
      "[2700] batch loss: 0.007\n",
      "[3000] batch loss: 0.007\n",
      "[3300] batch loss: 0.007\n",
      "[3600] batch loss: 0.007\n",
      "epoch 3 loss: 76.717 -> 34.329\n",
      "\n",
      "[ 300] batch loss: 0.007\n",
      "[ 600] batch loss: 0.007\n",
      "[ 900] batch loss: 0.005\n",
      "[1200] batch loss: 0.007\n",
      "[1500] batch loss: 0.006\n",
      "[1800] batch loss: 0.004\n",
      "[2100] batch loss: 0.005\n",
      "[2400] batch loss: 0.004\n",
      "[2700] batch loss: 0.003\n",
      "[3000] batch loss: 0.003\n",
      "[3300] batch loss: 0.003\n",
      "[3600] batch loss: 0.004\n",
      "epoch 4 loss: 34.329 -> 17.800\n",
      "\n",
      "[ 300] batch loss: 0.004\n",
      "[ 600] batch loss: 0.004\n",
      "[ 900] batch loss: 0.003\n",
      "[1200] batch loss: 0.004\n",
      "[1500] batch loss: 0.003\n",
      "[1800] batch loss: 0.003\n",
      "[2100] batch loss: 0.002\n",
      "[2400] batch loss: 0.002\n",
      "[2700] batch loss: 0.002\n",
      "[3000] batch loss: 0.002\n",
      "[3300] batch loss: 0.002\n",
      "[3600] batch loss: 0.002\n",
      "epoch 5 loss: 17.800 -> 10.111\n",
      "\n",
      "[ 300] batch loss: 0.002\n",
      "[ 600] batch loss: 0.002\n",
      "[ 900] batch loss: 0.002\n",
      "[1200] batch loss: 0.002\n",
      "[1500] batch loss: 0.002\n",
      "[1800] batch loss: 0.002\n",
      "[2100] batch loss: 0.001\n",
      "[2400] batch loss: 0.002\n",
      "[2700] batch loss: 0.001\n",
      "[3000] batch loss: 0.001\n",
      "[3300] batch loss: 0.001\n",
      "[3600] batch loss: 0.001\n",
      "epoch 6 loss: 10.111 -> 6.087\n",
      "\n",
      "[ 300] batch loss: 0.002\n",
      "[ 600] batch loss: 0.001\n",
      "[ 900] batch loss: 0.001\n",
      "[1200] batch loss: 0.001\n",
      "[1500] batch loss: 0.001\n",
      "[1800] batch loss: 0.001\n",
      "[2100] batch loss: 0.001\n",
      "[2400] batch loss: 0.001\n",
      "[2700] batch loss: 0.001\n",
      "[3000] batch loss: 0.001\n",
      "[3300] batch loss: 0.001\n",
      "[3600] batch loss: 0.001\n",
      "epoch 7 loss: 6.087 -> 4.059\n",
      "\n",
      "[ 300] batch loss: 0.001\n",
      "[ 600] batch loss: 0.001\n",
      "[ 900] batch loss: 0.001\n",
      "[1200] batch loss: 0.001\n",
      "[1500] batch loss: 0.001\n",
      "[1800] batch loss: 0.001\n",
      "[2100] batch loss: 0.001\n",
      "[2400] batch loss: 0.001\n",
      "[2700] batch loss: 0.001\n",
      "[3000] batch loss: 0.001\n",
      "[3300] batch loss: 0.000\n",
      "[3600] batch loss: 0.001\n",
      "epoch 8 loss: 4.059 -> 2.896\n",
      "\n",
      "[ 300] batch loss: 0.001\n",
      "[ 600] batch loss: 0.001\n",
      "[ 900] batch loss: 0.001\n",
      "[1200] batch loss: 0.001\n",
      "[1500] batch loss: 0.001\n",
      "[1800] batch loss: 0.001\n",
      "[2100] batch loss: 0.000\n",
      "[2400] batch loss: 0.001\n",
      "[2700] batch loss: 0.001\n",
      "[3000] batch loss: 0.000\n",
      "[3300] batch loss: 0.000\n",
      "[3600] batch loss: 0.001\n",
      "epoch 9 loss: 2.896 -> 2.216\n",
      "\n",
      "[ 300] batch loss: 0.001\n",
      "[ 600] batch loss: 0.001\n",
      "[ 900] batch loss: 0.000\n",
      "[1200] batch loss: 0.000\n",
      "[1500] batch loss: 0.001\n",
      "[1800] batch loss: 0.001\n",
      "[2100] batch loss: 0.000\n",
      "[2400] batch loss: 0.000\n",
      "[2700] batch loss: 0.000\n",
      "[3000] batch loss: 0.000\n",
      "[3300] batch loss: 0.000\n",
      "[3600] batch loss: 0.000\n",
      "epoch 10 loss: 2.216 -> 1.784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%skip $load_model\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 10\n",
    "reset(cnn_syn)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_syn):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _, _ = cnn_syn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 299:    # print every 300 mini-batches\n",
    "            print('[%4d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        break\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model\n",
    "\n",
    "torch.save(cnn_syn.state_dict(), para_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oBeLwnRLrcY"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs, _, _ = model(inputs)\n",
    "        correct += (torch.max(outputs.data, 1)[1] == labels.data).sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1632,
     "status": "ok",
     "timestamp": 1523144440407,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "Rn4sOe0iLuKI",
    "outputId": "f31ce94e-ef18-4564-9772-a3a9b22c1272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on SYNNUM test set (source only): 0.9999415936587401\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on SYNNUM test set (source only): \" + str(evaluate_accuracy(cnn_syn, testloader_syn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on SVHN test set (source only): 0.8795712968653965\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on SVHN test set (source only): \" + str(evaluate_accuracy(cnn_syn, testloader_svhn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (FC1): Linear(in_features=6272, out_features=3072, bias=True)\n",
      "  (FC2): Linear(in_features=3072, out_features=2048, bias=True)\n",
      "  (FC3): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_svhn = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_svhn.cuda()\n",
    "print(cnn_svhn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_svhn = False\n"
     ]
    }
   ],
   "source": [
    "para_file_svhn = \"./parameters/cnn_svhn\"\n",
    "load_model_svhn = os.path.isfile(para_file_svhn)\n",
    "print(\"load_model_svhn = \" + str(load_model_svhn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_svhn)\n",
    "cnn_svhn.load_state_dict(torch.load(para_file_svhn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_svhn.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50] batch loss: 2.235\n",
      "[100] batch loss: 1.943\n",
      "[150] batch loss: 1.094\n",
      "[200] batch loss: 0.783\n",
      "[250] batch loss: 0.652\n",
      "[300] batch loss: 0.581\n",
      "[350] batch loss: 0.534\n",
      "[400] batch loss: 0.466\n",
      "[450] batch loss: 0.484\n",
      "[500] batch loss: 0.442\n",
      "[550] batch loss: 0.441\n",
      "epoch 1 loss: inf -> 492.453\n",
      "\n",
      "[ 50] batch loss: 0.399\n",
      "[100] batch loss: 0.400\n",
      "[150] batch loss: 0.370\n",
      "[200] batch loss: 0.400\n",
      "[250] batch loss: 0.365\n",
      "[300] batch loss: 0.336\n",
      "[350] batch loss: 0.328\n",
      "[400] batch loss: 0.301\n",
      "[450] batch loss: 0.320\n",
      "[500] batch loss: 0.299\n",
      "[550] batch loss: 0.308\n",
      "epoch 2 loss: 492.453 -> 198.058\n",
      "\n",
      "[ 50] batch loss: 0.286\n",
      "[100] batch loss: 0.286\n",
      "[150] batch loss: 0.267\n",
      "[200] batch loss: 0.308\n",
      "[250] batch loss: 0.272\n",
      "[300] batch loss: 0.248\n",
      "[350] batch loss: 0.247\n",
      "[400] batch loss: 0.228\n",
      "[450] batch loss: 0.244\n",
      "[500] batch loss: 0.225\n",
      "[550] batch loss: 0.238\n",
      "epoch 3 loss: 198.058 -> 147.609\n",
      "\n",
      "[ 50] batch loss: 0.228\n",
      "[100] batch loss: 0.220\n",
      "[150] batch loss: 0.212\n",
      "[200] batch loss: 0.247\n",
      "[250] batch loss: 0.214\n",
      "[300] batch loss: 0.198\n",
      "[350] batch loss: 0.193\n",
      "[400] batch loss: 0.175\n",
      "[450] batch loss: 0.194\n",
      "[500] batch loss: 0.170\n",
      "[550] batch loss: 0.185\n",
      "epoch 4 loss: 147.609 -> 115.757\n",
      "\n",
      "[ 50] batch loss: 0.183\n",
      "[100] batch loss: 0.169\n",
      "[150] batch loss: 0.165\n",
      "[200] batch loss: 0.194\n",
      "[250] batch loss: 0.171\n",
      "[300] batch loss: 0.154\n",
      "[350] batch loss: 0.151\n",
      "[400] batch loss: 0.136\n",
      "[450] batch loss: 0.148\n",
      "[500] batch loss: 0.130\n",
      "[550] batch loss: 0.142\n",
      "epoch 5 loss: 115.757 -> 90.039\n",
      "\n",
      "[ 50] batch loss: 0.142\n",
      "[100] batch loss: 0.133\n",
      "[150] batch loss: 0.131\n",
      "[200] batch loss: 0.150\n",
      "[250] batch loss: 0.130\n",
      "[300] batch loss: 0.113\n",
      "[350] batch loss: 0.120\n",
      "[400] batch loss: 0.104\n",
      "[450] batch loss: 0.121\n",
      "[500] batch loss: 0.099\n",
      "[550] batch loss: 0.107\n",
      "epoch 6 loss: 90.039 -> 69.605\n",
      "\n",
      "[ 50] batch loss: 0.104\n",
      "[100] batch loss: 0.100\n",
      "[150] batch loss: 0.099\n",
      "[200] batch loss: 0.114\n",
      "[250] batch loss: 0.099\n",
      "[300] batch loss: 0.083\n",
      "[350] batch loss: 0.093\n",
      "[400] batch loss: 0.077\n",
      "[450] batch loss: 0.086\n",
      "[500] batch loss: 0.076\n",
      "[550] batch loss: 0.082\n",
      "epoch 7 loss: 69.605 -> 52.232\n",
      "\n",
      "[ 50] batch loss: 0.079\n",
      "[100] batch loss: 0.078\n",
      "[150] batch loss: 0.069\n",
      "[200] batch loss: 0.084\n",
      "[250] batch loss: 0.078\n",
      "[300] batch loss: 0.061\n",
      "[350] batch loss: 0.067\n",
      "[400] batch loss: 0.064\n",
      "[450] batch loss: 0.080\n",
      "[500] batch loss: 0.062\n",
      "[550] batch loss: 0.069\n",
      "epoch 8 loss: 52.232 -> 40.688\n",
      "\n",
      "[ 50] batch loss: 0.061\n",
      "[100] batch loss: 0.061\n",
      "[150] batch loss: 0.059\n",
      "[200] batch loss: 0.066\n",
      "[250] batch loss: 0.057\n",
      "[300] batch loss: 0.045\n",
      "[350] batch loss: 0.053\n",
      "[400] batch loss: 0.046\n",
      "[450] batch loss: 0.058\n",
      "[500] batch loss: 0.050\n",
      "[550] batch loss: 0.050\n",
      "epoch 9 loss: 40.688 -> 31.190\n",
      "\n",
      "[ 50] batch loss: 0.042\n",
      "[100] batch loss: 0.044\n",
      "[150] batch loss: 0.043\n",
      "[200] batch loss: 0.055\n",
      "[250] batch loss: 0.039\n",
      "[300] batch loss: 0.039\n",
      "[350] batch loss: 0.039\n",
      "[400] batch loss: 0.030\n",
      "[450] batch loss: 0.042\n",
      "[500] batch loss: 0.032\n",
      "[550] batch loss: 0.037\n",
      "epoch 10 loss: 31.190 -> 22.652\n",
      "\n",
      "[ 50] batch loss: 0.027\n",
      "[100] batch loss: 0.041\n",
      "[150] batch loss: 0.027\n",
      "[200] batch loss: 0.043\n",
      "[250] batch loss: 0.031\n",
      "[300] batch loss: 0.033\n",
      "[350] batch loss: 0.026\n",
      "[400] batch loss: 0.021\n",
      "[450] batch loss: 0.027\n",
      "[500] batch loss: 0.024\n",
      "[550] batch loss: 0.027\n",
      "epoch 11 loss: 22.652 -> 17.004\n",
      "\n",
      "[ 50] batch loss: 0.019\n",
      "[100] batch loss: 0.025\n",
      "[150] batch loss: 0.021\n",
      "[200] batch loss: 0.023\n",
      "[250] batch loss: 0.020\n",
      "[300] batch loss: 0.024\n",
      "[350] batch loss: 0.019\n",
      "[400] batch loss: 0.014\n",
      "[450] batch loss: 0.018\n",
      "[500] batch loss: 0.018\n",
      "[550] batch loss: 0.017\n",
      "epoch 12 loss: 17.004 -> 11.161\n",
      "\n",
      "[ 50] batch loss: 0.012\n",
      "[100] batch loss: 0.019\n",
      "[150] batch loss: 0.017\n",
      "[200] batch loss: 0.017\n",
      "[250] batch loss: 0.013\n",
      "[300] batch loss: 0.012\n",
      "[350] batch loss: 0.011\n",
      "[400] batch loss: 0.008\n",
      "[450] batch loss: 0.007\n",
      "[500] batch loss: 0.009\n",
      "[550] batch loss: 0.012\n",
      "epoch 13 loss: 11.161 -> 7.050\n",
      "\n",
      "[ 50] batch loss: 0.008\n",
      "[100] batch loss: 0.009\n",
      "[150] batch loss: 0.008\n",
      "[200] batch loss: 0.010\n",
      "[250] batch loss: 0.007\n",
      "[300] batch loss: 0.006\n",
      "[350] batch loss: 0.007\n",
      "[400] batch loss: 0.005\n",
      "[450] batch loss: 0.006\n",
      "[500] batch loss: 0.005\n",
      "[550] batch loss: 0.008\n",
      "epoch 14 loss: 7.050 -> 4.070\n",
      "\n",
      "[ 50] batch loss: 0.005\n",
      "[100] batch loss: 0.006\n",
      "[150] batch loss: 0.007\n",
      "[200] batch loss: 0.007\n",
      "[250] batch loss: 0.007\n",
      "[300] batch loss: 0.005\n",
      "[350] batch loss: 0.006\n",
      "[400] batch loss: 0.004\n",
      "[450] batch loss: 0.004\n",
      "[500] batch loss: 0.004\n",
      "[550] batch loss: 0.005\n",
      "epoch 15 loss: 4.070 -> 3.003\n",
      "\n",
      "[ 50] batch loss: 0.003\n",
      "[100] batch loss: 0.004\n",
      "[150] batch loss: 0.003\n",
      "[200] batch loss: 0.005\n",
      "[250] batch loss: 0.004\n",
      "[300] batch loss: 0.002\n",
      "[350] batch loss: 0.002\n",
      "[400] batch loss: 0.005\n",
      "[450] batch loss: 0.002\n",
      "[500] batch loss: 0.001\n",
      "[550] batch loss: 0.004\n",
      "epoch 16 loss: 3.003 -> 1.830\n",
      "\n",
      "[ 50] batch loss: 0.002\n",
      "[100] batch loss: 0.003\n",
      "[150] batch loss: 0.002\n",
      "[200] batch loss: 0.002\n",
      "[250] batch loss: 0.002\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.001\n",
      "[400] batch loss: 0.004\n",
      "[450] batch loss: 0.004\n",
      "[500] batch loss: 0.001\n",
      "[550] batch loss: 0.003\n",
      "epoch 17 loss: 1.830 -> 1.317\n",
      "\n",
      "[ 50] batch loss: 0.001\n",
      "[100] batch loss: 0.002\n",
      "[150] batch loss: 0.001\n",
      "[200] batch loss: 0.003\n",
      "[250] batch loss: 0.002\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.001\n",
      "[400] batch loss: 0.003\n",
      "[450] batch loss: 0.001\n",
      "[500] batch loss: 0.001\n",
      "[550] batch loss: 0.002\n",
      "epoch 18 loss: 1.317 -> 0.940\n",
      "\n",
      "[ 50] batch loss: 0.001\n",
      "[100] batch loss: 0.002\n",
      "[150] batch loss: 0.001\n",
      "[200] batch loss: 0.002\n",
      "[250] batch loss: 0.001\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.001\n",
      "[400] batch loss: 0.002\n",
      "[450] batch loss: 0.001\n",
      "[500] batch loss: 0.001\n",
      "[550] batch loss: 0.002\n",
      "epoch 19 loss: 0.940 -> 0.763\n",
      "\n",
      "[ 50] batch loss: 0.001\n",
      "[100] batch loss: 0.002\n",
      "[150] batch loss: 0.001\n",
      "[200] batch loss: 0.001\n",
      "[250] batch loss: 0.001\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.001\n",
      "[400] batch loss: 0.001\n",
      "[450] batch loss: 0.001\n",
      "[500] batch loss: 0.001\n",
      "[550] batch loss: 0.002\n",
      "epoch 20 loss: 0.763 -> 0.602\n",
      "\n",
      "[ 50] batch loss: 0.001\n",
      "[100] batch loss: 0.002\n",
      "[150] batch loss: 0.001\n",
      "[200] batch loss: 0.002\n",
      "[250] batch loss: 0.001\n",
      "[300] batch loss: 0.001\n",
      "[350] batch loss: 0.001\n",
      "[400] batch loss: 0.001\n",
      "[450] batch loss: 0.000\n",
      "[500] batch loss: 0.000\n",
      "[550] batch loss: 0.002\n",
      "epoch 21 loss: 0.602 -> 0.545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%skip $load_model_svhn\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 50\n",
    "reset(cnn_svhn)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_svhn):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _, _ = cnn_svhn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%3d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        break\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model_svhn\n",
    "\n",
    "torch.save(cnn_svhn.state_dict(), para_file_svhn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on SVHN test set (train on target): 0.9270897357098955\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on SVHN test set (train on target): \" + str(evaluate_accuracy(cnn_svhn, testloader_svhn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation\n",
    "\n",
    "## Join Source and Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import ST_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainset_da = ST_Dataset(trainset_syn, trainset_svhn, batch_size)\n",
    "trainloader_da = torch.utils.data.DataLoader(trainset_da, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 72th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "I don't know :)\n",
      "\n",
      "From domain:\n",
      "Target\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from test set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_da):\n",
    "    inputs, labels, domain = data\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\")\n",
    "if labels[idx].item() == -1:\n",
    "    print(\"I don't know :)\\n\")\n",
    "else:\n",
    "    print(str(labels[idx].item()) + \"\\n\")\n",
    "\n",
    "print(\"From domain:\")\n",
    "if domain[idx].item() == 0:\n",
    "    print(\"Source\")\n",
    "else:\n",
    "    print(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRL Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, lamda):\n",
    "        ctx.save_for_backward(lamda)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        lamda, = ctx.saved_tensors\n",
    "        return -lamda * grad_outputs, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init):\n",
    "        super(GRL, self).__init__()\n",
    "        self.GRL_func = GRL_func.apply\n",
    "        self.lamda = nn.Parameter(torch.Tensor(1), requires_grad=False)\n",
    "        self.set_lamda(lamda_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.GRL_func(x, self.lamda)\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.lamda[0] = lamda_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_DA(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init=0):\n",
    "        super(CNN_DA, self).__init__()\n",
    "        # lamda\n",
    "        self.lamda = lamda_init\n",
    "        # feature extractor\n",
    "        self.C1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.C2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.C3 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
    "        # label classifier\n",
    "        self.LC_FC1 = nn.Linear(128 * 7 * 7, 3072)\n",
    "        self.LC_FC2 = nn.Linear(3072, 2048)\n",
    "        self.LC_FC3 = nn.Linear(2048, 10)\n",
    "        # domain classifier\n",
    "        self.GRL_layer = GRL(lamda_init)\n",
    "        self.DC_FC1 = nn.Linear(128 * 7 * 7, 1024)\n",
    "        self.DC_FC2 = nn.Linear(1024, 1024)\n",
    "        self.DC_FC3 = nn.Linear(1024, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (3, 3), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (3, 3), stride=(2, 2))\n",
    "        # C3\n",
    "        x = F.relu(self.C3(x))\n",
    "        # x's size is (128, 128, 7, 7)\n",
    "        # flatten\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        f = x\n",
    "        # label classifier\n",
    "        # LC_FC1\n",
    "        x_l = F.relu(self.LC_FC1(x))\n",
    "        # LC_FC2\n",
    "        x_l = F.relu(self.LC_FC2(x_l))\n",
    "        lh = x_l\n",
    "        # LC_FC3\n",
    "        x_l = self.LC_FC3(x_l)\n",
    "        # domain classifier\n",
    "        # GRL\n",
    "        x_d = self.GRL_layer(x)\n",
    "        # DC_FC1\n",
    "        x_d = F.relu(self.DC_FC1(x_d))\n",
    "        # DC_FC2\n",
    "        x_d = F.relu(self.DC_FC2(x_d))\n",
    "        # DC_FC3\n",
    "        x_d = F.sigmoid(self.DC_FC3(x_d))\n",
    "        return x_l, x_d, f, lh\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.GRL_layer.set_lamda(lamda_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_DA(\n",
      "  (C1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (C2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (C3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (LC_FC1): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (LC_FC2): Linear(in_features=3072, out_features=2048, bias=True)\n",
      "  (LC_FC3): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  (GRL_layer): GRL()\n",
      "  (DC_FC1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (DC_FC2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (DC_FC3): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_da = CNN_DA(0)\n",
    "if (use_gpu):\n",
    "    cnn_da.cuda()\n",
    "print(cnn_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_da = False\n"
     ]
    }
   ],
   "source": [
    "para_file_da = \"./parameters/cnn_syn_to_svhn\"\n",
    "load_model_da = os.path.isfile(para_file_da)\n",
    "print(\"load_model_da = \" + str(load_model_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_da)\n",
    "cnn.load_state_dict(torch.load(para_file_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "criterion_DC = nn.BCELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "        \n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'input' to have the same dimension as tensor for 'result'; but 4 does not equal 2 (while checking arguments for cudnn_convolution)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-0e7e98d380f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs_LC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_DC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_da\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutputs_DC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss_LC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_LC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_LC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msource_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msource_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-3d2d1079cc40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# C3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# x's size is (128, 64, 4, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'input' to have the same dimension as tensor for 'result'; but 4 does not equal 2 (while checking arguments for cudnn_convolution)"
     ]
    }
   ],
   "source": [
    "#%%skip $load_model_da\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 50\n",
    "reset(cnn_da)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    lamda = adjust_lamda(cnn_da, p)\n",
    "    for i, data in enumerate(trainloader_da):\n",
    "        source_size = data[0].size()[0] // 2\n",
    "        inputs, labels, domains = data\n",
    "        domains = domains.to(torch.float32)\n",
    "        if (use_gpu):\n",
    "            inputs, labels, domains = inputs.cuda(), labels.cuda(), domains.cuda()\n",
    "        inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, outputs_DC, _, _ = cnn_da(inputs)\n",
    "        outputs_DC = outputs_DC.view(-1)\n",
    "        loss_LC = criterion_LC(outputs_LC[:source_size], labels[:source_size])\n",
    "        loss_DC = criterion_DC(outputs_DC, domains)\n",
    "        loss = loss_LC + loss_DC\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%3d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        prev_loss = epoch_loss\n",
    "        pass\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model_da\n",
    "\n",
    "torch.save(cnn.state_dict(), para_file_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_da_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs_LC, _ = model(inputs)\n",
    "        correct += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST-M test set (source only): 0.8186666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on MNIST-M test set (source only): \" + str(evaluate_da_accuracy(cnn_da, testloader_m)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "My_CNN_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
