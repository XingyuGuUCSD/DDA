{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTya_dtvpdaX"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e9W0cTFEPmAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold\n",
    "from math import exp, sqrt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from my_dataset import ST_Dataset\n",
    "from my_dataset import SYNSIGN\n",
    "from my_dataset import GTSRB\n",
    "%matplotlib inline\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1523506225267,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "4BrzNF_oQL77",
    "outputId": "fe8e2212-37c8-4027-d176-ef46991bcd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNSIGN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import os\n",
    "import csv\n",
    "import errno\n",
    "import scipy.io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import google_drive\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root_dir = \"data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "trainset_syn = SYNSIGN(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_syn = torch.utils.data.DataLoader(trainset_syn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_syn):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AX_Msxj4we7"
   },
   "source": [
    "# Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1523506395172,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "d5glpPlhprD4",
    "outputId": "6e9189d3-996c-434c-985e-0e88793fe4eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 96, kernel_size=5, padding=2)\n",
    "        self.C2 = nn.Conv2d(96, 144, kernel_size=3, padding=2)\n",
    "        self.C3 = nn.Conv2d(144, 256, kernel_size=5, padding=2)\n",
    "        \n",
    "        \n",
    "        self.FC1 = nn.Linear(256 * 5 * 5, 512)\n",
    "        self.FC2 = nn.Linear(512, 43)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C3\n",
    "        x = F.relu(self.C3(x))\n",
    "        # M3\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # x's size is (128, 256, 5, 5)\n",
    "        # flatten\n",
    "        x = x.view(-1, 256 * 5 * 5)\n",
    "        f = x\n",
    "        # FC1\n",
    "        x = F.relu(self.FC1(x))\n",
    "        # FC2\n",
    "        x = F.softmax(self.FC2(x))\n",
    "\n",
    "        return x, f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_syn = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_syn.cuda()\n",
    "print(cnn_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, sqrt(2 / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            size = m.weight.size()\n",
    "            fan_out = size[0] # number of rows\n",
    "            fan_in = size[1] # number of columns\n",
    "            m.weight.data.normal_(0, sqrt(2 / (fan_in + fan_out)))\n",
    "            m.bias.data.zero_()\n",
    "        elif hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcSb16SIFgZt"
   },
   "source": [
    "# Training on SYNSIGN\n",
    "\n",
    "Or you can load the parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "para_file = \"./parameters/cnn_synsign\"\n",
    "load_model = os.path.isfile(para_file)\n",
    "print(\"load_model = \" + str(load_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip (not $load_model)\n",
    "cnn_syn.load_state_dict(torch.load(para_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YRHBSIQ2FkOL"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_syn.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgUSfM0KF4iT"
   },
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 10050
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 606703,
     "status": "ok",
     "timestamp": 1523144380194,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "IlstjDLbFkQ6",
    "outputId": "59a56d48-bdaa-4e0e-bc20-0feaecd74fa5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%skip $load_model\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 100\n",
    "reset(cnn_syn)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_syn):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _, _ = cnn_syn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 299:    # print every 300 mini-batches\n",
    "            print('[%4d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        break\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model\n",
    "\n",
    "torch.save(cnn_syn.state_dict(), para_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oBeLwnRLrcY"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs, _, _ = model(inputs)\n",
    "        correct += (torch.max(outputs.data, 1)[1] == labels.data).sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on source (no testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1632,
     "status": "ok",
     "timestamp": 1523144440407,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "Rn4sOe0iLuKI",
    "outputId": "f31ce94e-ef18-4564-9772-a3a9b22c1272"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy on SYNSIGN train set: \" + str(evaluate_accuracy(cnn_syn, trainloader_syn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on GTSRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root_dir = \"data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_gtsrb = GTSRB(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_gtsrb = torch.utils.data.DataLoader(trainset_gtsrb, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_gtsrb = GTSRB(root_dir, train=False, transform=transform_m, download=True)\n",
    "testloader_gtsrb = torch.utils.data.DataLoader(testset_gtsrb, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 65th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_gtsrb):\n",
    "    inputs, labels = data\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSRB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_gtsrb = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_gtsrb.cuda()\n",
    "print(cnn_gtsrb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_file_gtsrb = \"./parameters/cnn_gtsrb\"\n",
    "load_model_gtsrb = os.path.isfile(para_file_gtsrb)\n",
    "print(\"load_model_gtsrb = \" + str(load_model_gtsrb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_gtsrb)\n",
    "cnn_gtsrb.load_state_dict(torch.load(para_file_gtsrb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_gtsrb.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%skip $load_model_gtsrb\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 50\n",
    "reset(cnn_gtsrb)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_gtsrb):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _, _ = cnn_gtsrb(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%3d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        break\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model_gtsrb\n",
    "\n",
    "torch.save(cnn_gtsrb.state_dict(), para_file_gtsrb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on SVHN test set (train on target): \" + str(evaluate_accuracy(cnn_gtsrb, testloader_gtsrb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation\n",
    "\n",
    "## Join Source and Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import ST_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Source and target dataset combination.\"\"\"\n",
    "    \n",
    "    def __init__(self, source, target, batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source (torch.utils.data.Dataset): Source dataset.\n",
    "            target (torch.utils.data.Dataset): Target dataset.\n",
    "            batch_size (int): Batch size.\n",
    "        \"\"\"\n",
    "        small_len = min(len(source), len(target))\n",
    "        large_len = max(len(source), len(target))\n",
    "        small_len = batch_size * (small_len // batch_size)\n",
    "        self.small_len = small_len\n",
    "        self.length = small_len * (large_len // small_len) * 2\n",
    "        self.batch_size = batch_size\n",
    "        self.half_batch_size = batch_size // 2\n",
    "        \n",
    "        channel, height, width = source[0][0].size()\n",
    "        \n",
    "        self.images = torch.Tensor(self.length, channel, height, width)\n",
    "        self.labels = torch.LongTensor(self.length)\n",
    "        self.domains = torch.LongTensor(self.length)\n",
    "        \n",
    "        for idx in range(self.length):\n",
    "            if idx // 64 % 2 == 0:\n",
    "                # source\n",
    "                idx_s = idx // 128 * 64 + idx % 128\n",
    "                self.images[idx] = source[idx_s][0]\n",
    "                self.labels[idx] = source[idx_s][1]\n",
    "                self.domains[idx] = 0\n",
    "            else:\n",
    "                # target\n",
    "                idx_t = (idx // 128 * 64 + idx % 128 - 64) % self.small_len\n",
    "                self.images[idx] = target[idx_t][0]\n",
    "                self.labels[idx] = target[idx_t][1]\n",
    "                self.domains[idx] = 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx], self.domains[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainset_da = ST_Dataset(trainset_syn, trainset_svhn, batch_size)\n",
    "trainloader_da = torch.utils.data.DataLoader(trainset_da, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly plot a sample from test set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_da):\n",
    "    inputs, labels, domain = data\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\")\n",
    "if labels[idx].item() == -1:\n",
    "    print(\"I don't know :)\\n\")\n",
    "else:\n",
    "    print(str(labels[idx].item()) + \"\\n\")\n",
    "\n",
    "print(\"From domain:\")\n",
    "if domain[idx].item() == 0:\n",
    "    print(\"Source\")\n",
    "else:\n",
    "    print(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRL Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, lamda):\n",
    "        ctx.save_for_backward(lamda)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        lamda, = ctx.saved_tensors\n",
    "        return -lamda * grad_outputs, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "grl = GRL_func.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = grl(x, torch.Tensor([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init):\n",
    "        super(GRL, self).__init__()\n",
    "        self.GRL_func = GRL_func.apply\n",
    "        self.lamda = nn.Parameter(torch.Tensor(1), requires_grad=False)\n",
    "        self.set_lamda(lamda_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.GRL_func(x, self.lamda)\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.lamda[0] = lamda_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_DA(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init=0):\n",
    "        super(CNN_DA, self).__init__()\n",
    "        # lamda\n",
    "        self.lamda = lamda_init\n",
    "        # feature extractor\n",
    "        self.C1 = nn.Conv2d(3, 96, kernel_size=5, padding=2)\n",
    "        self.C2 = nn.Conv2d(96, 144, kernel_size=3, padding=2)\n",
    "        self.C3 = nn.Conv2d(144, 256, kernel_size=5, padding=2)\n",
    "        # label classifier\n",
    "        self.FC1 = nn.Linear(256 * 5 * 5, 512)\n",
    "        self.FC2 = nn.Linear(512, 43)\n",
    "        # domain classifier\n",
    "        self.GRL_layer = GRL(lamda_init)\n",
    "        self.DC_FC1 = nn.Linear(256 * 5 * 5, 1024)\n",
    "        self.DC_FC2 = nn.Linear(1024, 1024)\n",
    "        self.DC_FC3 = nn.Linear(1024, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C3\n",
    "        x = F.relu(self.C3(x))\n",
    "        # M3\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # x's size is (128, 256, 5, 5)\n",
    "        # flatten\n",
    "        x = x.view(-1, 256 * 5 * 5)\n",
    "        f = x\n",
    "        # label classifier\n",
    "        # FC1\n",
    "        x_l = F.relu(self.FC1(x))\n",
    "        # FC2\n",
    "        x_l = F.softmax(self.FC2(x_l))\n",
    "        # domain classifier\n",
    "        # GRL\n",
    "        x_d = self.GRL_layer(x)\n",
    "        # DC_FC1\n",
    "        x_d = F.relu(self.DC_FC1(x_d))\n",
    "        # DC_FC2\n",
    "        x_d = F.relu(self.DC_FC2(x_d))\n",
    "        # DC_FC3\n",
    "        x_d = F.sigmoid(self.DC_FC3(x_d))\n",
    "        return x_l, x_d, f, lh\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.GRL_layer.set_lamda(lamda_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_da = CNN_DA(0)\n",
    "if (use_gpu):\n",
    "    cnn_da.cuda()\n",
    "print(cnn_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_file_da = \"./parameters/cnn_syn_to_svhn\"\n",
    "load_model_da = os.path.isfile(para_file_da)\n",
    "print(\"load_model_da = \" + str(load_model_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_da)\n",
    "cnn.load_state_dict(torch.load(para_file_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "criterion_DC = nn.BCELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "        \n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%skip $load_model_da\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 1\n",
    "# reset(cnn_da)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    p=1\n",
    "    adjust_lr(optimizer, p)\n",
    "    lamda = adjust_lamda(cnn_da, p)\n",
    "    for i, data in enumerate(trainloader_da):\n",
    "        source_size = data[0].size()[0] // 2\n",
    "        inputs, labels, domains = data\n",
    "        domains = domains.to(torch.float32)\n",
    "        if (use_gpu):\n",
    "            inputs, labels, domains = inputs.cuda(), labels.cuda(), domains.cuda()\n",
    "        inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, outputs_DC, _, _ = cnn_da(inputs)\n",
    "        outputs_DC = outputs_DC.view(-1)\n",
    "        loss_LC = criterion_LC(outputs_LC[:source_size], labels[:source_size])\n",
    "        loss_DC = criterion_DC(outputs_DC, domains)\n",
    "        loss = loss_LC + loss_DC\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 600 == 599:    # print every 600 mini-batches\n",
    "            print('[%4d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 600))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    print((\"Label classifier accuracy on SVHN test set (DA): %f\\n\"\n",
    "           \"Domain classifier accuracy on SVHN test set (DA): %f\")\n",
    "          %evaluate_da_accuracy(cnn_da, testloader_svhn, source=False))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        prev_loss = epoch_loss\n",
    "        pass\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8795712968653965\n",
    "0.894169\n",
    "0.9270897357098955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.894169-0.8795712968653965)/(0.9270897357098955-0.8795712968653965)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model_da\n",
    "\n",
    "torch.save(cnn_da.state_dict(), para_file_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_da_accuracy(model, dataloader, source):\n",
    "    correct_LC = 0\n",
    "    correct_DC = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs_LC, outputs_DC, _, _ = model(inputs)\n",
    "        correct_LC += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "        if source:\n",
    "            correct_DC += labels.size()[0] - outputs_DC.data.sum().item()\n",
    "        else:\n",
    "            correct_DC += outputs_DC.data.sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc_LC = correct_LC / total\n",
    "    acc_DC = correct_DC / total\n",
    "    return acc_LC, acc_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((\"Label classifier accuracy on SYNNUM test set (DA): %f\\n\"\n",
    "       \"Domain classifier accuracy on SYNNUM test set (DA): %f\")\n",
    "      %evaluate_da_accuracy(cnn_da, testloader_syn, source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"Label classifier accuracy on SVHN test set (DA): %f\\n\"\n",
    "       \"Domain classifier accuracy on SVHN test set (DA): %f\")\n",
    "      %evaluate_da_accuracy(cnn_da, testloader_svhn, source=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "def extract_features(model, dataloader, num=None):\n",
    "    features = None\n",
    "    lasthidden = None\n",
    "    labels = None\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels_ = data\n",
    "        if (use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        inputs = Variable(inputs)\n",
    "        outputs = model(inputs)\n",
    "        if len(outputs) == 3:\n",
    "            _, f, l = outputs\n",
    "        else:\n",
    "            _, _, f, l = outputs\n",
    "        if i == 0:\n",
    "            features = f\n",
    "            lasthidden = l\n",
    "            labels = labels_\n",
    "        else:\n",
    "            features = torch.cat((features, f))\n",
    "            lasthidden = torch.cat((lasthidden, l))\n",
    "            labels = torch.cat((labels, labels_))\n",
    "        if num:\n",
    "            if labels.size()[0] >= num:\n",
    "                break\n",
    "    features = features.data\n",
    "    lasthidden = lasthidden.data\n",
    "    if use_gpu:\n",
    "        features = features.cpu()\n",
    "        lasthidden = lasthidden.cpu()\n",
    "    return features, lasthidden, labels\n",
    "\n",
    "def visualize_single_dataset(data, labels, perplexity=50, sample_num=None):\n",
    "    total_num = labels.shape[0]\n",
    "    if sample_num:\n",
    "        idx = np.random.choice(total_num, sample_num, replace=False)\n",
    "        data, labels = data[idx, :], labels[idx]\n",
    "        total_num = sample_num\n",
    "    tsne = manifold.TSNE(n_components=2, init='random',\n",
    "                     random_state=0, perplexity=perplexity)\n",
    "    X = tsne.fit_transform(data)\n",
    "    colors = [\"red\", \"orange\", \"goldenrod\", \"yellow\", \"yellowgreen\", \"green\", \"teal\", \"blue\", \"violet\", \"purple\"]\n",
    "    for i in range(10):\n",
    "        plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i], alpha=0.4)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def visualize_da(source, target, perplexity=50, sample_num=None, save=None):\n",
    "    source_num = source.shape[0]\n",
    "    target_num = target.shape[0]\n",
    "    if sample_num:\n",
    "        source, target = source[:sample_num, :], target[:sample_num, :]\n",
    "        \n",
    "    data = np.vstack((source, target))\n",
    "\n",
    "    tsne = manifold.TSNE(n_components=2, init='random',\n",
    "                         random_state=0, perplexity=perplexity)\n",
    "    X = tsne.fit_transform(data)\n",
    "    plt.scatter(X[:sample_num, 0], X[:sample_num, 1], c=\"blue\", edgecolors=None, alpha=0.4)\n",
    "    plt.scatter(X[sample_num:, 0], X[sample_num:, 1], c=\"red\", edgecolors=None, alpha=0.4)\n",
    "    plt.axis(\"off\")\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_syn_da, lh_syn_da, l_syn_da = extract_features(cnn_da, testloader_syn, num=1000)\n",
    "f_svhn_da, lh_svhn_da, l_svhn_da = extract_features(cnn_da, testloader_svhn, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_da(f_syn_da.numpy(), f_svhn_da.numpy(), perplexity=50, sample_num=500,\n",
    "             save=\"./pics/SYNNUM_to_SVHN_features_Adapted.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_da(lh_syn_da.numpy(), lh_svhn_da.numpy(), perplexity=50, sample_num=500,\n",
    "             save=\"./pics/SYNNUM_to_SVHN_lasthidden_Adapted.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_syn, lh_syn, l_syn = extract_features(cnn_syn, testloader_syn, num=1000)\n",
    "f_svhn, lh_svhn, l_svhn = extract_features(cnn_syn, testloader_svhn, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_da(f_syn.numpy(), f_svhn.numpy(), perplexity=50, sample_num=500,\n",
    "             save=\"./pics/SYNNUM_to_SVHN_features_NonAdapted.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_da(lh_syn.numpy(), lh_svhn.numpy(), perplexity=50, sample_num=500,\n",
    "             save=\"./pics/SYNNUM_to_SVHN_lasthidden_NonAdapted.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_single_dataset(lh_svhn_da.numpy(), l_svhn_da.numpy(), perplexity=50, sample_num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_syn, lh_syn, l_syn = extract_features(cnn_syn, testloader_syn, num=1000)\n",
    "f_svhn, lh_svhn, l_svhn = extract_features(cnn_syn, testloader_svhn, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_single_dataset(lh_svhn.numpy(), l_svhn.numpy(), perplexity=50, sample_num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "My_CNN_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
