{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTya_dtvpdaX"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e9W0cTFEPmAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold\n",
    "from math import exp, sqrt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from my_dataset import ST_Dataset\n",
    "from my_dataset import SYNSIGN\n",
    "from my_dataset import GTSRB\n",
    "%matplotlib inline\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1523506225267,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "4BrzNF_oQL77",
    "outputId": "fe8e2212-37c8-4027-d176-ef46991bcd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNSIGN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import os\n",
    "import csv\n",
    "import errno\n",
    "import scipy.io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import google_drive\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root_dir = \"data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "trainset_syn = SYNSIGN(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_syn = torch.utils.data.DataLoader(trainset_syn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "The 12th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_syn):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AX_Msxj4we7"
   },
   "source": [
    "# Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1523506395172,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "d5glpPlhprD4",
    "outputId": "6e9189d3-996c-434c-985e-0e88793fe4eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 96, kernel_size=5, padding=2)\n",
    "        self.C2 = nn.Conv2d(96, 144, kernel_size=3, padding=2)\n",
    "        self.C3 = nn.Conv2d(144, 256, kernel_size=5, padding=2)\n",
    "        \n",
    "        \n",
    "        self.FC1 = nn.Linear(256 * 5 * 5, 512)\n",
    "        self.FC2 = nn.Linear(512, 43)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C3\n",
    "        x = F.relu(self.C3(x))\n",
    "        # M3\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # x's size is (128, 256, 5, 5)\n",
    "        # flatten\n",
    "        x = x.view(-1, 256 * 5 * 5)\n",
    "        f = x\n",
    "        # FC1\n",
    "        x = F.relu(self.FC1(x))\n",
    "        # FC2\n",
    "        x = F.softmax(self.FC2(x))\n",
    "\n",
    "        return x, f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C2): Conv2d(96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (C3): Conv2d(144, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (FC1): Linear(in_features=6400, out_features=512, bias=True)\n",
      "  (FC2): Linear(in_features=512, out_features=43, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_syn = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_syn.cuda()\n",
    "print(cnn_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, sqrt(2 / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            size = m.weight.size()\n",
    "            fan_out = size[0] # number of rows\n",
    "            fan_in = size[1] # number of columns\n",
    "            m.weight.data.normal_(0, sqrt(2 / (fan_in + fan_out)))\n",
    "            m.bias.data.zero_()\n",
    "        elif hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcSb16SIFgZt"
   },
   "source": [
    "# Training on SYNSIGN\n",
    "\n",
    "Or you can load the parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model = True\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "para_file = \"./parameters/cnn_synsign\"\n",
    "load_model = os.path.isfile(para_file)\n",
    "print(\"load_model = \" + str(load_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip (not $load_model)\n",
    "#cnn_syn.load_state_dict(torch.load(para_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YRHBSIQ2FkOL"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_syn.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgUSfM0KF4iT"
   },
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 10050
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 606703,
     "status": "ok",
     "timestamp": 1523144380194,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "IlstjDLbFkQ6",
    "outputId": "59a56d48-bdaa-4e0e-bc20-0feaecd74fa5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 300] batch loss: 3.681\n",
      "[ 600] batch loss: 3.548\n",
      "epoch 1 loss: inf -> 2810.840\n",
      "\n",
      "[ 300] batch loss: 3.487\n",
      "[ 600] batch loss: 3.444\n",
      "epoch 2 loss: 2810.840 -> 2700.628\n",
      "\n",
      "[ 300] batch loss: 3.389\n",
      "[ 600] batch loss: 3.383\n",
      "epoch 3 loss: 2700.628 -> 2648.436\n",
      "\n",
      "[ 300] batch loss: 3.385\n",
      "[ 600] batch loss: 3.377\n",
      "epoch 4 loss: 2648.436 -> 2641.195\n",
      "\n",
      "[ 300] batch loss: 3.359\n",
      "[ 600] batch loss: 3.354\n",
      "epoch 5 loss: 2641.195 -> 2622.658\n",
      "\n",
      "[ 300] batch loss: 3.307\n",
      "[ 600] batch loss: 3.268\n",
      "epoch 6 loss: 2622.658 -> 2568.137\n",
      "\n",
      "[ 300] batch loss: 3.269\n",
      "[ 600] batch loss: 3.266\n",
      "epoch 7 loss: 2568.137 -> 2555.883\n",
      "\n",
      "[ 300] batch loss: 3.268\n",
      "[ 600] batch loss: 3.257\n",
      "epoch 8 loss: 2555.883 -> 2544.733\n",
      "\n",
      "[ 300] batch loss: 3.223\n",
      "[ 600] batch loss: 3.219\n",
      "epoch 9 loss: 2544.733 -> 2519.190\n",
      "\n",
      "[ 300] batch loss: 3.210\n",
      "[ 600] batch loss: 3.196\n",
      "epoch 10 loss: 2519.190 -> 2504.197\n",
      "\n",
      "[ 300] batch loss: 3.198\n",
      "[ 600] batch loss: 3.174\n",
      "epoch 11 loss: 2504.197 -> 2489.879\n",
      "\n",
      "[ 300] batch loss: 3.175\n",
      "[ 600] batch loss: 3.172\n",
      "epoch 12 loss: 2489.879 -> 2481.926\n",
      "\n",
      "[ 300] batch loss: 3.159\n",
      "[ 600] batch loss: 3.146\n",
      "epoch 13 loss: 2481.926 -> 2461.546\n",
      "\n",
      "[ 300] batch loss: 3.114\n",
      "[ 600] batch loss: 3.102\n",
      "epoch 14 loss: 2461.546 -> 2429.977\n",
      "\n",
      "[ 300] batch loss: 3.082\n",
      "[ 600] batch loss: 3.079\n",
      "epoch 15 loss: 2429.977 -> 2405.484\n",
      "\n",
      "[ 300] batch loss: 3.037\n",
      "[ 600] batch loss: 3.033\n",
      "epoch 16 loss: 2405.484 -> 2372.991\n",
      "\n",
      "[ 300] batch loss: 3.033\n",
      "[ 600] batch loss: 3.018\n",
      "epoch 17 loss: 2372.991 -> 2362.806\n",
      "\n",
      "[ 300] batch loss: 3.009\n",
      "[ 600] batch loss: 2.984\n",
      "epoch 18 loss: 2362.806 -> 2337.043\n",
      "\n",
      "[ 300] batch loss: 2.966\n",
      "[ 600] batch loss: 2.965\n",
      "epoch 19 loss: 2337.043 -> 2318.240\n",
      "\n",
      "[ 300] batch loss: 2.965\n",
      "[ 600] batch loss: 2.964\n",
      "epoch 20 loss: 2318.240 -> 2317.728\n",
      "\n",
      "[ 300] batch loss: 2.965\n",
      "[ 600] batch loss: 2.962\n",
      "epoch 21 loss: 2317.728 -> 2312.835\n",
      "\n",
      "[ 300] batch loss: 2.944\n",
      "[ 600] batch loss: 2.941\n",
      "epoch 22 loss: 2312.835 -> 2300.088\n",
      "\n",
      "[ 300] batch loss: 2.943\n",
      "[ 600] batch loss: 2.941\n",
      "epoch 23 loss: 2300.088 -> 2299.612\n",
      "\n",
      "[ 300] batch loss: 2.943\n",
      "[ 600] batch loss: 2.941\n",
      "epoch 24 loss: 2299.612 -> 2299.575\n",
      "\n",
      "[ 300] batch loss: 2.942\n",
      "[ 600] batch loss: 2.940\n",
      "epoch 25 loss: 2299.575 -> 2299.334\n",
      "\n",
      "[ 300] batch loss: 2.942\n",
      "[ 600] batch loss: 2.940\n",
      "epoch 26 loss: 2299.334 -> 2299.228\n",
      "\n",
      "[ 300] batch loss: 2.942\n",
      "[ 600] batch loss: 2.940\n",
      "epoch 27 loss: 2299.228 -> 2299.215\n",
      "\n",
      "[ 300] batch loss: 2.942\n",
      "[ 600] batch loss: 2.940\n",
      "epoch 28 loss: 2299.215 -> 2299.063\n",
      "\n",
      "[ 300] batch loss: 2.942\n",
      "[ 600] batch loss: 2.940\n",
      "epoch 29 loss: 2299.063 -> 2298.056\n",
      "\n",
      "[ 300] batch loss: 2.937\n",
      "[ 600] batch loss: 2.918\n",
      "epoch 30 loss: 2298.056 -> 2286.890\n",
      "\n",
      "[ 300] batch loss: 2.919\n",
      "[ 600] batch loss: 2.917\n",
      "epoch 31 loss: 2286.890 -> 2280.987\n",
      "\n",
      "[ 300] batch loss: 2.919\n",
      "[ 600] batch loss: 2.917\n",
      "epoch 32 loss: 2280.987 -> 2280.714\n",
      "\n",
      "[ 300] batch loss: 2.919\n",
      "[ 600] batch loss: 2.916\n",
      "epoch 33 loss: 2280.714 -> 2280.633\n",
      "\n",
      "[ 300] batch loss: 2.918\n",
      "[ 600] batch loss: 2.917\n",
      "epoch 34 loss: 2280.633 -> 2280.609\n",
      "\n",
      "[ 300] batch loss: 2.919\n",
      "[ 600] batch loss: 2.917\n",
      "epoch 35 loss: 2280.609 -> 2280.659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%skip $load_model\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 1000\n",
    "reset(cnn_syn)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_syn):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _, _ = cnn_syn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 299:    # print every 300 mini-batches\n",
    "            print('[%4d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.001:\n",
    "        break\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model\n",
    "\n",
    "torch.save(cnn_syn.state_dict(), para_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oBeLwnRLrcY"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs, _, _ = model(inputs)\n",
    "        correct += (torch.max(outputs.data, 1)[1] == labels.data).sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on source (no testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1632,
     "status": "ok",
     "timestamp": 1523144440407,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "Rn4sOe0iLuKI",
    "outputId": "f31ce94e-ef18-4564-9772-a3a9b22c1272"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on SYNSIGN train set: 0.88256\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on SYNSIGN train set: \" + str(evaluate_accuracy(cnn_syn, trainloader_syn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSRB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root_dir = \"data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_gtsrb = GTSRB(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_gtsrb = torch.utils.data.DataLoader(trainset_gtsrb, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_gtsrb = GTSRB(root_dir, train=False, transform=transform_m, download=True)\n",
    "testloader_gtsrb = torch.utils.data.DataLoader(testset_gtsrb, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 126th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(34)\n"
     ]
    }
   ],
   "source": [
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_gtsrb):\n",
    "    inputs, labels = data\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on GTSRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GTSRB test set (source only): 0.5628661916072842\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on GTSRB test set (source only): \" + str(evaluate_accuracy(cnn_syn, testloader_gtsrb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C2): Conv2d(96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (C3): Conv2d(144, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (FC1): Linear(in_features=6400, out_features=512, bias=True)\n",
      "  (FC2): Linear(in_features=512, out_features=43, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_gtsrb = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_gtsrb.cuda()\n",
    "print(cnn_gtsrb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_gtsrb = False\n"
     ]
    }
   ],
   "source": [
    "para_file_gtsrb = \"./parameters/cnn_gtsrb\"\n",
    "load_model_gtsrb = os.path.isfile(para_file_gtsrb)\n",
    "print(\"load_model_gtsrb = \" + str(load_model_gtsrb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_gtsrb)\n",
    "cnn_gtsrb.load_state_dict(torch.load(para_file_gtsrb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_gtsrb.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50] batch loss: 3.760\n",
      "epoch 1 loss: inf -> 266.901\n",
      "\n",
      "[ 50] batch loss: 3.754\n",
      "epoch 2 loss: 266.901 -> 266.335\n",
      "\n",
      "[ 50] batch loss: 3.753\n",
      "epoch 3 loss: 266.335 -> 266.273\n",
      "\n",
      "[ 50] batch loss: 3.723\n",
      "epoch 4 loss: 266.273 -> 264.191\n",
      "\n",
      "[ 50] batch loss: 3.693\n",
      "epoch 5 loss: 264.191 -> 262.116\n",
      "\n",
      "[ 50] batch loss: 3.657\n",
      "epoch 6 loss: 262.116 -> 259.664\n",
      "\n",
      "[ 50] batch loss: 3.639\n",
      "epoch 7 loss: 259.664 -> 258.140\n",
      "\n",
      "[ 50] batch loss: 3.603\n",
      "epoch 8 loss: 258.140 -> 255.945\n",
      "\n",
      "[ 50] batch loss: 3.587\n",
      "epoch 9 loss: 255.945 -> 254.636\n",
      "\n",
      "[ 50] batch loss: 3.560\n",
      "epoch 10 loss: 254.636 -> 252.659\n",
      "\n",
      "[ 50] batch loss: 3.530\n",
      "epoch 11 loss: 252.659 -> 250.497\n",
      "\n",
      "[ 50] batch loss: 3.508\n",
      "epoch 12 loss: 250.497 -> 248.919\n",
      "\n",
      "[ 50] batch loss: 3.471\n",
      "epoch 13 loss: 248.919 -> 246.300\n",
      "\n",
      "[ 50] batch loss: 3.457\n",
      "epoch 14 loss: 246.300 -> 245.312\n",
      "\n",
      "[ 50] batch loss: 3.440\n",
      "epoch 15 loss: 245.312 -> 244.303\n",
      "\n",
      "[ 50] batch loss: 3.433\n",
      "epoch 16 loss: 244.303 -> 243.797\n",
      "\n",
      "[ 50] batch loss: 3.412\n",
      "epoch 17 loss: 243.797 -> 242.373\n",
      "\n",
      "[ 50] batch loss: 3.407\n",
      "epoch 18 loss: 242.373 -> 242.031\n",
      "\n",
      "[ 50] batch loss: 3.403\n",
      "epoch 19 loss: 242.031 -> 241.763\n",
      "\n",
      "[ 50] batch loss: 3.401\n",
      "epoch 20 loss: 241.763 -> 241.467\n",
      "\n",
      "[ 50] batch loss: 3.370\n",
      "epoch 21 loss: 241.467 -> 239.405\n",
      "\n",
      "[ 50] batch loss: 3.358\n",
      "epoch 22 loss: 239.405 -> 238.639\n",
      "\n",
      "[ 50] batch loss: 3.355\n",
      "epoch 23 loss: 238.639 -> 238.461\n",
      "\n",
      "[ 50] batch loss: 3.354\n",
      "epoch 24 loss: 238.461 -> 238.360\n",
      "\n",
      "[ 50] batch loss: 3.346\n",
      "epoch 25 loss: 238.360 -> 237.334\n",
      "\n",
      "[ 50] batch loss: 3.319\n",
      "epoch 26 loss: 237.334 -> 235.553\n",
      "\n",
      "[ 50] batch loss: 3.313\n",
      "epoch 27 loss: 235.553 -> 235.156\n",
      "\n",
      "[ 50] batch loss: 3.310\n",
      "epoch 28 loss: 235.156 -> 234.977\n",
      "\n",
      "[ 50] batch loss: 3.301\n",
      "epoch 29 loss: 234.977 -> 234.046\n",
      "\n",
      "[ 50] batch loss: 3.289\n",
      "epoch 30 loss: 234.046 -> 233.372\n",
      "\n",
      "[ 50] batch loss: 3.288\n",
      "epoch 31 loss: 233.372 -> 233.275\n",
      "\n",
      "[ 50] batch loss: 3.284\n",
      "epoch 32 loss: 233.275 -> 232.805\n",
      "\n",
      "[ 50] batch loss: 3.259\n",
      "epoch 33 loss: 232.805 -> 230.970\n",
      "\n",
      "[ 50] batch loss: 3.253\n",
      "epoch 34 loss: 230.970 -> 230.667\n",
      "\n",
      "[ 50] batch loss: 3.249\n",
      "epoch 35 loss: 230.667 -> 230.434\n",
      "\n",
      "[ 50] batch loss: 3.239\n",
      "epoch 36 loss: 230.434 -> 229.552\n",
      "\n",
      "[ 50] batch loss: 3.220\n",
      "epoch 37 loss: 229.552 -> 228.510\n",
      "\n",
      "[ 50] batch loss: 3.218\n",
      "epoch 38 loss: 228.510 -> 228.376\n",
      "\n",
      "[ 50] batch loss: 3.213\n",
      "epoch 39 loss: 228.376 -> 227.809\n",
      "\n",
      "[ 50] batch loss: 3.197\n",
      "epoch 40 loss: 227.809 -> 226.982\n",
      "\n",
      "[ 50] batch loss: 3.196\n",
      "epoch 41 loss: 226.982 -> 226.910\n",
      "\n",
      "[ 50] batch loss: 3.192\n",
      "epoch 42 loss: 226.910 -> 226.693\n",
      "\n",
      "[ 50] batch loss: 3.184\n",
      "epoch 43 loss: 226.693 -> 225.945\n",
      "\n",
      "[ 50] batch loss: 3.171\n",
      "epoch 44 loss: 225.945 -> 225.151\n",
      "\n",
      "[ 50] batch loss: 3.170\n",
      "epoch 45 loss: 225.151 -> 225.104\n",
      "\n",
      "[ 50] batch loss: 3.170\n",
      "epoch 46 loss: 225.104 -> 224.844\n",
      "\n",
      "[ 50] batch loss: 3.149\n",
      "epoch 47 loss: 224.844 -> 223.539\n",
      "\n",
      "[ 50] batch loss: 3.148\n",
      "epoch 48 loss: 223.539 -> 223.494\n",
      "\n",
      "[ 50] batch loss: 3.147\n",
      "epoch 49 loss: 223.494 -> 223.413\n",
      "\n",
      "[ 50] batch loss: 3.146\n",
      "epoch 50 loss: 223.413 -> 223.104\n",
      "\n",
      "[ 50] batch loss: 3.126\n",
      "epoch 51 loss: 223.104 -> 221.852\n",
      "\n",
      "[ 50] batch loss: 3.111\n",
      "epoch 52 loss: 221.852 -> 220.654\n",
      "\n",
      "[ 50] batch loss: 3.101\n",
      "epoch 53 loss: 220.654 -> 220.132\n",
      "\n",
      "[ 50] batch loss: 3.087\n",
      "epoch 54 loss: 220.132 -> 219.099\n",
      "\n",
      "[ 50] batch loss: 3.081\n",
      "epoch 55 loss: 219.099 -> 218.748\n",
      "\n",
      "[ 50] batch loss: 3.080\n",
      "epoch 56 loss: 218.748 -> 218.708\n",
      "\n",
      "[ 50] batch loss: 3.080\n",
      "epoch 57 loss: 218.708 -> 218.695\n",
      "\n",
      "[ 50] batch loss: 3.079\n",
      "epoch 58 loss: 218.695 -> 218.461\n",
      "\n",
      "[ 50] batch loss: 3.064\n",
      "epoch 59 loss: 218.461 -> 217.509\n",
      "\n",
      "[ 50] batch loss: 3.060\n",
      "epoch 60 loss: 217.509 -> 217.258\n",
      "\n",
      "[ 50] batch loss: 3.057\n",
      "epoch 61 loss: 217.258 -> 217.099\n",
      "\n",
      "[ 50] batch loss: 3.057\n",
      "epoch 62 loss: 217.099 -> 217.062\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 63 loss: 217.062 -> 217.043\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 64 loss: 217.043 -> 217.035\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 65 loss: 217.035 -> 217.030\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 66 loss: 217.030 -> 217.026\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 67 loss: 217.026 -> 217.023\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 68 loss: 217.023 -> 217.021\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 69 loss: 217.021 -> 217.018\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 70 loss: 217.018 -> 217.016\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 71 loss: 217.016 -> 217.014\n",
      "\n",
      "[ 50] batch loss: 3.056\n",
      "epoch 72 loss: 217.014 -> 216.675\n",
      "\n",
      "[ 50] batch loss: 3.013\n",
      "epoch 73 loss: 216.675 -> 213.968\n",
      "\n",
      "[ 50] batch loss: 2.995\n",
      "epoch 74 loss: 213.968 -> 212.656\n",
      "\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 75 loss: 212.656 -> 212.268\n",
      "\n",
      "[ 50] batch loss: 2.988\n",
      "epoch 76 loss: 212.268 -> 212.190\n",
      "\n",
      "[ 50] batch loss: 2.988\n",
      "epoch 77 loss: 212.190 -> 212.175\n",
      "\n",
      "[ 50] batch loss: 2.987\n",
      "epoch 78 loss: 212.175 -> 212.138\n",
      "\n",
      "[ 50] batch loss: 2.986\n",
      "epoch 79 loss: 212.138 -> 212.108\n",
      "\n",
      "[ 50] batch loss: 2.987\n",
      "epoch 80 loss: 212.108 -> 212.084\n",
      "\n",
      "[ 50] batch loss: 2.984\n",
      "epoch 81 loss: 212.084 -> 211.565\n",
      "\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 82 loss: 211.565 -> 210.630\n",
      "\n",
      "[ 50] batch loss: 2.965\n",
      "epoch 83 loss: 210.630 -> 210.556\n",
      "\n",
      "[ 50] batch loss: 2.962\n",
      "epoch 84 loss: 210.556 -> 210.371\n",
      "\n",
      "[ 50] batch loss: 2.961\n",
      "epoch 85 loss: 210.371 -> 210.296\n",
      "\n",
      "[ 50] batch loss: 2.961\n",
      "epoch 86 loss: 210.296 -> 210.290\n",
      "\n",
      "[ 50] batch loss: 2.961\n",
      "epoch 87 loss: 210.290 -> 210.287\n",
      "\n",
      "[ 50] batch loss: 2.961\n",
      "epoch 88 loss: 210.287 -> 210.284\n",
      "\n",
      "[ 50] batch loss: 2.961\n",
      "epoch 89 loss: 210.284 -> 210.282\n",
      "\n",
      "[ 50] batch loss: 2.961\n",
      "epoch 90 loss: 210.282 -> 210.281\n",
      "\n",
      "[ 50] batch loss: 2.960\n",
      "epoch 91 loss: 210.281 -> 210.279\n",
      "\n",
      "[ 50] batch loss: 2.960\n",
      "epoch 92 loss: 210.279 -> 210.273\n",
      "\n",
      "[ 50] batch loss: 2.960\n",
      "epoch 93 loss: 210.273 -> 210.272\n",
      "\n",
      "[ 50] batch loss: 2.960\n",
      "epoch 94 loss: 210.272 -> 210.269\n",
      "\n",
      "[ 50] batch loss: 2.960\n",
      "epoch 95 loss: 210.269 -> 210.268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%skip $load_model_gtsrb\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 1000\n",
    "reset(cnn_gtsrb)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_gtsrb):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, _, _ = cnn_gtsrb(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%3d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.001:\n",
    "        break   \n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip $load_model_gtsrb\n",
    "\n",
    "torch.save(cnn_gtsrb.state_dict(), para_file_gtsrb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GTSTB test set (train on target): 0.5551860649247823\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on GTSTB test set (train on target): \" + str(evaluate_accuracy(cnn_gtsrb, testloader_gtsrb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation\n",
    "\n",
    "## Join Source and Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Source and target dataset combination.\"\"\"\n",
    "    \n",
    "    def __init__(self, source, target, batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source (torch.utils.data.Dataset): Source dataset.\n",
    "            target (torch.utils.data.Dataset): Target dataset.\n",
    "            batch_size (int): Batch size.\n",
    "        \"\"\"\n",
    "        small_len = min(len(source), len(target))\n",
    "        large_len = max(len(source), len(target))\n",
    "        \n",
    "        small_len = batch_size * (small_len // batch_size)\n",
    "        self.small_len = small_len\n",
    "        large_len = batch_size * (large_len // batch_size)\n",
    "        self.large_len = large_len\n",
    "        \n",
    "        self.length = small_len * (large_len // small_len) * 2\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.half_batch_size = batch_size // 2\n",
    "        \n",
    "        channel, height, width = source[0][0].shape\n",
    "        \n",
    "        \n",
    "        self.images = torch.Tensor(self.length, channel, height, width)\n",
    "        self.labels = torch.LongTensor(self.length)\n",
    "        self.domains = torch.LongTensor(self.length)\n",
    "\n",
    "        \n",
    "        for idx in range(self.length):\n",
    "            if idx // 64 % 2 == 0:\n",
    "                # source\n",
    "                idx_s = idx // 128 * 64 + idx % 128\n",
    "                self.images[idx] = torch.from_numpy(source[idx_s][0]).type(torch.FloatTensor)\n",
    "                self.labels[idx] = source[idx_s][1]\n",
    "                self.domains[idx] = 0\n",
    "            else:\n",
    "                # target\n",
    "                idx_t = (idx // 128 * 64 + idx % 128 - 64) % self.small_len\n",
    "                self.images[idx] = torch.from_numpy(target[idx_t][0]).type(torch.FloatTensor)\n",
    "                self.labels[idx] = -1 #target[idx_t][1]\n",
    "                self.domains[idx] = 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx], self.domains[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainset_da = ST_Dataset(trainset_syn, trainset_gtsrb, batch_size)\n",
    "trainloader_da = torch.utils.data.DataLoader(trainset_da, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 114th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "I don't know :)\n",
      "\n",
      "From domain:\n",
      "Target\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from test set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_da):\n",
    "    inputs, labels, domain = data\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\")\n",
    "if labels[idx].item() == -1:\n",
    "    print(\"I don't know :)\\n\")\n",
    "else:\n",
    "    print(str(labels[idx].item()) + \"\\n\")\n",
    "\n",
    "print(\"From domain:\")\n",
    "if domain[idx].item() == 0:\n",
    "    print(\"Source\")\n",
    "else:\n",
    "    print(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRL Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, lamda):\n",
    "        ctx.save_for_backward(lamda)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        lamda, = ctx.saved_tensors\n",
    "        return -lamda * grad_outputs, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init):\n",
    "        super(GRL, self).__init__()\n",
    "        self.GRL_func = GRL_func.apply\n",
    "        self.lamda = nn.Parameter(torch.Tensor(1), requires_grad=False)\n",
    "        self.set_lamda(lamda_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.GRL_func(x, self.lamda)\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.lamda[0] = lamda_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_DA(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init=0):\n",
    "        super(CNN_DA, self).__init__()\n",
    "        # lamda\n",
    "        self.lamda = lamda_init\n",
    "        # feature extractor\n",
    "        self.C1 = nn.Conv2d(3, 96, kernel_size=5, padding=2)\n",
    "        self.C2 = nn.Conv2d(96, 144, kernel_size=3, padding=2)\n",
    "        self.C3 = nn.Conv2d(144, 256, kernel_size=5, padding=2)\n",
    "        # label classifier\n",
    "        self.FC1 = nn.Linear(256 * 5 * 5, 512)\n",
    "        self.FC2 = nn.Linear(512, 43)\n",
    "        # domain classifier\n",
    "        self.GRL_layer = GRL(lamda_init)\n",
    "        self.DC_FC1 = nn.Linear(256 * 5 * 5, 1024)\n",
    "        self.DC_FC2 = nn.Linear(1024, 1024)\n",
    "        self.DC_FC3 = nn.Linear(1024, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C3\n",
    "        x = F.relu(self.C3(x))\n",
    "        # M3\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # x's size is (128, 256, 5, 5)\n",
    "        # flatten\n",
    "        x = x.view(-1, 256 * 5 * 5)\n",
    "        f = x\n",
    "        # label classifier\n",
    "        # FC1\n",
    "        x_l = F.relu(self.FC1(x))\n",
    "        # FC2F\n",
    "        x_l = F.softmax(self.FC2(x_l))\n",
    "        # domain classifier\n",
    "        # GRL\n",
    "        x_d = self.GRL_layer(x)\n",
    "        # DC_FC1\n",
    "        x_d = F.relu(self.DC_FC1(x_d))\n",
    "        # DC_FC2\n",
    "        x_d = F.relu(self.DC_FC2(x_d))\n",
    "        lh = x\n",
    "        # DC_FC3\n",
    "        x_d = F.sigmoid(self.DC_FC3(x_d))\n",
    "        return x_l, x_d, f, lh\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.GRL_layer.set_lamda(lamda_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_DA(\n",
      "  (C1): Conv2d(3, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C2): Conv2d(96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (C3): Conv2d(144, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (FC1): Linear(in_features=6400, out_features=512, bias=True)\n",
      "  (FC2): Linear(in_features=512, out_features=43, bias=True)\n",
      "  (GRL_layer): GRL()\n",
      "  (DC_FC1): Linear(in_features=6400, out_features=1024, bias=True)\n",
      "  (DC_FC2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (DC_FC3): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_da = CNN_DA(0)\n",
    "if (use_gpu):\n",
    "    cnn_da.cuda()\n",
    "print(cnn_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_da = False\n"
     ]
    }
   ],
   "source": [
    "para_file_da = \"./parameters/cnn_syn_to_gtsrp\"\n",
    "load_model_da = os.path.isfile(para_file_da)\n",
    "print(\"load_model_da = \" + str(load_model_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_da)\n",
    "cnn.load_state_dict(torch.load(para_file_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.001\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "criterion_DC = nn.BCELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "        \n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_da_accuracy(model, dataloader, source):\n",
    "    correct_LC = 0\n",
    "    correct_DC = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs_LC, outputs_DC, _, _ = model(inputs)\n",
    "        correct_LC += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "        if source:\n",
    "            correct_DC += labels.size()[0] - outputs_DC.data.sum().item()\n",
    "        else:\n",
    "            correct_DC += outputs_DC.data.sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc_LC = correct_LC / total\n",
    "    acc_DC = correct_DC / total\n",
    "    return acc_LC, acc_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 600] batch loss: 4.452\n",
      "[1200] batch loss: 4.452\n",
      "epoch 1 loss: inf -> 6856.118\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.007601\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.484894\n",
      "[ 600] batch loss: 4.451\n",
      "[1200] batch loss: 4.451\n",
      "epoch 2 loss: 6856.118 -> 6854.194\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.022565\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.494416\n",
      "[ 600] batch loss: 4.451\n",
      "[1200] batch loss: 4.452\n",
      "epoch 3 loss: 6854.194 -> 6855.850\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.048456\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.502884\n",
      "[ 600] batch loss: 4.454\n",
      "[1200] batch loss: 4.455\n",
      "epoch 4 loss: 6855.850 -> 6860.810\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.049406\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.508512\n",
      "[ 600] batch loss: 4.457\n",
      "[1200] batch loss: 4.457\n",
      "epoch 5 loss: 6860.810 -> 6863.523\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.053127\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.509238\n",
      "[ 600] batch loss: 4.457\n",
      "[1200] batch loss: 4.457\n",
      "epoch 6 loss: 6863.523 -> 6863.896\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.048219\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.509336\n",
      "[ 600] batch loss: 4.456\n",
      "[1200] batch loss: 4.456\n",
      "epoch 7 loss: 6863.896 -> 6861.939\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.030800\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.506521\n",
      "[ 600] batch loss: 4.456\n",
      "[1200] batch loss: 4.456\n",
      "epoch 8 loss: 6861.939 -> 6861.978\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.030721\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.503884\n",
      "[ 600] batch loss: 4.456\n",
      "[1200] batch loss: 4.456\n",
      "epoch 9 loss: 6861.978 -> 6862.791\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.016627\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.502068\n",
      "[ 600] batch loss: 4.457\n",
      "[1200] batch loss: 4.456\n",
      "epoch 10 loss: 6862.791 -> 6862.676\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.012747\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.500523\n",
      "[ 600] batch loss: 4.456\n",
      "[1200] batch loss: 4.455\n",
      "epoch 11 loss: 6862.676 -> 6861.198\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.019873\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.501177\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.455\n",
      "epoch 12 loss: 6861.198 -> 6860.971\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.018131\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.500110\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.455\n",
      "epoch 13 loss: 6860.971 -> 6860.224\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.017815\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.501315\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.456\n",
      "epoch 14 loss: 6860.224 -> 6861.510\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.018606\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.497602\n",
      "[ 600] batch loss: 4.456\n",
      "[1200] batch loss: 4.456\n",
      "epoch 15 loss: 6861.510 -> 6862.172\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.018606\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.498712\n",
      "[ 600] batch loss: 4.456\n",
      "[1200] batch loss: 4.456\n",
      "epoch 16 loss: 6862.172 -> 6861.940\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.010293\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.499671\n",
      "[ 600] batch loss: 4.456\n",
      "[1200] batch loss: 4.455\n",
      "epoch 17 loss: 6861.940 -> 6861.147\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.027870\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.498138\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.455\n",
      "epoch 18 loss: 6861.147 -> 6860.445\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.029770\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.500920\n",
      "[ 600] batch loss: 4.454\n",
      "[1200] batch loss: 4.454\n",
      "epoch 19 loss: 6860.445 -> 6859.505\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.038638\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.499678\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.455\n",
      "epoch 20 loss: 6859.505 -> 6860.163\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.014173\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.498858\n",
      "[ 600] batch loss: 4.454\n",
      "[1200] batch loss: 4.454\n",
      "epoch 21 loss: 6860.163 -> 6859.862\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.006017\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.498926\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.455\n",
      "epoch 22 loss: 6859.862 -> 6860.236\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.010451\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.502348\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.455\n",
      "epoch 23 loss: 6860.236 -> 6860.448\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.008551\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.503246\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.455\n",
      "epoch 24 loss: 6860.448 -> 6861.054\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.036580\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.502183\n",
      "[ 600] batch loss: 4.455\n",
      "[1200] batch loss: 4.455\n",
      "epoch 25 loss: 6861.054 -> 6860.539\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.011006\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.501548\n",
      "[ 600] batch loss: 4.454\n",
      "[1200] batch loss: 4.455\n",
      "epoch 26 loss: 6860.539 -> 6859.989\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.015519\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.500632\n",
      "[ 600] batch loss: 4.454\n",
      "[1200] batch loss: 4.454\n",
      "epoch 27 loss: 6859.989 -> 6859.782\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.051465\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.500855\n",
      "[ 600] batch loss: 4.455\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ecacc731e8a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m600\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m599\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# print every 600 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%skip $load_model_da\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 100\n",
    "reset(cnn_da)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    p=1\n",
    "    adjust_lr(optimizer, p)\n",
    "    lamda = adjust_lamda(cnn_da, p)\n",
    "    for i, data in enumerate(trainloader_da):\n",
    "        source_size = data[0].size()[0] // 2\n",
    "        inputs, labels, domains = data\n",
    "        domains = domains.to(torch.float32)\n",
    "        if (use_gpu):\n",
    "            inputs, labels, domains = inputs.type(torch.FloatTensor).cuda(), labels.cuda(), domains.cuda()\n",
    "        inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, outputs_DC, _, _ = cnn_da(inputs)\n",
    "        outputs_DC = outputs_DC.view(-1)\n",
    "        loss_LC = criterion_LC(outputs_LC[:source_size], labels[:source_size])\n",
    "        loss_DC = criterion_DC(outputs_DC, domains)\n",
    "        loss = loss_LC + loss_DC\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 600 == 599:    # print every 600 mini-batches\n",
    "            print('[%4d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 600))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    print((\"Label classifier accuracy on GTSRP test set (DA): %f\\n\"\n",
    "           \"Domain classifier accuracy on GTSRP test set (DA): %f\")\n",
    "          %evaluate_da_accuracy(cnn_da, testloader_gtsrb, source=False))\n",
    "    if prev_loss - epoch_loss < 0.001:\n",
    "        prev_loss = epoch_loss\n",
    "        pass\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model_da\n",
    "\n",
    "torch.save(cnn_da.state_dict(), para_file_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((\"Label classifier accuracy on SYNNUM test set (DA): %f\\n\"\n",
    "       \"Domain classifier accuracy on SYNNUM test set (DA): %f\")\n",
    "      %evaluate_da_accuracy(cnn_da, testloader_syn, source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"Label classifier accuracy on SVHN test set (DA): %f\\n\"\n",
    "       \"Domain classifier accuracy on SVHN test set (DA): %f\")\n",
    "      %evaluate_da_accuracy(cnn_da, testloader_svhn, source=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "My_CNN_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
